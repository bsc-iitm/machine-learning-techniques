{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Machine Learning Techniques \u00b6 This is a set of notes for the MLT course.","title":"Home"},{"location":"#machine-learning-techniques","text":"This is a set of notes for the MLT course.","title":"Machine Learning Techniques"},{"location":"slides/week-5/","text":"Course Outline \u00b6 Linear regression Least square classification Perceptron Logistic regression Naive Bayes Softmax regression Support Vector Machines (SVM) Decision trees Ensemble techniques K-means clustering Artificial Neural Networks Story so far \u00b6 Regression ::: incremental Linear regression Polynomial regression ::: Story so far \u00b6 Regression Linear regression Polynomial regression Classification ::: incremental Least square classification Perceptron ::: Unifying theme \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} ::: What is one striking similarity among all the models we have seen so far? ::: {.column width=\"0%\"} ::: ::: Linearity \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: ::: Linearity \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} ::: $$ \\huge{w^T x} $$ ::: ::: {.column width=\"0%\"} ::: Linearity \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} ::: $$ \\huge{w^T x} = w_0 + w_1x_1 + \\cdots + w_n x_n $$ ::: ::: {.column width=\"0%\"} ::: Linearity \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} $$ w^T x = w_0 + w_1x_1 + \\cdots + w_n x_n $$ ::: ::: {.column width=\"50%\"} What does \\(w_i\\) represent? ::: ::: Linearity \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} $$ w^T x = w_0 + w_1x_1 + \\cdots + w_n x_n $$ ::: ::: {.column width=\"50%\"} What does \\(w_i\\) represent? The importance of feature \\(x_i\\) ::: ::: Linearity \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} $$ w^T x $$ ::: ::: {.column width=\"50%\"} ::: incremental Linear regression Polynomial regression Least square classification Perceptron Logistic Regression? ::: ::: ::: Logistic Regression \u00b6 ::: {.columns align=left} ::: {.column width=\"100%\"} Setting ::: incremental Binary classification Discriminative model ::: ::: ::: {.column width=\"0%\"} ::: ::: Task \u00b6 ::: {.columns align=left} ::: {.column width=\"100%\"} ::: incremental Feature matrix: \\(X\\) \\(m \\times n\\) \\(m\\) data-points, \\(n\\) features Label vector: \\(y\\) \\(m \\times 1\\) \\(0\\) or \\(1\\) ::: ::: ::: {.column width=\"0%\"} ::: ::: Task \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} Feature matrix: \\(X\\) \\(m \\times n\\) \\(m\\) data-points, \\(n\\) features Label vector: \\(y\\) \\(m \\times 1\\) \\(0\\) or \\(1\\) ::: ::: {.column width=\"50%\"} Learn a function \\(h\\) such that given a feature vector \\(x\\) , the predicted label is given as: $$ y_{\\text{pred}} = h(x) $$ ::: ::: Types of models \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} $$ P(y\\ |\\ x) $$ Discriminative ::: ::: {.column width=\"50%\"} $$ P(x, y) $$ Generative ::: ::: Logistic Regression \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ P(y = 1\\ |\\ x) = ? $$ ::: ::: {.column width=\"0%\"} ::: ::: Logistic Regression \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ P(y = 1\\ |\\ x) = \\sigma(w^T x) $$ ::: ::: {.column width=\"0%\"} ::: ::: Logistic Regression \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ P(y = 1\\ |\\ x) = \\sigma(w^T x) $$ What kind of a random variable is \\(y\\) ? ::: ::: {.column width=\"0%\"} ::: ::: Sigmoid \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\sigma(z) = \\cfrac{1}{1 + e^{-z}} $$ ::: ::: {.column width=\"0%\"} ::: ::: Sigmoid \u00b6 ::: {.columns align=center} ::: {.column width=\"20%\"} $$ \\sigma(z) = \\cfrac{1}{1 + e^{-z}} $$ ::: ::: {.column width=\"80%\"} Here \\(g = \\sigma\\) : ::: ::: Logistic Regression \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} $$ P(y = 1\\ |\\ x) = \\sigma(w^T x) $$ ::: ::: {.column width=\"50%\"} $$ P(y = 0\\ |\\ x) = ? $$ ::: ::: Logistic Regression \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} $$ P(y = 1\\ |\\ x) = \\sigma(w^T x) $$ ::: ::: {.column width=\"50%\"} $$ P(y = 0\\ |\\ x) = 1 - \\sigma(w^T x) $$ ::: ::: MLE \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\begin{aligned} L(w) &= \\prod \\limits_{i = 1}^{m} P(y = y_i\\ |\\ x_i)\\quad \\quad \\quad \\quad \\quad \\quad\\ \\ \\ \\\\\\ \\end{aligned} $$ ::: ::: {.column width=\"0%\"} ::: ::: MLE \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\begin{aligned} L(w) &= \\prod \\limits_{i = 1}^{m} P(y = y_i\\ |\\ x_i)\\\\ &= \\prod \\limits_{i = 1}^{m} \\left[ \\sigma(w^Tx_i) \\right]^{y_i} \\left[ 1 - \\sigma(w^Tx_i) \\right]^{1 - y_i} \\end{aligned} $$ ::: ::: {.column width=\"0%\"} ::: ::: MLE \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\begin{aligned} L(w) &= \\prod \\limits_{i = 1}^{m} \\left[ \\sigma(w^Tx_i) \\right]^{y_i} \\left[ 1 - \\sigma(w^Tx_i) \\right]^{1 - y_i} \\end{aligned} $$ ::: ::: {.column width=\"0%\"} ::: ::: MLE \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\begin{aligned} l(w) &= \\log (L(w))\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\\\ &= \\log \\left (\\prod \\limits_{i = 1}^{m} \\left[ \\sigma(w^Tx_i) \\right]^{y_i} \\left[ 1 - \\sigma(w^Tx_i) \\right]^{1 - y_i} \\right) \\end{aligned} $$ ::: ::: {.column width=\"0%\"} ::: ::: MLE \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\begin{aligned} l(w) &= \\log (L(w))\\\\ &= \\log \\left (\\prod \\limits_{i = 1}^{m} \\left[ \\sigma(w^Tx_i) \\right]^{y_i} \\left[ 1 - \\sigma(w^Tx_i) \\right]^{1 - y_i} \\right)\\\\ &= \\sum \\limits_{i = 1}^{m} y_i \\cdot \\log (\\sigma(w^T x_i)) + (1 - y_i) \\log (1 - \\sigma(w^Tx_i)) \\end{aligned} $$ ::: ::: {.column width=\"0%\"} ::: ::: MLE \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\begin{aligned} \\max\\quad l(w) \\end{aligned} $$ ::: ::: {.column width=\"0%\"} ::: ::: MLE \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\begin{aligned} \\min\\quad -l(w) \\end{aligned} $$ ::: ::: {.column width=\"0%\"} ::: ::: MLE \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\begin{aligned} \\min\\quad -l(w) \\end{aligned} $$ Minimize the negative log-likelihood ::: ::: {.column width=\"0%\"} ::: ::: MLE \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\begin{aligned} \\min\\quad - \\sum \\limits_{i = 1}^{m} y_i \\cdot \\log (\\sigma(w^T x_i)) + (1 - y_i) \\log (1 - \\sigma(w^Tx_i)) \\end{aligned} $$ Note: \\(-\\) sign is for the entire sum ::: ::: {.column width=\"0%\"} ::: ::: MLE \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} $$ \\begin{aligned} - \\left[ y_i \\cdot \\log (\\sigma(w^T x_i)) + (1 - y_i) \\log (1 - \\sigma(w^Tx_i))\\right] \\end{aligned} $$ ::: ::: {.column width=\"50%\"} ::: ::: MLE \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} $$ \\begin{aligned} - \\left[ y_i \\cdot \\log (\\sigma(w^T x_i)) + (1 - y_i) \\log (1 - \\sigma(w^Tx_i))\\right] \\end{aligned} $$ ::: ::: {.column width=\"50%\"} $$ -\\left [ p \\log q + (1 - p) \\log (1 - q) \\right] $$ ::: :::","title":"MLT | Week-5"},{"location":"slides/week-5/#course-outline","text":"Linear regression Least square classification Perceptron Logistic regression Naive Bayes Softmax regression Support Vector Machines (SVM) Decision trees Ensemble techniques K-means clustering Artificial Neural Networks","title":"Course Outline"},{"location":"slides/week-5/#story-so-far","text":"Regression ::: incremental Linear regression Polynomial regression :::","title":"Story so far"},{"location":"slides/week-5/#story-so-far_1","text":"Regression Linear regression Polynomial regression Classification ::: incremental Least square classification Perceptron :::","title":"Story so far"},{"location":"slides/week-5/#unifying-theme","text":"::: {.columns align=center} ::: {.column width=\"100%\"} ::: What is one striking similarity among all the models we have seen so far? ::: {.column width=\"0%\"} ::: :::","title":"Unifying theme"},{"location":"slides/week-5/#linearity","text":"::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: :::","title":"Linearity"},{"location":"slides/week-5/#linearity_1","text":"::: {.columns align=center} ::: {.column width=\"100%\"} ::: $$ \\huge{w^T x} $$ ::: ::: {.column width=\"0%\"} :::","title":"Linearity"},{"location":"slides/week-5/#linearity_2","text":"::: {.columns align=center} ::: {.column width=\"100%\"} ::: $$ \\huge{w^T x} = w_0 + w_1x_1 + \\cdots + w_n x_n $$ ::: ::: {.column width=\"0%\"} :::","title":"Linearity"},{"location":"slides/week-5/#linearity_3","text":"::: {.columns align=center} ::: {.column width=\"50%\"} $$ w^T x = w_0 + w_1x_1 + \\cdots + w_n x_n $$ ::: ::: {.column width=\"50%\"} What does \\(w_i\\) represent? ::: :::","title":"Linearity"},{"location":"slides/week-5/#linearity_4","text":"::: {.columns align=center} ::: {.column width=\"50%\"} $$ w^T x = w_0 + w_1x_1 + \\cdots + w_n x_n $$ ::: ::: {.column width=\"50%\"} What does \\(w_i\\) represent? The importance of feature \\(x_i\\) ::: :::","title":"Linearity"},{"location":"slides/week-5/#linearity_5","text":"::: {.columns align=left} ::: {.column width=\"50%\"} $$ w^T x $$ ::: ::: {.column width=\"50%\"} ::: incremental Linear regression Polynomial regression Least square classification Perceptron Logistic Regression? ::: ::: :::","title":"Linearity"},{"location":"slides/week-5/#logistic-regression","text":"::: {.columns align=left} ::: {.column width=\"100%\"} Setting ::: incremental Binary classification Discriminative model ::: ::: ::: {.column width=\"0%\"} ::: :::","title":"Logistic Regression"},{"location":"slides/week-5/#task","text":"::: {.columns align=left} ::: {.column width=\"100%\"} ::: incremental Feature matrix: \\(X\\) \\(m \\times n\\) \\(m\\) data-points, \\(n\\) features Label vector: \\(y\\) \\(m \\times 1\\) \\(0\\) or \\(1\\) ::: ::: ::: {.column width=\"0%\"} ::: :::","title":"Task"},{"location":"slides/week-5/#task_1","text":"::: {.columns align=left} ::: {.column width=\"50%\"} Feature matrix: \\(X\\) \\(m \\times n\\) \\(m\\) data-points, \\(n\\) features Label vector: \\(y\\) \\(m \\times 1\\) \\(0\\) or \\(1\\) ::: ::: {.column width=\"50%\"} Learn a function \\(h\\) such that given a feature vector \\(x\\) , the predicted label is given as: $$ y_{\\text{pred}} = h(x) $$ ::: :::","title":"Task"},{"location":"slides/week-5/#types-of-models","text":"::: {.columns align=center} ::: {.column width=\"50%\"} $$ P(y\\ |\\ x) $$ Discriminative ::: ::: {.column width=\"50%\"} $$ P(x, y) $$ Generative ::: :::","title":"Types of models"},{"location":"slides/week-5/#logistic-regression_1","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ P(y = 1\\ |\\ x) = ? $$ ::: ::: {.column width=\"0%\"} ::: :::","title":"Logistic Regression"},{"location":"slides/week-5/#logistic-regression_2","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ P(y = 1\\ |\\ x) = \\sigma(w^T x) $$ ::: ::: {.column width=\"0%\"} ::: :::","title":"Logistic Regression"},{"location":"slides/week-5/#logistic-regression_3","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ P(y = 1\\ |\\ x) = \\sigma(w^T x) $$ What kind of a random variable is \\(y\\) ? ::: ::: {.column width=\"0%\"} ::: :::","title":"Logistic Regression"},{"location":"slides/week-5/#sigmoid","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\sigma(z) = \\cfrac{1}{1 + e^{-z}} $$ ::: ::: {.column width=\"0%\"} ::: :::","title":"Sigmoid"},{"location":"slides/week-5/#sigmoid_1","text":"::: {.columns align=center} ::: {.column width=\"20%\"} $$ \\sigma(z) = \\cfrac{1}{1 + e^{-z}} $$ ::: ::: {.column width=\"80%\"} Here \\(g = \\sigma\\) : ::: :::","title":"Sigmoid"},{"location":"slides/week-5/#logistic-regression_4","text":"::: {.columns align=center} ::: {.column width=\"50%\"} $$ P(y = 1\\ |\\ x) = \\sigma(w^T x) $$ ::: ::: {.column width=\"50%\"} $$ P(y = 0\\ |\\ x) = ? $$ ::: :::","title":"Logistic Regression"},{"location":"slides/week-5/#logistic-regression_5","text":"::: {.columns align=center} ::: {.column width=\"50%\"} $$ P(y = 1\\ |\\ x) = \\sigma(w^T x) $$ ::: ::: {.column width=\"50%\"} $$ P(y = 0\\ |\\ x) = 1 - \\sigma(w^T x) $$ ::: :::","title":"Logistic Regression"},{"location":"slides/week-5/#mle","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\begin{aligned} L(w) &= \\prod \\limits_{i = 1}^{m} P(y = y_i\\ |\\ x_i)\\quad \\quad \\quad \\quad \\quad \\quad\\ \\ \\ \\\\\\ \\end{aligned} $$ ::: ::: {.column width=\"0%\"} ::: :::","title":"MLE"},{"location":"slides/week-5/#mle_1","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\begin{aligned} L(w) &= \\prod \\limits_{i = 1}^{m} P(y = y_i\\ |\\ x_i)\\\\ &= \\prod \\limits_{i = 1}^{m} \\left[ \\sigma(w^Tx_i) \\right]^{y_i} \\left[ 1 - \\sigma(w^Tx_i) \\right]^{1 - y_i} \\end{aligned} $$ ::: ::: {.column width=\"0%\"} ::: :::","title":"MLE"},{"location":"slides/week-5/#mle_2","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\begin{aligned} L(w) &= \\prod \\limits_{i = 1}^{m} \\left[ \\sigma(w^Tx_i) \\right]^{y_i} \\left[ 1 - \\sigma(w^Tx_i) \\right]^{1 - y_i} \\end{aligned} $$ ::: ::: {.column width=\"0%\"} ::: :::","title":"MLE"},{"location":"slides/week-5/#mle_3","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\begin{aligned} l(w) &= \\log (L(w))\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\\\ &= \\log \\left (\\prod \\limits_{i = 1}^{m} \\left[ \\sigma(w^Tx_i) \\right]^{y_i} \\left[ 1 - \\sigma(w^Tx_i) \\right]^{1 - y_i} \\right) \\end{aligned} $$ ::: ::: {.column width=\"0%\"} ::: :::","title":"MLE"},{"location":"slides/week-5/#mle_4","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\begin{aligned} l(w) &= \\log (L(w))\\\\ &= \\log \\left (\\prod \\limits_{i = 1}^{m} \\left[ \\sigma(w^Tx_i) \\right]^{y_i} \\left[ 1 - \\sigma(w^Tx_i) \\right]^{1 - y_i} \\right)\\\\ &= \\sum \\limits_{i = 1}^{m} y_i \\cdot \\log (\\sigma(w^T x_i)) + (1 - y_i) \\log (1 - \\sigma(w^Tx_i)) \\end{aligned} $$ ::: ::: {.column width=\"0%\"} ::: :::","title":"MLE"},{"location":"slides/week-5/#mle_5","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\begin{aligned} \\max\\quad l(w) \\end{aligned} $$ ::: ::: {.column width=\"0%\"} ::: :::","title":"MLE"},{"location":"slides/week-5/#mle_6","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\begin{aligned} \\min\\quad -l(w) \\end{aligned} $$ ::: ::: {.column width=\"0%\"} ::: :::","title":"MLE"},{"location":"slides/week-5/#mle_7","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\begin{aligned} \\min\\quad -l(w) \\end{aligned} $$ Minimize the negative log-likelihood ::: ::: {.column width=\"0%\"} ::: :::","title":"MLE"},{"location":"slides/week-5/#mle_8","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\begin{aligned} \\min\\quad - \\sum \\limits_{i = 1}^{m} y_i \\cdot \\log (\\sigma(w^T x_i)) + (1 - y_i) \\log (1 - \\sigma(w^Tx_i)) \\end{aligned} $$ Note: \\(-\\) sign is for the entire sum ::: ::: {.column width=\"0%\"} ::: :::","title":"MLE"},{"location":"slides/week-5/#mle_9","text":"::: {.columns align=center} ::: {.column width=\"50%\"} $$ \\begin{aligned} - \\left[ y_i \\cdot \\log (\\sigma(w^T x_i)) + (1 - y_i) \\log (1 - \\sigma(w^Tx_i))\\right] \\end{aligned} $$ ::: ::: {.column width=\"50%\"} ::: :::","title":"MLE"},{"location":"slides/week-5/#mle_10","text":"::: {.columns align=center} ::: {.column width=\"50%\"} $$ \\begin{aligned} - \\left[ y_i \\cdot \\log (\\sigma(w^T x_i)) + (1 - y_i) \\log (1 - \\sigma(w^Tx_i))\\right] \\end{aligned} $$ ::: ::: {.column width=\"50%\"} $$ -\\left [ p \\log q + (1 - p) \\log (1 - q) \\right] $$ ::: :::","title":"MLE"},{"location":"slides/week-6/","text":"Course Outline \u00b6 Linear regression Least square classification Perceptron Logistic regression Naive Bayes Softmax regression K-NN Support Vector Machines (SVM) Decision trees Ensemble techniques K-means clustering Artificial Neural Networks Multivariate Normal Distribution \u00b6 ::: {.columns align=center} ::: {.column width=\"25%\"} $$ \\mathbf{x} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\mathbf{\\Sigma}) $$ ::: ::: {.column width=\"75%\"} ::: ::: Multivariate Normal Distribution \u00b6 ::: {.columns align=center} ::: {.column width=\"25%\"} $$ \\mathbf{x} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\mathbf{\\Sigma}) $$ ::: ::: {.column width=\"75%\"} $$ \\mathbf{x} = \\begin{bmatrix} x_1\\ \\vdots\\ x_n \\end{bmatrix} \\quad \\quad \\boldsymbol{\\mu} = \\begin{bmatrix} \\mu_1\\ \\vdots\\ \\mu_n \\end{bmatrix} $$ $$ \\Sigma_{ij} = \\mathbf{E}[(x_i - \\mu_i)(x_j - \\mu_j)] = \\text{Cov}[x_i, x_j] $$ ::: ::: Multivariate Normal Distribution \u00b6 ::: {.columns align=center} ::: {.column width=\"25%\"} $$ \\mathbf{x} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\mathbf{\\Sigma}) $$ ::: ::: {.column width=\"75%\"} $$ f_{\\mathbf{x}}(x_1, \\cdots, x_n) = \\cfrac{\\exp \\left[ -\\cfrac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu})^T \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}) \\right]}{\\sqrt{(2 \\pi)^n|\\mathbf{\\Sigma}|}} $$ ::: ::: Bivariate Normal Distribution (Diagonal \\(\\Sigma\\) ) \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} $$ \\mathbf{x} = \\begin{bmatrix} x_1\\ x_2 \\end{bmatrix} \\quad \\quad \\boldsymbol{\\mu} = \\begin{bmatrix} \\mu_1\\ \\mu_2 \\end{bmatrix} $$ $$ \\mathbf{\\Sigma} = \\begin{bmatrix} \\sigma_1^2 & 0\\ 0 & \\sigma_2^2 \\end{bmatrix} $$ ::: ::: ::: {.column width=\"50%\"} ::: ::: Bivariate Normal Distribution (Diagonal \\(\\Sigma\\) ) \u00b6 ::: {.columns align=center} ::: {.column width=\"25%\"} $$ \\mathbf{x} = \\begin{bmatrix} x_1\\ x_2 \\end{bmatrix} \\quad \\quad \\boldsymbol{\\mu} = \\begin{bmatrix} \\mu_1\\ \\mu_2 \\end{bmatrix} $$ $$ \\mathbf{\\Sigma} = \\begin{bmatrix} \\sigma_1^2 & 0\\ 0 & \\sigma_2^2 \\end{bmatrix} $$ ::: ::: {.column width=\"75%\"} $$ \\begin{bmatrix} x_1 - \\mu_1 & x_2 - \\mu_2 \\end{bmatrix} \\begin{bmatrix} \\frac{1}{\\sigma_1^2} & 0\\ 0 & \\frac{1}{\\sigma_2^2} \\end{bmatrix} \\begin{bmatrix} x_1 - \\mu_1\\ x_2 - \\mu_2 \\end{bmatrix} $$ ::: ::: Bivariate Normal Distribution (Diagonal \\(\\Sigma\\) ) \u00b6 ::: {.columns align=center} ::: {.column width=\"25%\"} $$ \\cfrac{\\exp \\left[ -\\cfrac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu})^T \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}) \\right]}{\\sqrt{(2 \\pi)^n|\\mathbf{\\Sigma}|}} $$ ::: ::: {.column width=\"75%\"} $$ \\left( \\cfrac{x_1 - \\mu_1}{\\sigma_1} \\right)^2 + \\left( \\cfrac{x_2 - \\mu_2}{\\sigma_2} \\right)^2 $$ ::: ::: Bivariate Normal Distribution (Diagonal \\(\\Sigma\\) ) \u00b6 ::: {.columns align=center} ::: {.column width=\"25%\"} $$ \\cfrac{\\exp \\left[ -\\cfrac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu})^T \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}) \\right]}{\\sqrt{(2 \\pi)^n|\\mathbf{\\Sigma}|}} $$ ::: ::: {.column width=\"75%\"} $$ \\cfrac{\\exp \\left[ -\\cfrac{1}{2}\\left( \\cfrac{x_1 - \\mu_1}{\\sigma_1} \\right)^2 - \\cfrac{1}{2}\\left( \\cfrac{x_2 - \\mu_2}{\\sigma_2} \\right)^2 \\right]}{\\sqrt{(2 \\pi)^2 \\sigma_1^2 \\sigma_2^2}} $$ ::: ::: Bivariate Normal Distribution (Diagonal \\(\\Sigma\\) ) \u00b6 ::: {.columns align=center} ::: {.column width=\"25%\"} $$ \\cfrac{\\exp \\left[ -\\cfrac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu})^T \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}) \\right]}{\\sqrt{(2 \\pi)^n|\\mathbf{\\Sigma}|}} $$ ::: ::: {.column width=\"75%\"} $$ \\cfrac{\\exp \\left[ -\\cfrac{1}{2}\\left( \\cfrac{x_1 - \\mu_1}{\\sigma_1} \\right)^2\\right] \\cdot \\exp \\left[ -\\cfrac{1}{2}\\left( \\cfrac{x_2 - \\mu_2}{\\sigma_2} \\right)^2 \\right]}{\\sqrt{(2 \\pi) \\sigma_1^2 } \\cdot \\sqrt{(2 \\pi) \\sigma_2^2 }} $$ ::: ::: Bivariate Normal Distribution (Diagonal \\(\\Sigma\\) ) \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ f_{\\mathbf{X}}(x_1, x_2) = f_{x_1}(x_1) \\cdot f_{x_2}(x_2) $$ ::: ::: {.column width=\"0%\"} ::: ::: Bivariate Normal Distribution (Diagonal \\(\\Sigma\\) ) \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ f_{\\mathbf{X}}(x_1, x_2) = f_{x_1}(x_1) \\cdot f_{x_2}(x_2) $$ \\(x_1\\) and \\(x_2\\) are independent ::: ::: {.column width=\"0%\"} ::: ::: Gaussian NB \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ P(x, y) = P(y) \\cdot P(x\\ |\\ y) $$ ::: ::: {.column width=\"0%\"} ::: ::: Gaussian NB \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ P(x, y) = P(y) \\cdot P(x\\ |\\ y) $$ \\(P(x\\ |\\ y)\\) is a multivariate Gaussian with \u2014\u2014\u2014\u2014\u2014 covariance matrix. ::: ::: {.column width=\"0%\"} ::: ::: Gaussian NB \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ P(x, y) = P(y) \\cdot P(x\\ |\\ y) $$ \\(P(x\\ |\\ y)\\) is a multivariate Gaussian with diagonal covariance matrix. ::: ::: {.column width=\"0%\"} ::: ::: MLE \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\max \\prod \\limits_{i = 1}^{n} P(y^{(i)}) P(x^{(i)}\\ |\\ y^{(i)}) $$ ::: ::: {.column width=\"0%\"} ::: ::: MLE \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\max \\prod \\limits_{i = 1}^{n} P(y^{(i)}) \\prod \\limits_{j = 1}^{m} P(x_{j}^{(i)}\\ |\\ y^{(i)}) $$ ::: ::: {.column width=\"0%\"} ::: ::: MLE \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\max \\sum \\limits_{i = 1}^{n} \\log P(y^{(i)}) \\sum \\limits_{j = 1}^{m} \\log P(x_{j}^{(i)}\\ |\\ y^{(i)}) $$ ::: ::: {.column width=\"0%\"} ::: ::: MLE \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\max \\sum \\limits_{i = 1}^{n} \\log P(y^{(i)}) \\sum \\limits_{j = 1}^{m} -\\cfrac{1}{2} \\log (2 \\pi \\sigma_j^2) - \\cfrac{1}{2} \\left( \\cfrac{x_j^{(i)} - \\mu_j}{\\sigma_j} \\right)^2 $$ ::: ::: {.column width=\"0%\"} ::: ::: MLE \u00b6 ::: {.columns align=center} ::: {.column width=\"40%\"} $$ \\hat{\\mu} {jc} = \\cfrac{\\sum \\limits ^{n} x_j^{(i)} \\mathcal{1}{y^{(i)} = c}}{\\sum \\limits_{i = 1}^{n} \\mathcal{1}{y^{(i)} = c}} $$ ::: ::: {.column width=\"60%\"} $$ \\sigma_{jc}^2 = \\cfrac{\\sum \\limits_{1 = 1}^{n} (x_j^{(i)} - \\hat{\\mu} {jc})^2 \\mathcal{1}{y^{(i)} = c}}{\\sum \\limits ^{n} \\mathcal{1}{y^{(i)} = c}} $$ ::: ::: MLE \u00b6 ::: {.columns align=center} ::: {.column width=\"40%\"} $$ \\boldsymbol{\\hat{\\mu} c} = \\cfrac{\\sum \\limits ^{n} \\mathbf{x^{(i)}}\\mathcal{1}{y^{(i)} = c}}{\\sum \\limits_{i = 1}^{n}\\mathcal{1}{y^{(i)} = c}} $$ ::: ::: {.column width=\"60%\"} $$ \\text{diag}(\\mathbf{\\Sigma_c}) = \\cfrac{\\sum \\limits_{i = 1}^{n} (\\mathbf{x^{(i)} - \\boldsymbol{\\hat{\\mu}} {c}}) \\odot (\\mathbf{x^{(i)} - \\boldsymbol{\\hat{\\mu}} }) \\mathcal{1}{y^{(i)} = c}}{\\sum \\limits_{i = 1}^{n}\\mathcal{1}{y^{(i)} = c}} $$ ::: ::: NumPy \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: ::: NumPy \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: ::: Inference \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\hat{y} = \\arg \\max_{c} P(y = c\\ |\\ x) $$ ::: ::: {.column width=\"50%\"} ::: ::: Inference \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\hat{y} = \\arg \\max_{c} P(y = c) P(x\\ |\\ y = c) $$ ::: ::: {.column width=\"50%\"} ::: ::: Inference \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"50%\"} ::: :::","title":"MLT | Week-6 | Gaussian Naive Bayes"},{"location":"slides/week-6/#course-outline","text":"Linear regression Least square classification Perceptron Logistic regression Naive Bayes Softmax regression K-NN Support Vector Machines (SVM) Decision trees Ensemble techniques K-means clustering Artificial Neural Networks","title":"Course Outline"},{"location":"slides/week-6/#multivariate-normal-distribution","text":"::: {.columns align=center} ::: {.column width=\"25%\"} $$ \\mathbf{x} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\mathbf{\\Sigma}) $$ ::: ::: {.column width=\"75%\"} ::: :::","title":"Multivariate Normal Distribution"},{"location":"slides/week-6/#multivariate-normal-distribution_1","text":"::: {.columns align=center} ::: {.column width=\"25%\"} $$ \\mathbf{x} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\mathbf{\\Sigma}) $$ ::: ::: {.column width=\"75%\"} $$ \\mathbf{x} = \\begin{bmatrix} x_1\\ \\vdots\\ x_n \\end{bmatrix} \\quad \\quad \\boldsymbol{\\mu} = \\begin{bmatrix} \\mu_1\\ \\vdots\\ \\mu_n \\end{bmatrix} $$ $$ \\Sigma_{ij} = \\mathbf{E}[(x_i - \\mu_i)(x_j - \\mu_j)] = \\text{Cov}[x_i, x_j] $$ ::: :::","title":"Multivariate Normal Distribution"},{"location":"slides/week-6/#multivariate-normal-distribution_2","text":"::: {.columns align=center} ::: {.column width=\"25%\"} $$ \\mathbf{x} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\mathbf{\\Sigma}) $$ ::: ::: {.column width=\"75%\"} $$ f_{\\mathbf{x}}(x_1, \\cdots, x_n) = \\cfrac{\\exp \\left[ -\\cfrac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu})^T \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}) \\right]}{\\sqrt{(2 \\pi)^n|\\mathbf{\\Sigma}|}} $$ ::: :::","title":"Multivariate Normal Distribution"},{"location":"slides/week-6/#bivariate-normal-distribution-diagonal-sigma","text":"::: {.columns align=center} ::: {.column width=\"50%\"} $$ \\mathbf{x} = \\begin{bmatrix} x_1\\ x_2 \\end{bmatrix} \\quad \\quad \\boldsymbol{\\mu} = \\begin{bmatrix} \\mu_1\\ \\mu_2 \\end{bmatrix} $$ $$ \\mathbf{\\Sigma} = \\begin{bmatrix} \\sigma_1^2 & 0\\ 0 & \\sigma_2^2 \\end{bmatrix} $$ ::: ::: ::: {.column width=\"50%\"} ::: :::","title":"Bivariate Normal Distribution (Diagonal \\(\\Sigma\\))"},{"location":"slides/week-6/#bivariate-normal-distribution-diagonal-sigma_1","text":"::: {.columns align=center} ::: {.column width=\"25%\"} $$ \\mathbf{x} = \\begin{bmatrix} x_1\\ x_2 \\end{bmatrix} \\quad \\quad \\boldsymbol{\\mu} = \\begin{bmatrix} \\mu_1\\ \\mu_2 \\end{bmatrix} $$ $$ \\mathbf{\\Sigma} = \\begin{bmatrix} \\sigma_1^2 & 0\\ 0 & \\sigma_2^2 \\end{bmatrix} $$ ::: ::: {.column width=\"75%\"} $$ \\begin{bmatrix} x_1 - \\mu_1 & x_2 - \\mu_2 \\end{bmatrix} \\begin{bmatrix} \\frac{1}{\\sigma_1^2} & 0\\ 0 & \\frac{1}{\\sigma_2^2} \\end{bmatrix} \\begin{bmatrix} x_1 - \\mu_1\\ x_2 - \\mu_2 \\end{bmatrix} $$ ::: :::","title":"Bivariate Normal Distribution (Diagonal \\(\\Sigma\\))"},{"location":"slides/week-6/#bivariate-normal-distribution-diagonal-sigma_2","text":"::: {.columns align=center} ::: {.column width=\"25%\"} $$ \\cfrac{\\exp \\left[ -\\cfrac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu})^T \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}) \\right]}{\\sqrt{(2 \\pi)^n|\\mathbf{\\Sigma}|}} $$ ::: ::: {.column width=\"75%\"} $$ \\left( \\cfrac{x_1 - \\mu_1}{\\sigma_1} \\right)^2 + \\left( \\cfrac{x_2 - \\mu_2}{\\sigma_2} \\right)^2 $$ ::: :::","title":"Bivariate Normal Distribution (Diagonal \\(\\Sigma\\))"},{"location":"slides/week-6/#bivariate-normal-distribution-diagonal-sigma_3","text":"::: {.columns align=center} ::: {.column width=\"25%\"} $$ \\cfrac{\\exp \\left[ -\\cfrac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu})^T \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}) \\right]}{\\sqrt{(2 \\pi)^n|\\mathbf{\\Sigma}|}} $$ ::: ::: {.column width=\"75%\"} $$ \\cfrac{\\exp \\left[ -\\cfrac{1}{2}\\left( \\cfrac{x_1 - \\mu_1}{\\sigma_1} \\right)^2 - \\cfrac{1}{2}\\left( \\cfrac{x_2 - \\mu_2}{\\sigma_2} \\right)^2 \\right]}{\\sqrt{(2 \\pi)^2 \\sigma_1^2 \\sigma_2^2}} $$ ::: :::","title":"Bivariate Normal Distribution (Diagonal \\(\\Sigma\\))"},{"location":"slides/week-6/#bivariate-normal-distribution-diagonal-sigma_4","text":"::: {.columns align=center} ::: {.column width=\"25%\"} $$ \\cfrac{\\exp \\left[ -\\cfrac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu})^T \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}) \\right]}{\\sqrt{(2 \\pi)^n|\\mathbf{\\Sigma}|}} $$ ::: ::: {.column width=\"75%\"} $$ \\cfrac{\\exp \\left[ -\\cfrac{1}{2}\\left( \\cfrac{x_1 - \\mu_1}{\\sigma_1} \\right)^2\\right] \\cdot \\exp \\left[ -\\cfrac{1}{2}\\left( \\cfrac{x_2 - \\mu_2}{\\sigma_2} \\right)^2 \\right]}{\\sqrt{(2 \\pi) \\sigma_1^2 } \\cdot \\sqrt{(2 \\pi) \\sigma_2^2 }} $$ ::: :::","title":"Bivariate Normal Distribution (Diagonal \\(\\Sigma\\))"},{"location":"slides/week-6/#bivariate-normal-distribution-diagonal-sigma_5","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ f_{\\mathbf{X}}(x_1, x_2) = f_{x_1}(x_1) \\cdot f_{x_2}(x_2) $$ ::: ::: {.column width=\"0%\"} ::: :::","title":"Bivariate Normal Distribution (Diagonal \\(\\Sigma\\))"},{"location":"slides/week-6/#bivariate-normal-distribution-diagonal-sigma_6","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ f_{\\mathbf{X}}(x_1, x_2) = f_{x_1}(x_1) \\cdot f_{x_2}(x_2) $$ \\(x_1\\) and \\(x_2\\) are independent ::: ::: {.column width=\"0%\"} ::: :::","title":"Bivariate Normal Distribution (Diagonal \\(\\Sigma\\))"},{"location":"slides/week-6/#gaussian-nb","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ P(x, y) = P(y) \\cdot P(x\\ |\\ y) $$ ::: ::: {.column width=\"0%\"} ::: :::","title":"Gaussian NB"},{"location":"slides/week-6/#gaussian-nb_1","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ P(x, y) = P(y) \\cdot P(x\\ |\\ y) $$ \\(P(x\\ |\\ y)\\) is a multivariate Gaussian with \u2014\u2014\u2014\u2014\u2014 covariance matrix. ::: ::: {.column width=\"0%\"} ::: :::","title":"Gaussian NB"},{"location":"slides/week-6/#gaussian-nb_2","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ P(x, y) = P(y) \\cdot P(x\\ |\\ y) $$ \\(P(x\\ |\\ y)\\) is a multivariate Gaussian with diagonal covariance matrix. ::: ::: {.column width=\"0%\"} ::: :::","title":"Gaussian NB"},{"location":"slides/week-6/#mle","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\max \\prod \\limits_{i = 1}^{n} P(y^{(i)}) P(x^{(i)}\\ |\\ y^{(i)}) $$ ::: ::: {.column width=\"0%\"} ::: :::","title":"MLE"},{"location":"slides/week-6/#mle_1","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\max \\prod \\limits_{i = 1}^{n} P(y^{(i)}) \\prod \\limits_{j = 1}^{m} P(x_{j}^{(i)}\\ |\\ y^{(i)}) $$ ::: ::: {.column width=\"0%\"} ::: :::","title":"MLE"},{"location":"slides/week-6/#mle_2","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\max \\sum \\limits_{i = 1}^{n} \\log P(y^{(i)}) \\sum \\limits_{j = 1}^{m} \\log P(x_{j}^{(i)}\\ |\\ y^{(i)}) $$ ::: ::: {.column width=\"0%\"} ::: :::","title":"MLE"},{"location":"slides/week-6/#mle_3","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\max \\sum \\limits_{i = 1}^{n} \\log P(y^{(i)}) \\sum \\limits_{j = 1}^{m} -\\cfrac{1}{2} \\log (2 \\pi \\sigma_j^2) - \\cfrac{1}{2} \\left( \\cfrac{x_j^{(i)} - \\mu_j}{\\sigma_j} \\right)^2 $$ ::: ::: {.column width=\"0%\"} ::: :::","title":"MLE"},{"location":"slides/week-6/#mle_4","text":"::: {.columns align=center} ::: {.column width=\"40%\"} $$ \\hat{\\mu} {jc} = \\cfrac{\\sum \\limits ^{n} x_j^{(i)} \\mathcal{1}{y^{(i)} = c}}{\\sum \\limits_{i = 1}^{n} \\mathcal{1}{y^{(i)} = c}} $$ ::: ::: {.column width=\"60%\"} $$ \\sigma_{jc}^2 = \\cfrac{\\sum \\limits_{1 = 1}^{n} (x_j^{(i)} - \\hat{\\mu} {jc})^2 \\mathcal{1}{y^{(i)} = c}}{\\sum \\limits ^{n} \\mathcal{1}{y^{(i)} = c}} $$ ::: :::","title":"MLE"},{"location":"slides/week-6/#mle_5","text":"::: {.columns align=center} ::: {.column width=\"40%\"} $$ \\boldsymbol{\\hat{\\mu} c} = \\cfrac{\\sum \\limits ^{n} \\mathbf{x^{(i)}}\\mathcal{1}{y^{(i)} = c}}{\\sum \\limits_{i = 1}^{n}\\mathcal{1}{y^{(i)} = c}} $$ ::: ::: {.column width=\"60%\"} $$ \\text{diag}(\\mathbf{\\Sigma_c}) = \\cfrac{\\sum \\limits_{i = 1}^{n} (\\mathbf{x^{(i)} - \\boldsymbol{\\hat{\\mu}} {c}}) \\odot (\\mathbf{x^{(i)} - \\boldsymbol{\\hat{\\mu}} }) \\mathcal{1}{y^{(i)} = c}}{\\sum \\limits_{i = 1}^{n}\\mathcal{1}{y^{(i)} = c}} $$ ::: :::","title":"MLE"},{"location":"slides/week-6/#numpy","text":"::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: :::","title":"NumPy"},{"location":"slides/week-6/#numpy_1","text":"::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: :::","title":"NumPy"},{"location":"slides/week-6/#inference","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\hat{y} = \\arg \\max_{c} P(y = c\\ |\\ x) $$ ::: ::: {.column width=\"50%\"} ::: :::","title":"Inference"},{"location":"slides/week-6/#inference_1","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\hat{y} = \\arg \\max_{c} P(y = c) P(x\\ |\\ y = c) $$ ::: ::: {.column width=\"50%\"} ::: :::","title":"Inference"},{"location":"slides/week-6/#inference_2","text":"::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"50%\"} ::: :::","title":"Inference"},{"location":"slides/week-8/lecture_1/","text":"Course Outline \u00b6 Linear regression Least square classification Perceptron Logistic regression Naive Bayes Softmax regression Support Vector Machines (SVM) Decision trees Ensemble techniques K-means clustering Artificial Neural Networks Lecture Outline \u00b6 ::: incremental Motivation Geometry Hard-margin SVM Formulation Optimization (recap) Optimization Soft-margin SVM Approximate solution ::: Story so far \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: ::: Story so far \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: ::: Story so far \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} ::: incremental Least square classification Perceptron Logistic regression ::: ::: ::: Story so far \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} Least square classification Perceptron Logistic regression What is a common among all these models? ::: ::: Story so far \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} Least square classification Perceptron Logistic regression What is a common among all these models? ::: ::: Boundaries \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} ::: ::: Boundaries \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: ::: Boundaries \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: ::: Boundaries \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: ::: \"Best\" Boundary? \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: ::: \"Best\" Boundary? \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: ::: \"Best\" Boundary? \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: ::: \"Best\" Boundary? \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: ::: \"Best\" Boundary? \u00b6 ::: {.columns align=left} ::: {.column width=\"100%\"} ::: incremental A decision boundary that is \"pointo-phobic\" is a good one. Stay away from data-points of either class. The most pointo-phobic boundary is the best one. The \"middle path\". ::: ::: ::: {.column width=\"0%\"} ::: :::","title":"Support Vector Machines"},{"location":"slides/week-8/lecture_1/#course-outline","text":"Linear regression Least square classification Perceptron Logistic regression Naive Bayes Softmax regression Support Vector Machines (SVM) Decision trees Ensemble techniques K-means clustering Artificial Neural Networks","title":"Course Outline"},{"location":"slides/week-8/lecture_1/#lecture-outline","text":"::: incremental Motivation Geometry Hard-margin SVM Formulation Optimization (recap) Optimization Soft-margin SVM Approximate solution :::","title":"Lecture Outline"},{"location":"slides/week-8/lecture_1/#story-so-far","text":"::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: :::","title":"Story so far"},{"location":"slides/week-8/lecture_1/#story-so-far_1","text":"::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: :::","title":"Story so far"},{"location":"slides/week-8/lecture_1/#story-so-far_2","text":"::: {.columns align=left} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} ::: incremental Least square classification Perceptron Logistic regression ::: ::: :::","title":"Story so far"},{"location":"slides/week-8/lecture_1/#story-so-far_3","text":"::: {.columns align=left} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} Least square classification Perceptron Logistic regression What is a common among all these models? ::: :::","title":"Story so far"},{"location":"slides/week-8/lecture_1/#story-so-far_4","text":"::: {.columns align=left} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} Least square classification Perceptron Logistic regression What is a common among all these models? ::: :::","title":"Story so far"},{"location":"slides/week-8/lecture_1/#boundaries","text":"::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} ::: :::","title":"Boundaries"},{"location":"slides/week-8/lecture_1/#boundaries_1","text":"::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: :::","title":"Boundaries"},{"location":"slides/week-8/lecture_1/#boundaries_2","text":"::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: :::","title":"Boundaries"},{"location":"slides/week-8/lecture_1/#boundaries_3","text":"::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: :::","title":"Boundaries"},{"location":"slides/week-8/lecture_1/#best-boundary","text":"::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: :::","title":"\"Best\" Boundary?"},{"location":"slides/week-8/lecture_1/#best-boundary_1","text":"::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: :::","title":"\"Best\" Boundary?"},{"location":"slides/week-8/lecture_1/#best-boundary_2","text":"::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: :::","title":"\"Best\" Boundary?"},{"location":"slides/week-8/lecture_1/#best-boundary_3","text":"::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: :::","title":"\"Best\" Boundary?"},{"location":"slides/week-8/lecture_1/#best-boundary_4","text":"::: {.columns align=left} ::: {.column width=\"100%\"} ::: incremental A decision boundary that is \"pointo-phobic\" is a good one. Stay away from data-points of either class. The most pointo-phobic boundary is the best one. The \"middle path\". ::: ::: ::: {.column width=\"0%\"} ::: :::","title":"\"Best\" Boundary?"},{"location":"slides/week-8/lecture_2/","text":"Course Outline \u00b6 Linear regression Least square classification Perceptron Logistic regression Naive Bayes Softmax regression Support Vector Machines (SVM) Decision trees Ensemble techniques K-means clustering Artificial Neural Networks Lecture Outline \u00b6 ::: incremental Motivation Geometry Hard-margin SVM Formulation Optimization (recap) Optimization Soft-margin SVM Approximate solution ::: Geometry \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} ::: ::: Geometry \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} $$ w_0 \\cdot 1 + w_1 \\cdot x_1 + w_2 \\cdot x_2 = 0 $$ ::: ::: Geometry \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} $$ w_0 + w_1 x_1 + w_2 x_2 = 0 $$ ::: ::: Geometry \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} $$ w_1 x_1 + w_2 x_2 + b = 0 $$ ::: ::: Geometry \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} $$ w^T x + b = 0 $$ ::: ::: Geometry \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} ::: incremental If we have \\(m\\) features: \\(x \\in \\mathbb{R}^m\\) \\(w \\in \\mathbb{R}^m\\) \\(b \\in \\mathbb{R}\\) \\(w^Tx + b = 0\\) is a hyperplane in \\(\\mathbb{R}^m\\) ::: ::: ::: Geometry \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} ::: incremental If \\(w^Tx + b = 0\\) is a hyperplane, then: \\((2w)^Tx + 2b = 0\\) \\((3w)^Tx + 3b = 0\\) \\((kw)^Tx + kb = 0\\) , where \\(k \\neq 0\\) For a given hyperplane, \\((w, b)\\) is not unique ::: ::: ::: Distance \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} ::: ::: Distance \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} ::: ::: Distance \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} ::: ::: Distance \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} $$ \\begin{array}[c|c|c] & w^Tx + b & = \\qquad \\qquad \\qquad \\qquad\\ & \\end{array} $$ ::: ::: Distance \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} $$ \\begin{array}[c|c|c] & w^Tx + b & = & w^T(x - x_0 + x_0) + b\\\\ \\end{array} $$ ::: ::: Distance \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} $$ \\begin{array}[c|c|c] & w^Tx + b & = & w^T(x - x_0 + x_0) + b\\\\ & = & w^T(x - x_0) + w^T x_0 + b &\\\\ \\end{array} $$ ::: ::: Distance \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} $$ \\begin{array}[c|c|c] & w^Tx + b & = & w^T(x - x_0 + x_0) + b\\\\ & = & w^T(x - x_0) + w^T x_0 + b &\\\\ & = & w^T(x - x_0) \\end{array} $$ ::: ::: Distance \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} $$ \\begin{array}[c|c|c] & w^Tx + b & = & w^T(x - x_0 + x_0) + b\\\\ & = & w^T(x - x_0) + w^T x_0 + b &\\\\ & = & w^T(x - x_0)\\\\ & = & ||w|| \\cdot ||x - x_0||\\\\ \\end{array} $$ ::: ::: Distance \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} $$ \\begin{array}[c|c|c] & w^Tx + b & = & w^T(x - x_0 + x_0) + b\\\\ & = & w^T(x - x_0) + w^T x_0 + b &\\\\ & = & w^T(x - x_0)\\\\ & = & ||w|| \\cdot ||x - x_0||\\\\ ||x - x_0|| & = & \\cfrac{w^Tx + b}{||w||} \\end{array} $$ ::: ::: Distance \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} $$ \\begin{array}[c|c|c] & w^Tx + b & = & w^T(x - x_0 + x_0) + b\\\\ & = & w^T(x - x_0) + w^T x_0 + b &\\\\ & = & w^T(x - x_0)\\\\ & = & ||w|| \\cdot ||x - x_0||\\\\ ||x - x_0|| & = & \\cfrac{w^Tx + b}{||w||} \\end{array} $$ NOTE : If \\(x\\) is on the other side of the boundary, we have to introduce a negative sign. ::: ::: Distance \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} Distance of a point \\(x\\) from the hyperplane specified by \\((w, b)\\) is given by: $$ \\huge \\left | \\cfrac{w^Tx + b}{||w||} \\right | $$ ::: :::","title":"Support Vector Machines"},{"location":"slides/week-8/lecture_2/#course-outline","text":"Linear regression Least square classification Perceptron Logistic regression Naive Bayes Softmax regression Support Vector Machines (SVM) Decision trees Ensemble techniques K-means clustering Artificial Neural Networks","title":"Course Outline"},{"location":"slides/week-8/lecture_2/#lecture-outline","text":"::: incremental Motivation Geometry Hard-margin SVM Formulation Optimization (recap) Optimization Soft-margin SVM Approximate solution :::","title":"Lecture Outline"},{"location":"slides/week-8/lecture_2/#geometry","text":"::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} ::: :::","title":"Geometry"},{"location":"slides/week-8/lecture_2/#geometry_1","text":"::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} $$ w_0 \\cdot 1 + w_1 \\cdot x_1 + w_2 \\cdot x_2 = 0 $$ ::: :::","title":"Geometry"},{"location":"slides/week-8/lecture_2/#geometry_2","text":"::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} $$ w_0 + w_1 x_1 + w_2 x_2 = 0 $$ ::: :::","title":"Geometry"},{"location":"slides/week-8/lecture_2/#geometry_3","text":"::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} $$ w_1 x_1 + w_2 x_2 + b = 0 $$ ::: :::","title":"Geometry"},{"location":"slides/week-8/lecture_2/#geometry_4","text":"::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} $$ w^T x + b = 0 $$ ::: :::","title":"Geometry"},{"location":"slides/week-8/lecture_2/#geometry_5","text":"::: {.columns align=left} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} ::: incremental If we have \\(m\\) features: \\(x \\in \\mathbb{R}^m\\) \\(w \\in \\mathbb{R}^m\\) \\(b \\in \\mathbb{R}\\) \\(w^Tx + b = 0\\) is a hyperplane in \\(\\mathbb{R}^m\\) ::: ::: :::","title":"Geometry"},{"location":"slides/week-8/lecture_2/#geometry_6","text":"::: {.columns align=left} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} ::: incremental If \\(w^Tx + b = 0\\) is a hyperplane, then: \\((2w)^Tx + 2b = 0\\) \\((3w)^Tx + 3b = 0\\) \\((kw)^Tx + kb = 0\\) , where \\(k \\neq 0\\) For a given hyperplane, \\((w, b)\\) is not unique ::: ::: :::","title":"Geometry"},{"location":"slides/week-8/lecture_2/#distance","text":"::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} ::: :::","title":"Distance"},{"location":"slides/week-8/lecture_2/#distance_1","text":"::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} ::: :::","title":"Distance"},{"location":"slides/week-8/lecture_2/#distance_2","text":"::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} ::: :::","title":"Distance"},{"location":"slides/week-8/lecture_2/#distance_3","text":"::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} $$ \\begin{array}[c|c|c] & w^Tx + b & = \\qquad \\qquad \\qquad \\qquad\\ & \\end{array} $$ ::: :::","title":"Distance"},{"location":"slides/week-8/lecture_2/#distance_4","text":"::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} $$ \\begin{array}[c|c|c] & w^Tx + b & = & w^T(x - x_0 + x_0) + b\\\\ \\end{array} $$ ::: :::","title":"Distance"},{"location":"slides/week-8/lecture_2/#distance_5","text":"::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} $$ \\begin{array}[c|c|c] & w^Tx + b & = & w^T(x - x_0 + x_0) + b\\\\ & = & w^T(x - x_0) + w^T x_0 + b &\\\\ \\end{array} $$ ::: :::","title":"Distance"},{"location":"slides/week-8/lecture_2/#distance_6","text":"::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} $$ \\begin{array}[c|c|c] & w^Tx + b & = & w^T(x - x_0 + x_0) + b\\\\ & = & w^T(x - x_0) + w^T x_0 + b &\\\\ & = & w^T(x - x_0) \\end{array} $$ ::: :::","title":"Distance"},{"location":"slides/week-8/lecture_2/#distance_7","text":"::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} $$ \\begin{array}[c|c|c] & w^Tx + b & = & w^T(x - x_0 + x_0) + b\\\\ & = & w^T(x - x_0) + w^T x_0 + b &\\\\ & = & w^T(x - x_0)\\\\ & = & ||w|| \\cdot ||x - x_0||\\\\ \\end{array} $$ ::: :::","title":"Distance"},{"location":"slides/week-8/lecture_2/#distance_8","text":"::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} $$ \\begin{array}[c|c|c] & w^Tx + b & = & w^T(x - x_0 + x_0) + b\\\\ & = & w^T(x - x_0) + w^T x_0 + b &\\\\ & = & w^T(x - x_0)\\\\ & = & ||w|| \\cdot ||x - x_0||\\\\ ||x - x_0|| & = & \\cfrac{w^Tx + b}{||w||} \\end{array} $$ ::: :::","title":"Distance"},{"location":"slides/week-8/lecture_2/#distance_9","text":"::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} $$ \\begin{array}[c|c|c] & w^Tx + b & = & w^T(x - x_0 + x_0) + b\\\\ & = & w^T(x - x_0) + w^T x_0 + b &\\\\ & = & w^T(x - x_0)\\\\ & = & ||w|| \\cdot ||x - x_0||\\\\ ||x - x_0|| & = & \\cfrac{w^Tx + b}{||w||} \\end{array} $$ NOTE : If \\(x\\) is on the other side of the boundary, we have to introduce a negative sign. ::: :::","title":"Distance"},{"location":"slides/week-8/lecture_2/#distance_10","text":"::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} Distance of a point \\(x\\) from the hyperplane specified by \\((w, b)\\) is given by: $$ \\huge \\left | \\cfrac{w^Tx + b}{||w||} \\right | $$ ::: :::","title":"Distance"},{"location":"slides/week-8/lecture_3/","text":"Course Outline \u00b6 Linear regression Least square classification Perceptron Logistic regression Naive Bayes Softmax regression Support Vector Machines (SVM) Decision trees Ensemble techniques K-means clustering Artificial Neural Networks Lecture Outline \u00b6 ::: incremental Motivation Geometry Hard-margin SVM Formulation Optimization (recap) Optimization Soft-margin SVM Approximate solution ::: Pointo-phobic\u2026 \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} Stay away from data-points of either class ::: ::: {.column width=\"0%\"} ::: ::: Margin \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"50%\"} ::: ::: Margin \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"50%\"} ::: ::: Margin \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"50%\"} ::: ::: Margin \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} Margin Distance of the closest point from the decision boundary ::: ::: Margin \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} Margin Distance of the closest point from the decision boundary ::: ::: Margin \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} Margin Distance of the closest point from the decision boundary As the decision boundary changes, the margin also changes. ::: ::: Max-Margin \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} The best classifier is the one with maximum margin. ::: ::: Hard-margin SVM \u00b6 ::: {.columns align=left} ::: {.column width=\"100%\"} ::: incremental Assumption \u2014 Data-points are linearly separable Hard decisions: all points of a class strictly belong to one side of the boundary Find the boundary that has the maximum margin We can formulate this as an optimization problem ::: ::: ::: {.column width=\"50%\"} ::: ::: Setup \u00b6 ::: {.columns align=left} ::: {.column width=\"100%\"} ::: incremental Binary classification problem \\(n\\) data-points, \\(m\\) features \\(x_i \\in \\mathbb{R}^m\\) , feature vector of the \\(i^{th}\\) point \\(y_i \\in \\{-1, 1\\}\\) , label of the \\(i^{th}\\) point \\(D = \\big \\{(x_i, y_i) \\big\\}_{i = 1}^{n}\\) ::: ::: ::: {.column width=\"50%\"} ::: ::: Formulation \u00b6 ::: {.columns align=left} ::: {.column width=\"40%\"} Distance of the \\(i^{th}\\) data-point from the decision boundary ::: ::: {.column width=\"60%\"} ::: ::: Formulation \u00b6 ::: {.columns align=left} ::: {.column width=\"40%\"} Distance of the \\(i^{th}\\) data-point from the decision boundary ::: ::: {.column width=\"60%\"} $$ d_i = \\left| \\cfrac{w^Tx_i + b}{||w||} \\right| $$ ::: ::: Formulation \u00b6 ::: {.columns align=left} ::: {.column width=\"40%\"} ::: ::: {.column width=\"60%\"} $$ d_i = \\left| \\cfrac{w^Tx_i + b}{||w||} \\right| $$ ::: ::: Formulation \u00b6 ::: {.columns align=left} ::: {.column width=\"40%\"} ::: ::: {.column width=\"60%\"} $$ d_i = \\cfrac{y_i(w^Tx_i + b)}{||w||} $$ ::: ::: Formulation \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} Margin : \\(d\\) Distance of the closest point from the boundary ::: ::: {.column width=\"50%\"} ::: ::: Formulation \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} Margin : \\(d\\) Distance of the closest point from the boundary ::: ::: {.column width=\"50%\"} $$ d = \\min \\limits_{i} \\quad d_i $$ ::: ::: Formulation \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} Margin : \\(d\\) Distance of the closest point from the boundary ::: ::: {.column width=\"50%\"} $$ d = \\min \\limits_{i} \\quad \\cfrac{y_i(w^Tx_i + b)}{||w||} $$ ::: ::: Formulation \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} \\(x_j\\) is closest to the boundary ::: ::: {.column width=\"50%\"} $$ d = \\cfrac{y_j(w^Tx_j + b)}{||w||} $$ ::: ::: Formulation \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} Recall: \\((kw, kb)\\) and \\((w, b)\\) are the same decision boundary for \\(k \\neq 0\\) ::: ::: {.column width=\"50%\"} $$ d = \\cfrac{y_j(w^Tx_j + b)}{||w||} $$ ::: ::: Formulation \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} Recall: \\((kw, kb)\\) and \\((w, b)\\) are the same decision boundary for \\(k \\neq 0\\) Choose \\((w, b)\\) s.t \\[ y_j(w^T x_j + b) = 1 \\] ::: ::: {.column width=\"50%\"} $$ d = \\cfrac{y_j(w^Tx_j + b)}{||w||} $$ ::: ::: Formulation \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} Recall: \\((kw, kb)\\) and \\((w, b)\\) are the same decision boundary for \\(k \\neq 0\\) Choose \\((w, b)\\) s.t \\[ y_j(w^T x_j + b) = 1 \\] ::: ::: {.column width=\"50%\"} $$ d = \\cfrac{1}{||w||} $$ ::: ::: Formulation \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} Margin of the boundary \\((w, b)\\) for \\(D = \\big \\{(x_i, y_i) \\big\\}_{i = 1}^{n}\\) is given by: $$ d = \\cfrac{1}{||w||} $$ Where, \\(x_j\\) is the closest point to it with \\(y_j(w^T x_j + b) = 1\\) ::: ::: {.column width=\"50%\"} ::: ::: Formulation \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} If \\(x_j\\) is the closest point with \\(y_j(w^T x_j + b) = 1\\) , then what can we say about the other points in the dataset? ::: ::: {.column width=\"50%\"} ::: ::: Formulation \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} If \\(x_j\\) is the closest point with \\(y_j(w^T x_j + b) = 1\\) , then: $$ y_i(w^T x_i + b) \\geq 1, \\quad 1 \\leq i \\leq n $$ ::: ::: {.column width=\"50%\"} ::: ::: Formulation \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\max \\limits_{w, b} \\quad \\cfrac{1}{||w||} $$ ::: ::: {.column width=\"0%\"} ::: ::: Formulation \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\max \\limits_{w, b} \\quad \\cfrac{1}{||w||} $$ subject to: ::: ::: {.column width=\"0%\"} ::: ::: Formulation \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\max \\limits_{w, b} \\quad \\cfrac{1}{||w||} $$ subject to: $$ y_i(w^T x_i + b) \\geq 1, \\quad 1 \\leq i \\leq n $$ ::: ::: {.column width=\"0%\"} ::: ::: Formulation \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\min \\limits_{w, b} \\quad \\cfrac{||w||^2}{2} $$ subject to: $$ y_i(w^T x_i + b) \\geq 1, \\quad 1 \\leq i \\leq n $$ ::: ::: {.column width=\"0%\"} ::: ::: Formulation \u00b6 ::: {.columns align=left} ::: {.column width=\"40%\"} $$ \\min \\limits_{w, b} \\quad \\cfrac{||w||^2}{2} $$ subject to: $$ y_i(w^T x_i + b) \\geq 1, \\quad 1 \\leq i \\leq n $$ ::: ::: {.column width=\"60%\"} ::: :::","title":"Support Vector Machines"},{"location":"slides/week-8/lecture_3/#course-outline","text":"Linear regression Least square classification Perceptron Logistic regression Naive Bayes Softmax regression Support Vector Machines (SVM) Decision trees Ensemble techniques K-means clustering Artificial Neural Networks","title":"Course Outline"},{"location":"slides/week-8/lecture_3/#lecture-outline","text":"::: incremental Motivation Geometry Hard-margin SVM Formulation Optimization (recap) Optimization Soft-margin SVM Approximate solution :::","title":"Lecture Outline"},{"location":"slides/week-8/lecture_3/#pointo-phobic","text":"::: {.columns align=center} ::: {.column width=\"100%\"} Stay away from data-points of either class ::: ::: {.column width=\"0%\"} ::: :::","title":"Pointo-phobic\u2026"},{"location":"slides/week-8/lecture_3/#margin","text":"::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"50%\"} ::: :::","title":"Margin"},{"location":"slides/week-8/lecture_3/#margin_1","text":"::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"50%\"} ::: :::","title":"Margin"},{"location":"slides/week-8/lecture_3/#margin_2","text":"::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"50%\"} ::: :::","title":"Margin"},{"location":"slides/week-8/lecture_3/#margin_3","text":"::: {.columns align=left} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} Margin Distance of the closest point from the decision boundary ::: :::","title":"Margin"},{"location":"slides/week-8/lecture_3/#margin_4","text":"::: {.columns align=left} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} Margin Distance of the closest point from the decision boundary ::: :::","title":"Margin"},{"location":"slides/week-8/lecture_3/#margin_5","text":"::: {.columns align=left} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} Margin Distance of the closest point from the decision boundary As the decision boundary changes, the margin also changes. ::: :::","title":"Margin"},{"location":"slides/week-8/lecture_3/#max-margin","text":"::: {.columns align=left} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} The best classifier is the one with maximum margin. ::: :::","title":"Max-Margin"},{"location":"slides/week-8/lecture_3/#hard-margin-svm","text":"::: {.columns align=left} ::: {.column width=\"100%\"} ::: incremental Assumption \u2014 Data-points are linearly separable Hard decisions: all points of a class strictly belong to one side of the boundary Find the boundary that has the maximum margin We can formulate this as an optimization problem ::: ::: ::: {.column width=\"50%\"} ::: :::","title":"Hard-margin SVM"},{"location":"slides/week-8/lecture_3/#setup","text":"::: {.columns align=left} ::: {.column width=\"100%\"} ::: incremental Binary classification problem \\(n\\) data-points, \\(m\\) features \\(x_i \\in \\mathbb{R}^m\\) , feature vector of the \\(i^{th}\\) point \\(y_i \\in \\{-1, 1\\}\\) , label of the \\(i^{th}\\) point \\(D = \\big \\{(x_i, y_i) \\big\\}_{i = 1}^{n}\\) ::: ::: ::: {.column width=\"50%\"} ::: :::","title":"Setup"},{"location":"slides/week-8/lecture_3/#formulation","text":"::: {.columns align=left} ::: {.column width=\"40%\"} Distance of the \\(i^{th}\\) data-point from the decision boundary ::: ::: {.column width=\"60%\"} ::: :::","title":"Formulation"},{"location":"slides/week-8/lecture_3/#formulation_1","text":"::: {.columns align=left} ::: {.column width=\"40%\"} Distance of the \\(i^{th}\\) data-point from the decision boundary ::: ::: {.column width=\"60%\"} $$ d_i = \\left| \\cfrac{w^Tx_i + b}{||w||} \\right| $$ ::: :::","title":"Formulation"},{"location":"slides/week-8/lecture_3/#formulation_2","text":"::: {.columns align=left} ::: {.column width=\"40%\"} ::: ::: {.column width=\"60%\"} $$ d_i = \\left| \\cfrac{w^Tx_i + b}{||w||} \\right| $$ ::: :::","title":"Formulation"},{"location":"slides/week-8/lecture_3/#formulation_3","text":"::: {.columns align=left} ::: {.column width=\"40%\"} ::: ::: {.column width=\"60%\"} $$ d_i = \\cfrac{y_i(w^Tx_i + b)}{||w||} $$ ::: :::","title":"Formulation"},{"location":"slides/week-8/lecture_3/#formulation_4","text":"::: {.columns align=left} ::: {.column width=\"50%\"} Margin : \\(d\\) Distance of the closest point from the boundary ::: ::: {.column width=\"50%\"} ::: :::","title":"Formulation"},{"location":"slides/week-8/lecture_3/#formulation_5","text":"::: {.columns align=left} ::: {.column width=\"50%\"} Margin : \\(d\\) Distance of the closest point from the boundary ::: ::: {.column width=\"50%\"} $$ d = \\min \\limits_{i} \\quad d_i $$ ::: :::","title":"Formulation"},{"location":"slides/week-8/lecture_3/#formulation_6","text":"::: {.columns align=left} ::: {.column width=\"50%\"} Margin : \\(d\\) Distance of the closest point from the boundary ::: ::: {.column width=\"50%\"} $$ d = \\min \\limits_{i} \\quad \\cfrac{y_i(w^Tx_i + b)}{||w||} $$ ::: :::","title":"Formulation"},{"location":"slides/week-8/lecture_3/#formulation_7","text":"::: {.columns align=left} ::: {.column width=\"50%\"} \\(x_j\\) is closest to the boundary ::: ::: {.column width=\"50%\"} $$ d = \\cfrac{y_j(w^Tx_j + b)}{||w||} $$ ::: :::","title":"Formulation"},{"location":"slides/week-8/lecture_3/#formulation_8","text":"::: {.columns align=left} ::: {.column width=\"50%\"} Recall: \\((kw, kb)\\) and \\((w, b)\\) are the same decision boundary for \\(k \\neq 0\\) ::: ::: {.column width=\"50%\"} $$ d = \\cfrac{y_j(w^Tx_j + b)}{||w||} $$ ::: :::","title":"Formulation"},{"location":"slides/week-8/lecture_3/#formulation_9","text":"::: {.columns align=left} ::: {.column width=\"50%\"} Recall: \\((kw, kb)\\) and \\((w, b)\\) are the same decision boundary for \\(k \\neq 0\\) Choose \\((w, b)\\) s.t \\[ y_j(w^T x_j + b) = 1 \\] ::: ::: {.column width=\"50%\"} $$ d = \\cfrac{y_j(w^Tx_j + b)}{||w||} $$ ::: :::","title":"Formulation"},{"location":"slides/week-8/lecture_3/#formulation_10","text":"::: {.columns align=left} ::: {.column width=\"50%\"} Recall: \\((kw, kb)\\) and \\((w, b)\\) are the same decision boundary for \\(k \\neq 0\\) Choose \\((w, b)\\) s.t \\[ y_j(w^T x_j + b) = 1 \\] ::: ::: {.column width=\"50%\"} $$ d = \\cfrac{1}{||w||} $$ ::: :::","title":"Formulation"},{"location":"slides/week-8/lecture_3/#formulation_11","text":"::: {.columns align=center} ::: {.column width=\"100%\"} Margin of the boundary \\((w, b)\\) for \\(D = \\big \\{(x_i, y_i) \\big\\}_{i = 1}^{n}\\) is given by: $$ d = \\cfrac{1}{||w||} $$ Where, \\(x_j\\) is the closest point to it with \\(y_j(w^T x_j + b) = 1\\) ::: ::: {.column width=\"50%\"} ::: :::","title":"Formulation"},{"location":"slides/week-8/lecture_3/#formulation_12","text":"::: {.columns align=center} ::: {.column width=\"100%\"} If \\(x_j\\) is the closest point with \\(y_j(w^T x_j + b) = 1\\) , then what can we say about the other points in the dataset? ::: ::: {.column width=\"50%\"} ::: :::","title":"Formulation"},{"location":"slides/week-8/lecture_3/#formulation_13","text":"::: {.columns align=center} ::: {.column width=\"100%\"} If \\(x_j\\) is the closest point with \\(y_j(w^T x_j + b) = 1\\) , then: $$ y_i(w^T x_i + b) \\geq 1, \\quad 1 \\leq i \\leq n $$ ::: ::: {.column width=\"50%\"} ::: :::","title":"Formulation"},{"location":"slides/week-8/lecture_3/#formulation_14","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\max \\limits_{w, b} \\quad \\cfrac{1}{||w||} $$ ::: ::: {.column width=\"0%\"} ::: :::","title":"Formulation"},{"location":"slides/week-8/lecture_3/#formulation_15","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\max \\limits_{w, b} \\quad \\cfrac{1}{||w||} $$ subject to: ::: ::: {.column width=\"0%\"} ::: :::","title":"Formulation"},{"location":"slides/week-8/lecture_3/#formulation_16","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\max \\limits_{w, b} \\quad \\cfrac{1}{||w||} $$ subject to: $$ y_i(w^T x_i + b) \\geq 1, \\quad 1 \\leq i \\leq n $$ ::: ::: {.column width=\"0%\"} ::: :::","title":"Formulation"},{"location":"slides/week-8/lecture_3/#formulation_17","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\min \\limits_{w, b} \\quad \\cfrac{||w||^2}{2} $$ subject to: $$ y_i(w^T x_i + b) \\geq 1, \\quad 1 \\leq i \\leq n $$ ::: ::: {.column width=\"0%\"} ::: :::","title":"Formulation"},{"location":"slides/week-8/lecture_3/#formulation_18","text":"::: {.columns align=left} ::: {.column width=\"40%\"} $$ \\min \\limits_{w, b} \\quad \\cfrac{||w||^2}{2} $$ subject to: $$ y_i(w^T x_i + b) \\geq 1, \\quad 1 \\leq i \\leq n $$ ::: ::: {.column width=\"60%\"} ::: :::","title":"Formulation"},{"location":"slides/week-8/lecture_4/","text":"Course Outline \u00b6 Linear regression Least square classification Perceptron Logistic regression Naive Bayes Softmax regression Support Vector Machines (SVM) Decision trees Ensemble techniques K-means clustering Artificial Neural Networks Lecture Outline \u00b6 ::: incremental Motivation Geometry Hard-margin SVM Formulation Optimization (recap) Optimization Soft-margin SVM Approximate solution ::: Constrained Optimization \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} $$ \\min \\limits_{x} \\quad x_1^2 + x_2^2 $$ subject to $$ (x_1 - 5)^2 + (x_2 - 7)^2 \\leq 5 $$ ::: ::: General Form \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} $$ \\min \\limits_{x} \\quad f(x) $$ subject to $$ g(x) \\leq 0 $$ ::: ::: Active constraint \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} ::: ::: Active constraint \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} ::: ::: Active constraint \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} ::: ::: Active constraint \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} ::: ::: Active constraint \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} ::: ::: Active constraint \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} ::: ::: Active constraint \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} ::: ::: Active constraint \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} $$ \\nabla f(x) = -\\lambda \\nabla g(x), \\quad \\lambda > 0 $$ ::: ::: Active constraint \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} $$ \\nabla f(x) + \\lambda \\nabla g(x) = 0, \\quad \\lambda > 0 $$ ::: ::: Active constraint \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} $$ \\nabla \\left( f(x) + \\lambda g(x) \\right) = 0, \\quad \\lambda > 0 $$ ::: ::: Active constraint \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} ::: incremental \\[ \\nabla L(x, \\lambda) = 0 \\] \\(L(x, \\lambda) = f(x) + \\lambda g(x)\\) \\(\\lambda > 0\\) \\(g(x) = 0\\) Active constraint ::: ::: ::: Inactive constraint \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} ::: ::: Inactive constraint \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} $$ \\nabla f(x) = 0 $$ ::: ::: Inactive constraint \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} ::: incremental $$ \\nabla L(x, \\lambda) = 0 $$ \\(L(x, \\lambda) = f(x) + \\lambda g(x)\\) \\(\\lambda = 0\\) \\(g(x) < 0\\) Inactive constraint ::: ::: ::: Towards KKT \u00b6 \\[ L(x, \\lambda) = f(x) + \\lambda g(x) \\] \\[ \\nabla L(x, \\lambda) = 0 \\] ::: {.columns align=left} ::: {.column width=\"50%\"} \\(\\lambda > 0\\) \\(g(x) = 0\\) Active constraint ::: ::: {.column width=\"50%\"} \\(\\lambda = 0\\) \\(g(x) < 0\\) Inactive constraint ::: ::: Towards KKT \u00b6 \\[ L(x, \\lambda) = f(x) + \\lambda g(x) \\] \\[ \\nabla L(x, \\lambda) = 0 \\] ::: {.columns align=left} ::: {.column width=\"50%\"} \\(\\lambda > 0\\) \\(g(x) = 0\\) Active constraint ::: ::: {.column width=\"50%\"} \\(\\lambda = 0\\) \\(g(x) < 0\\) Inactive constraint ::: $$ \\lambda g(x) = 0 $$ ::: KKT \u00b6 Lagrangian $$ L(x, \\lambda) = f(x) + \\lambda g(x) $$ Condition-1 $$ \\nabla L(x, \\lambda) = 0 $$ Condition-2 $$ g(x) \\leq 0 $$ Condition-3 $$ \\lambda \\geq 0 $$ Condition-4 $$ \\lambda g(x) = 0 $$ KKT - Template \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} $$ \\min \\limits_{x} \\quad f(x) $$ subject to $$ g(x) \\leq 0 $$ ::: ::: {.column width=\"50%\"} Lagrangian $$ L(x, \\lambda) = f(x) + \\lambda g(x) $$ Condition-1 $$ \\nabla L(x, \\lambda) = 0 $$ Condition-2 $$ g(x) \\leq 0 $$ Condition-3 $$ \\lambda \\geq 0 $$ Condition-4 $$ \\lambda g(x) = 0 $$ ::: :::","title":"Support Vector Machines"},{"location":"slides/week-8/lecture_4/#course-outline","text":"Linear regression Least square classification Perceptron Logistic regression Naive Bayes Softmax regression Support Vector Machines (SVM) Decision trees Ensemble techniques K-means clustering Artificial Neural Networks","title":"Course Outline"},{"location":"slides/week-8/lecture_4/#lecture-outline","text":"::: incremental Motivation Geometry Hard-margin SVM Formulation Optimization (recap) Optimization Soft-margin SVM Approximate solution :::","title":"Lecture Outline"},{"location":"slides/week-8/lecture_4/#constrained-optimization","text":"::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} $$ \\min \\limits_{x} \\quad x_1^2 + x_2^2 $$ subject to $$ (x_1 - 5)^2 + (x_2 - 7)^2 \\leq 5 $$ ::: :::","title":"Constrained Optimization"},{"location":"slides/week-8/lecture_4/#general-form","text":"::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} $$ \\min \\limits_{x} \\quad f(x) $$ subject to $$ g(x) \\leq 0 $$ ::: :::","title":"General Form"},{"location":"slides/week-8/lecture_4/#active-constraint","text":"::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} ::: :::","title":"Active constraint"},{"location":"slides/week-8/lecture_4/#active-constraint_1","text":"::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} ::: :::","title":"Active constraint"},{"location":"slides/week-8/lecture_4/#active-constraint_2","text":"::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} ::: :::","title":"Active constraint"},{"location":"slides/week-8/lecture_4/#active-constraint_3","text":"::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} ::: :::","title":"Active constraint"},{"location":"slides/week-8/lecture_4/#active-constraint_4","text":"::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} ::: :::","title":"Active constraint"},{"location":"slides/week-8/lecture_4/#active-constraint_5","text":"::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} ::: :::","title":"Active constraint"},{"location":"slides/week-8/lecture_4/#active-constraint_6","text":"::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} ::: :::","title":"Active constraint"},{"location":"slides/week-8/lecture_4/#active-constraint_7","text":"::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} $$ \\nabla f(x) = -\\lambda \\nabla g(x), \\quad \\lambda > 0 $$ ::: :::","title":"Active constraint"},{"location":"slides/week-8/lecture_4/#active-constraint_8","text":"::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} $$ \\nabla f(x) + \\lambda \\nabla g(x) = 0, \\quad \\lambda > 0 $$ ::: :::","title":"Active constraint"},{"location":"slides/week-8/lecture_4/#active-constraint_9","text":"::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} $$ \\nabla \\left( f(x) + \\lambda g(x) \\right) = 0, \\quad \\lambda > 0 $$ ::: :::","title":"Active constraint"},{"location":"slides/week-8/lecture_4/#active-constraint_10","text":"::: {.columns align=left} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} ::: incremental \\[ \\nabla L(x, \\lambda) = 0 \\] \\(L(x, \\lambda) = f(x) + \\lambda g(x)\\) \\(\\lambda > 0\\) \\(g(x) = 0\\) Active constraint ::: ::: :::","title":"Active constraint"},{"location":"slides/week-8/lecture_4/#inactive-constraint","text":"::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} ::: :::","title":"Inactive constraint"},{"location":"slides/week-8/lecture_4/#inactive-constraint_1","text":"::: {.columns align=center} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} $$ \\nabla f(x) = 0 $$ ::: :::","title":"Inactive constraint"},{"location":"slides/week-8/lecture_4/#inactive-constraint_2","text":"::: {.columns align=left} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} ::: incremental $$ \\nabla L(x, \\lambda) = 0 $$ \\(L(x, \\lambda) = f(x) + \\lambda g(x)\\) \\(\\lambda = 0\\) \\(g(x) < 0\\) Inactive constraint ::: ::: :::","title":"Inactive constraint"},{"location":"slides/week-8/lecture_4/#towards-kkt","text":"\\[ L(x, \\lambda) = f(x) + \\lambda g(x) \\] \\[ \\nabla L(x, \\lambda) = 0 \\] ::: {.columns align=left} ::: {.column width=\"50%\"} \\(\\lambda > 0\\) \\(g(x) = 0\\) Active constraint ::: ::: {.column width=\"50%\"} \\(\\lambda = 0\\) \\(g(x) < 0\\) Inactive constraint ::: :::","title":"Towards KKT"},{"location":"slides/week-8/lecture_4/#towards-kkt_1","text":"\\[ L(x, \\lambda) = f(x) + \\lambda g(x) \\] \\[ \\nabla L(x, \\lambda) = 0 \\] ::: {.columns align=left} ::: {.column width=\"50%\"} \\(\\lambda > 0\\) \\(g(x) = 0\\) Active constraint ::: ::: {.column width=\"50%\"} \\(\\lambda = 0\\) \\(g(x) < 0\\) Inactive constraint ::: $$ \\lambda g(x) = 0 $$ :::","title":"Towards KKT"},{"location":"slides/week-8/lecture_4/#kkt","text":"Lagrangian $$ L(x, \\lambda) = f(x) + \\lambda g(x) $$ Condition-1 $$ \\nabla L(x, \\lambda) = 0 $$ Condition-2 $$ g(x) \\leq 0 $$ Condition-3 $$ \\lambda \\geq 0 $$ Condition-4 $$ \\lambda g(x) = 0 $$","title":"KKT"},{"location":"slides/week-8/lecture_4/#kkt-template","text":"::: {.columns align=left} ::: {.column width=\"50%\"} $$ \\min \\limits_{x} \\quad f(x) $$ subject to $$ g(x) \\leq 0 $$ ::: ::: {.column width=\"50%\"} Lagrangian $$ L(x, \\lambda) = f(x) + \\lambda g(x) $$ Condition-1 $$ \\nabla L(x, \\lambda) = 0 $$ Condition-2 $$ g(x) \\leq 0 $$ Condition-3 $$ \\lambda \\geq 0 $$ Condition-4 $$ \\lambda g(x) = 0 $$ ::: :::","title":"KKT - Template"},{"location":"slides/week-8/lecture_5/","text":"Course Outline \u00b6 Linear regression Least square classification Perceptron Logistic regression Naive Bayes Softmax regression Support Vector Machines (SVM) Decision trees Ensemble techniques K-means clustering Artificial Neural Networks Lecture Outline \u00b6 ::: incremental Motivation Geometry Hard-margin SVM Formulation Optimization (recap) Optimization Soft-margin SVM Approximate solution ::: Hard-margin SVM \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\min \\limits_{w, b} \\quad \\cfrac{||w||^2}{2} $$ subject to: $$ y_i(w^Tx_i + b) \\geq 1,\\quad 1 \\leq i \\leq n $$ ::: ::: {.column width=\"50%\"} ::: ::: Hard-margin SVM \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\min \\limits_{w, b} \\quad \\cfrac{||w||^2}{2} $$ subject to: $$ 1 - y_i(w^Tx_i + b) \\leq 0,\\quad 1 \\leq i \\leq n $$ ::: ::: {.column width=\"50%\"} ::: ::: Step-1: Lagrangian \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ L(w, b, \\lambda) = \\cfrac{||w||^2}{2} + \\sum \\limits_{i = 1}^{n} \\lambda_i \\left[ 1 - y_i(w^Tx_i + b)\\right] $$ ::: ::: {.column width=\"50%\"} ::: ::: Step-2: \\(\\nabla L = 0\\) \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\nabla_w L = w - \\sum \\limits_{i = 1}^{n} \\lambda_i y_i x_i = 0 $$ ::: ::: {.column width=\"50%\"} ::: ::: Step-2: \\(\\nabla L = 0\\) \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ w = \\sum \\limits_{i = 1}^{n} \\lambda_i y_i x_i $$ ::: ::: {.column width=\"50%\"} ::: ::: Step-2: \\(\\nabla L = 0\\) \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\nabla_b L =- \\sum \\limits_{i = 1}^{n} \\lambda_i y_i = 0 $$ ::: ::: {.column width=\"50%\"} ::: ::: Step-2: \\(\\nabla L = 0\\) \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\sum \\limits_{i = 1}^{n} \\lambda_i y_i = 0 $$ ::: ::: {.column width=\"50%\"} ::: ::: Step-3: Eliminate \\(w, b\\) \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} $$ L(w, b, \\lambda) = \\cfrac{||w||^2}{2} + \\sum \\limits_{i = 1}^{n} \\lambda_i \\left[ 1 - y_i(w^Tx_i + b)\\right] $$ $$ w = \\sum \\limits_{1 = 1}^{n} \\lambda_i y_i x_i $$ $$ \\sum \\limits_{i = 1}^{n} \\lambda_i y_i = 0 $$ ::: ::: {.column width=\"50%\"} ::: ::: Step-3: Eliminate \\(w, b\\) \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} $$ L(w, b, \\lambda) = \\cfrac{||w||^2}{2} + \\sum \\limits_{i = 1}^{n} \\lambda_i -\\sum \\limits_{i = 1}^{n} \\lambda_i y_i(w^Tx_i) + \\sum \\limits_{i = 1}^{n} \\lambda_i y_i b $$ $$ w = \\sum \\limits_{1 = 1}^{n} \\lambda_i y_i x_i $$ $$ \\sum \\limits_{i = 1}^{n} \\lambda_i y_i = 0 $$ ::: ::: {.column width=\"50%\"} ::: ::: Step-3: Eliminate \\(w, b\\) \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} $$ L(w, b, \\lambda) = \\cfrac{||w||^2}{2} + \\sum \\limits_{i = 1}^{n} \\lambda_i -\\sum \\limits_{i = 1}^{n} \\lambda_i y_i(w^Tx_i) $$ $$ w = \\sum \\limits_{1 = 1}^{n} \\lambda_i y_i x_i $$ $$ \\sum \\limits_{i = 1}^{n} \\lambda_i y_i = 0 $$ ::: ::: {.column width=\"50%\"} ::: ::: Step-3: Eliminate \\(w, b\\) \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} $$ L(w, b, \\lambda) = \\cfrac{||w||^2}{2} + \\sum \\limits_{i = 1}^{n} \\lambda_i -\\sum \\limits_{i = 1}^{n} \\lambda_i y_i(w^Tx_i) $$ $$ w = \\sum \\limits_{1 = 1}^{n} \\lambda_i y_i x_i $$ $$ \\sum \\limits_{i = 1}^{n} \\lambda_i y_i = 0 $$ ::: ::: {.column width=\"50%\"} $$ \\begin{aligned} \\sum \\limits_{i = 1}^{n} \\lambda_i y_i (w^T x_i) &= w^T \\left (\\sum \\limits_{i = 1}^{n} \\lambda_i y_i x_i \\right)\\\\ &= w^T w\\\\ &= ||w||^2 \\end{aligned} $$ ::: ::: Step-3: Eliminate \\(w, b\\) \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} $$ L(w, b, \\lambda) = -\\cfrac{||w||^2}{2} + \\sum \\limits_{i = 1}^{n} \\lambda_i $$ $$ w = \\sum \\limits_{1 = 1}^{n} \\lambda_i y_i x_i $$ $$ \\sum \\limits_{i = 1}^{n} \\lambda_i y_i = 0 $$ ::: ::: {.column width=\"50%\"} ::: ::: Step-3: Eliminate \\(w, b\\) \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} $$ L(w, b, \\lambda) = -\\cfrac{||w||^2}{2} + \\sum \\limits_{i = 1}^{n} \\lambda_i $$ $$ w = \\sum \\limits_{1 = 1}^{n} \\lambda_i y_i x_i $$ $$ \\sum \\limits_{i = 1}^{n} \\lambda_i y_i = 0 $$ ::: ::: {.column width=\"50%\"} $$ \\begin{aligned} ||w||^2 &= w^Tw\\ &= \\left ( \\sum \\limits_{i = 1}^{n} \\lambda_i y_i x_i \\right)^T \\left( \\sum \\limits_{j = 1}^{n} \\lambda_j y_j x_j \\right)\\ &= \\left ( \\sum \\limits_{i = 1}^{n} \\lambda_i y_i x_i^T \\right) \\left( \\sum \\limits_{j = 1}^{n} \\lambda_j y_j x_j \\right)\\ &= \\cfrac{1}{2} \\sum \\limits_{i = 1}^{n} \\sum \\limits_{j = 1}^{n} (y_i y_j x_i^Tx_j) \\lambda_i \\lambda_j \\end{aligned} $$ ::: ::: Step-3: Eliminate \\(w, b\\) \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ L(\\lambda) = \\sum \\limits_{i = 1}^{n} \\lambda_i - \\cfrac{1}{2} \\sum \\limits_{i = 1}^{n} \\sum \\limits_{j = 1}^{n} (y_i y_j x_i^Tx_j) \\lambda_i \\lambda_j $$ ::: ::: {.column width=\"50%\"} ::: ::: Step-4: Dual \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\max \\limits_{\\lambda}\\quad \\sum \\limits_{i = 1}^{n} \\lambda_i - \\cfrac{1}{2} \\sum \\limits_{i = 1}^{n} \\sum \\limits_{j = 1}^{n} (y_i y_j x_i^Tx_j) \\lambda_i \\lambda_j $$ subject to the constraints: $$ \\lambda_i \\geq 0, \\quad 1 \\leq i \\leq n $$ and $$ \\sum \\limits_{i = 1}^{n} \\lambda_i y_i = 0 $$ ::: ::: {.column width=\"50%\"} ::: ::: Step-5: Quadratic programming \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} $$ \\max \\limits_{\\lambda}\\quad \\sum \\limits_{i = 1}^{n} \\lambda_i - \\cfrac{1}{2} \\sum \\limits_{i = 1}^{n} \\sum \\limits_{j = 1}^{n} (y_i y_j x_i^Tx_j) \\lambda_i \\lambda_j $$ subject to the constraints: $$ \\lambda_i \\geq 0, \\quad 1 \\leq i \\leq n $$ and $$ \\sum \\limits_{i = 1}^{n} \\lambda_i y_i = 0 $$ ::: ::: {.column width=\"50%\"} QP solver returns optimal \\(\\lambda\\) ::: ::: Step-6: Compute \\(w\\) and \\(b\\) \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\large w = \\sum \\limits_{i = 1}^{n} \\lambda_i y_i x_i $$ ::: ::: {.column width=\"50%\"} ::: ::: Support vectors \u00b6 ::: {.columns align=left} ::: {.column width=\"40%\"} $$ \\large w = \\sum \\limits_{i = 1}^{n} \\lambda_i y_i x_i $$ ::: ::: {.column width=\"60%\"} ::: incremental KKT condition: \\(\\lambda_i \\geq 0\\) KKT condition: \\(\\lambda_i \\left[1 - y_i(w^T x_i + b) \\right] = 0\\) If constraint is active, \\(y_i(w^T x_i + b) = 1\\) and \\(\\lambda_i > 0\\) If constraint is inactive, \\(y_i(w^Tx_i + b) > 1\\) and \\(\\lambda_i = 0\\) For most of the data-points \\(\\lambda_i = 0\\) Those points for which \\(\\lambda_i > 0\\) are the support vectors Support vectors lie on lines that are parallel to the decision boundary: \\(w^T x + b = 1\\) \\(w^T x + b = -1\\) If \\(S\\) is the set of support vectors, then we can rewrite \\(w\\) as: \\(w = \\sum \\limits_{x_i \\in S} \\lambda_i y_i x_i\\) ::: ::: ::: Support vectors \u00b6 ::: {.columns align=left} ::: {.column width=\"40%\"} $$ \\large w = \\sum \\limits_{x_i \\in S} \\lambda_i y_i x_i $$ ::: ::: {.column width=\"60%\"} ::: ::: Inference \u00b6 ::: {.columns align=left} ::: {.column width=\"100%\"} $$ \\hat{y} = \\text{sign}(w^Tx + b) $$ ::: ::: {.column width=\"50%\"} ::: :::","title":"Support Vector Machines"},{"location":"slides/week-8/lecture_5/#course-outline","text":"Linear regression Least square classification Perceptron Logistic regression Naive Bayes Softmax regression Support Vector Machines (SVM) Decision trees Ensemble techniques K-means clustering Artificial Neural Networks","title":"Course Outline"},{"location":"slides/week-8/lecture_5/#lecture-outline","text":"::: incremental Motivation Geometry Hard-margin SVM Formulation Optimization (recap) Optimization Soft-margin SVM Approximate solution :::","title":"Lecture Outline"},{"location":"slides/week-8/lecture_5/#hard-margin-svm","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\min \\limits_{w, b} \\quad \\cfrac{||w||^2}{2} $$ subject to: $$ y_i(w^Tx_i + b) \\geq 1,\\quad 1 \\leq i \\leq n $$ ::: ::: {.column width=\"50%\"} ::: :::","title":"Hard-margin SVM"},{"location":"slides/week-8/lecture_5/#hard-margin-svm_1","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\min \\limits_{w, b} \\quad \\cfrac{||w||^2}{2} $$ subject to: $$ 1 - y_i(w^Tx_i + b) \\leq 0,\\quad 1 \\leq i \\leq n $$ ::: ::: {.column width=\"50%\"} ::: :::","title":"Hard-margin SVM"},{"location":"slides/week-8/lecture_5/#step-1-lagrangian","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ L(w, b, \\lambda) = \\cfrac{||w||^2}{2} + \\sum \\limits_{i = 1}^{n} \\lambda_i \\left[ 1 - y_i(w^Tx_i + b)\\right] $$ ::: ::: {.column width=\"50%\"} ::: :::","title":"Step-1: Lagrangian"},{"location":"slides/week-8/lecture_5/#step-2-nabla-l-0","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\nabla_w L = w - \\sum \\limits_{i = 1}^{n} \\lambda_i y_i x_i = 0 $$ ::: ::: {.column width=\"50%\"} ::: :::","title":"Step-2: \\(\\nabla L = 0\\)"},{"location":"slides/week-8/lecture_5/#step-2-nabla-l-0_1","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ w = \\sum \\limits_{i = 1}^{n} \\lambda_i y_i x_i $$ ::: ::: {.column width=\"50%\"} ::: :::","title":"Step-2: \\(\\nabla L = 0\\)"},{"location":"slides/week-8/lecture_5/#step-2-nabla-l-0_2","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\nabla_b L =- \\sum \\limits_{i = 1}^{n} \\lambda_i y_i = 0 $$ ::: ::: {.column width=\"50%\"} ::: :::","title":"Step-2: \\(\\nabla L = 0\\)"},{"location":"slides/week-8/lecture_5/#step-2-nabla-l-0_3","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\sum \\limits_{i = 1}^{n} \\lambda_i y_i = 0 $$ ::: ::: {.column width=\"50%\"} ::: :::","title":"Step-2: \\(\\nabla L = 0\\)"},{"location":"slides/week-8/lecture_5/#step-3-eliminate-w-b","text":"::: {.columns align=center} ::: {.column width=\"50%\"} $$ L(w, b, \\lambda) = \\cfrac{||w||^2}{2} + \\sum \\limits_{i = 1}^{n} \\lambda_i \\left[ 1 - y_i(w^Tx_i + b)\\right] $$ $$ w = \\sum \\limits_{1 = 1}^{n} \\lambda_i y_i x_i $$ $$ \\sum \\limits_{i = 1}^{n} \\lambda_i y_i = 0 $$ ::: ::: {.column width=\"50%\"} ::: :::","title":"Step-3: Eliminate \\(w, b\\)"},{"location":"slides/week-8/lecture_5/#step-3-eliminate-w-b_1","text":"::: {.columns align=center} ::: {.column width=\"50%\"} $$ L(w, b, \\lambda) = \\cfrac{||w||^2}{2} + \\sum \\limits_{i = 1}^{n} \\lambda_i -\\sum \\limits_{i = 1}^{n} \\lambda_i y_i(w^Tx_i) + \\sum \\limits_{i = 1}^{n} \\lambda_i y_i b $$ $$ w = \\sum \\limits_{1 = 1}^{n} \\lambda_i y_i x_i $$ $$ \\sum \\limits_{i = 1}^{n} \\lambda_i y_i = 0 $$ ::: ::: {.column width=\"50%\"} ::: :::","title":"Step-3: Eliminate \\(w, b\\)"},{"location":"slides/week-8/lecture_5/#step-3-eliminate-w-b_2","text":"::: {.columns align=center} ::: {.column width=\"50%\"} $$ L(w, b, \\lambda) = \\cfrac{||w||^2}{2} + \\sum \\limits_{i = 1}^{n} \\lambda_i -\\sum \\limits_{i = 1}^{n} \\lambda_i y_i(w^Tx_i) $$ $$ w = \\sum \\limits_{1 = 1}^{n} \\lambda_i y_i x_i $$ $$ \\sum \\limits_{i = 1}^{n} \\lambda_i y_i = 0 $$ ::: ::: {.column width=\"50%\"} ::: :::","title":"Step-3: Eliminate \\(w, b\\)"},{"location":"slides/week-8/lecture_5/#step-3-eliminate-w-b_3","text":"::: {.columns align=center} ::: {.column width=\"50%\"} $$ L(w, b, \\lambda) = \\cfrac{||w||^2}{2} + \\sum \\limits_{i = 1}^{n} \\lambda_i -\\sum \\limits_{i = 1}^{n} \\lambda_i y_i(w^Tx_i) $$ $$ w = \\sum \\limits_{1 = 1}^{n} \\lambda_i y_i x_i $$ $$ \\sum \\limits_{i = 1}^{n} \\lambda_i y_i = 0 $$ ::: ::: {.column width=\"50%\"} $$ \\begin{aligned} \\sum \\limits_{i = 1}^{n} \\lambda_i y_i (w^T x_i) &= w^T \\left (\\sum \\limits_{i = 1}^{n} \\lambda_i y_i x_i \\right)\\\\ &= w^T w\\\\ &= ||w||^2 \\end{aligned} $$ ::: :::","title":"Step-3: Eliminate \\(w, b\\)"},{"location":"slides/week-8/lecture_5/#step-3-eliminate-w-b_4","text":"::: {.columns align=center} ::: {.column width=\"50%\"} $$ L(w, b, \\lambda) = -\\cfrac{||w||^2}{2} + \\sum \\limits_{i = 1}^{n} \\lambda_i $$ $$ w = \\sum \\limits_{1 = 1}^{n} \\lambda_i y_i x_i $$ $$ \\sum \\limits_{i = 1}^{n} \\lambda_i y_i = 0 $$ ::: ::: {.column width=\"50%\"} ::: :::","title":"Step-3: Eliminate \\(w, b\\)"},{"location":"slides/week-8/lecture_5/#step-3-eliminate-w-b_5","text":"::: {.columns align=center} ::: {.column width=\"50%\"} $$ L(w, b, \\lambda) = -\\cfrac{||w||^2}{2} + \\sum \\limits_{i = 1}^{n} \\lambda_i $$ $$ w = \\sum \\limits_{1 = 1}^{n} \\lambda_i y_i x_i $$ $$ \\sum \\limits_{i = 1}^{n} \\lambda_i y_i = 0 $$ ::: ::: {.column width=\"50%\"} $$ \\begin{aligned} ||w||^2 &= w^Tw\\ &= \\left ( \\sum \\limits_{i = 1}^{n} \\lambda_i y_i x_i \\right)^T \\left( \\sum \\limits_{j = 1}^{n} \\lambda_j y_j x_j \\right)\\ &= \\left ( \\sum \\limits_{i = 1}^{n} \\lambda_i y_i x_i^T \\right) \\left( \\sum \\limits_{j = 1}^{n} \\lambda_j y_j x_j \\right)\\ &= \\cfrac{1}{2} \\sum \\limits_{i = 1}^{n} \\sum \\limits_{j = 1}^{n} (y_i y_j x_i^Tx_j) \\lambda_i \\lambda_j \\end{aligned} $$ ::: :::","title":"Step-3: Eliminate \\(w, b\\)"},{"location":"slides/week-8/lecture_5/#step-3-eliminate-w-b_6","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ L(\\lambda) = \\sum \\limits_{i = 1}^{n} \\lambda_i - \\cfrac{1}{2} \\sum \\limits_{i = 1}^{n} \\sum \\limits_{j = 1}^{n} (y_i y_j x_i^Tx_j) \\lambda_i \\lambda_j $$ ::: ::: {.column width=\"50%\"} ::: :::","title":"Step-3: Eliminate \\(w, b\\)"},{"location":"slides/week-8/lecture_5/#step-4-dual","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\max \\limits_{\\lambda}\\quad \\sum \\limits_{i = 1}^{n} \\lambda_i - \\cfrac{1}{2} \\sum \\limits_{i = 1}^{n} \\sum \\limits_{j = 1}^{n} (y_i y_j x_i^Tx_j) \\lambda_i \\lambda_j $$ subject to the constraints: $$ \\lambda_i \\geq 0, \\quad 1 \\leq i \\leq n $$ and $$ \\sum \\limits_{i = 1}^{n} \\lambda_i y_i = 0 $$ ::: ::: {.column width=\"50%\"} ::: :::","title":"Step-4: Dual"},{"location":"slides/week-8/lecture_5/#step-5-quadratic-programming","text":"::: {.columns align=center} ::: {.column width=\"50%\"} $$ \\max \\limits_{\\lambda}\\quad \\sum \\limits_{i = 1}^{n} \\lambda_i - \\cfrac{1}{2} \\sum \\limits_{i = 1}^{n} \\sum \\limits_{j = 1}^{n} (y_i y_j x_i^Tx_j) \\lambda_i \\lambda_j $$ subject to the constraints: $$ \\lambda_i \\geq 0, \\quad 1 \\leq i \\leq n $$ and $$ \\sum \\limits_{i = 1}^{n} \\lambda_i y_i = 0 $$ ::: ::: {.column width=\"50%\"} QP solver returns optimal \\(\\lambda\\) ::: :::","title":"Step-5: Quadratic programming"},{"location":"slides/week-8/lecture_5/#step-6-compute-w-and-b","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\large w = \\sum \\limits_{i = 1}^{n} \\lambda_i y_i x_i $$ ::: ::: {.column width=\"50%\"} ::: :::","title":"Step-6: Compute \\(w\\) and \\(b\\)"},{"location":"slides/week-8/lecture_5/#support-vectors","text":"::: {.columns align=left} ::: {.column width=\"40%\"} $$ \\large w = \\sum \\limits_{i = 1}^{n} \\lambda_i y_i x_i $$ ::: ::: {.column width=\"60%\"} ::: incremental KKT condition: \\(\\lambda_i \\geq 0\\) KKT condition: \\(\\lambda_i \\left[1 - y_i(w^T x_i + b) \\right] = 0\\) If constraint is active, \\(y_i(w^T x_i + b) = 1\\) and \\(\\lambda_i > 0\\) If constraint is inactive, \\(y_i(w^Tx_i + b) > 1\\) and \\(\\lambda_i = 0\\) For most of the data-points \\(\\lambda_i = 0\\) Those points for which \\(\\lambda_i > 0\\) are the support vectors Support vectors lie on lines that are parallel to the decision boundary: \\(w^T x + b = 1\\) \\(w^T x + b = -1\\) If \\(S\\) is the set of support vectors, then we can rewrite \\(w\\) as: \\(w = \\sum \\limits_{x_i \\in S} \\lambda_i y_i x_i\\) ::: ::: :::","title":"Support vectors"},{"location":"slides/week-8/lecture_5/#support-vectors_1","text":"::: {.columns align=left} ::: {.column width=\"40%\"} $$ \\large w = \\sum \\limits_{x_i \\in S} \\lambda_i y_i x_i $$ ::: ::: {.column width=\"60%\"} ::: :::","title":"Support vectors"},{"location":"slides/week-8/lecture_5/#inference","text":"::: {.columns align=left} ::: {.column width=\"100%\"} $$ \\hat{y} = \\text{sign}(w^Tx + b) $$ ::: ::: {.column width=\"50%\"} ::: :::","title":"Inference"},{"location":"slides/week-8/lecture_6/","text":"Course Outline \u00b6 Linear regression Least square classification Perceptron Logistic regression Naive Bayes Softmax regression Support Vector Machines (SVM) Decision trees Ensemble techniques K-means clustering Artificial Neural Networks Lecture Outline \u00b6 ::: incremental Motivation Geometry Hard-margin SVM Formulation Optimization (recap) Optimization Soft-margin SVM Approximate solution ::: Nearly Linearly separable \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: ::: Nearly Linearly separable \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: ::: Errors \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: ::: Errors \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: ::: Errors \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: ::: Errors \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: ::: Errors \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: ::: Errors \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: ::: Errors \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: ::: Errors \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: ::: Errors \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: ::: Errors \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: ::: Errors \u00b6 ::: {.columns align=center} ::: {.column width=\"40%\"} ::: ::: {.column width=\"60%\"} $$ \\xi_i = \\begin{cases} 0,\\ & x_i \\text{ outside margin}\\ 1 - y_i(w^T x_i + b), \\ & x_i \\text{ inside margin}\\ \\end{cases} $$ ::: ::: Constraints \u00b6 ::: {.columns align=center} ::: {.column width=\"40%\"} ::: ::: {.column width=\"60%\"} $$ \\xi_i = \\begin{cases} 0,\\ & x_i \\text{ outside margin}\\ 1 - y_i(w^T x_i + b), \\ & x_i \\text{ inside margin}\\ \\end{cases} $$ $$ \\xi_i \\geq 0 $$ and $$ y_i(w^T x_i + b) \\geq 1 - \\xi_i $$ ::: ::: Objective function \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\cfrac{||w||^2}{2} + C \\sum \\limits_{i = 1}^{n} \\xi_i $$ ::: ::: {.column width=\"50%\"} ::: ::: Primal \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\min \\limits_{w, b} \\quad\\cfrac{||w||^2}{2} + C \\sum \\limits_{i = 1}^{n} \\xi_i $$ subject to $$ \\xi_i \\geq 0,\\quad 1 \\leq i \\leq n $$ $$ y_i(w^T x_i + b) \\geq 1 - \\xi_i, \\quad 1 \\leq i \\leq n $$ ::: ::: {.column width=\"50%\"} ::: ::: Dual \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\max \\limits_{\\lambda}\\quad \\sum \\limits_{i = 1}^{n} \\lambda_i - \\cfrac{1}{2} \\sum \\limits_{i = 1}^{n} \\sum \\limits_{j = 1}^{n} (y_i y_j x_i^Tx_j) \\lambda_i \\lambda_j $$ subject to: $$ 0 \\leq \\lambda_i \\leq C, \\quad 1 \\leq i \\leq n $$ $$ \\sum \\limits_{i = 1}^{n} \\lambda_i y_i = 0 $$ ::: ::: {.column width=\"50%\"} ::: :::","title":"Support Vector Machines"},{"location":"slides/week-8/lecture_6/#course-outline","text":"Linear regression Least square classification Perceptron Logistic regression Naive Bayes Softmax regression Support Vector Machines (SVM) Decision trees Ensemble techniques K-means clustering Artificial Neural Networks","title":"Course Outline"},{"location":"slides/week-8/lecture_6/#lecture-outline","text":"::: incremental Motivation Geometry Hard-margin SVM Formulation Optimization (recap) Optimization Soft-margin SVM Approximate solution :::","title":"Lecture Outline"},{"location":"slides/week-8/lecture_6/#nearly-linearly-separable","text":"::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: :::","title":"Nearly Linearly separable"},{"location":"slides/week-8/lecture_6/#nearly-linearly-separable_1","text":"::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: :::","title":"Nearly Linearly separable"},{"location":"slides/week-8/lecture_6/#errors","text":"::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: :::","title":"Errors"},{"location":"slides/week-8/lecture_6/#errors_1","text":"::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: :::","title":"Errors"},{"location":"slides/week-8/lecture_6/#errors_2","text":"::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: :::","title":"Errors"},{"location":"slides/week-8/lecture_6/#errors_3","text":"::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: :::","title":"Errors"},{"location":"slides/week-8/lecture_6/#errors_4","text":"::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: :::","title":"Errors"},{"location":"slides/week-8/lecture_6/#errors_5","text":"::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: :::","title":"Errors"},{"location":"slides/week-8/lecture_6/#errors_6","text":"::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: :::","title":"Errors"},{"location":"slides/week-8/lecture_6/#errors_7","text":"::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: :::","title":"Errors"},{"location":"slides/week-8/lecture_6/#errors_8","text":"::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: :::","title":"Errors"},{"location":"slides/week-8/lecture_6/#errors_9","text":"::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} ::: :::","title":"Errors"},{"location":"slides/week-8/lecture_6/#errors_10","text":"::: {.columns align=center} ::: {.column width=\"40%\"} ::: ::: {.column width=\"60%\"} $$ \\xi_i = \\begin{cases} 0,\\ & x_i \\text{ outside margin}\\ 1 - y_i(w^T x_i + b), \\ & x_i \\text{ inside margin}\\ \\end{cases} $$ ::: :::","title":"Errors"},{"location":"slides/week-8/lecture_6/#constraints","text":"::: {.columns align=center} ::: {.column width=\"40%\"} ::: ::: {.column width=\"60%\"} $$ \\xi_i = \\begin{cases} 0,\\ & x_i \\text{ outside margin}\\ 1 - y_i(w^T x_i + b), \\ & x_i \\text{ inside margin}\\ \\end{cases} $$ $$ \\xi_i \\geq 0 $$ and $$ y_i(w^T x_i + b) \\geq 1 - \\xi_i $$ ::: :::","title":"Constraints"},{"location":"slides/week-8/lecture_6/#objective-function","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\cfrac{||w||^2}{2} + C \\sum \\limits_{i = 1}^{n} \\xi_i $$ ::: ::: {.column width=\"50%\"} ::: :::","title":"Objective function"},{"location":"slides/week-8/lecture_6/#primal","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\min \\limits_{w, b} \\quad\\cfrac{||w||^2}{2} + C \\sum \\limits_{i = 1}^{n} \\xi_i $$ subject to $$ \\xi_i \\geq 0,\\quad 1 \\leq i \\leq n $$ $$ y_i(w^T x_i + b) \\geq 1 - \\xi_i, \\quad 1 \\leq i \\leq n $$ ::: ::: {.column width=\"50%\"} ::: :::","title":"Primal"},{"location":"slides/week-8/lecture_6/#dual","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\max \\limits_{\\lambda}\\quad \\sum \\limits_{i = 1}^{n} \\lambda_i - \\cfrac{1}{2} \\sum \\limits_{i = 1}^{n} \\sum \\limits_{j = 1}^{n} (y_i y_j x_i^Tx_j) \\lambda_i \\lambda_j $$ subject to: $$ 0 \\leq \\lambda_i \\leq C, \\quad 1 \\leq i \\leq n $$ $$ \\sum \\limits_{i = 1}^{n} \\lambda_i y_i = 0 $$ ::: ::: {.column width=\"50%\"} ::: :::","title":"Dual"},{"location":"slides/week-8/lecture_7/","text":"Course Outline \u00b6 Linear regression Least square classification Perceptron Logistic regression Naive Bayes Softmax regression Support Vector Machines (SVM) Decision trees Ensemble techniques K-means clustering Artificial Neural Networks Lecture Outline \u00b6 ::: incremental Motivation Geometry Hard-margin SVM Formulation Optimization (recap) Optimization Soft-margin SVM Approximate solution ::: Formulation \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} $$ \\min \\limits_{w, b} \\quad\\cfrac{||w||^2}{2} + C \\sum \\limits_{i = 1}^{n} \\xi_i $$ subject to $$ \\xi_i \\geq 0,\\quad 1 \\leq i \\leq n $$ $$ y_i(w^T x_i + b) \\geq 1 - \\xi_i, \\quad 1 \\leq i \\leq n $$ ::: ::: {.column width=\"50%\"} $$ \\xi_i = \\begin{cases} 0,\\ & x_i \\text{ outside margin}\\ 1 - y_i(w^T x_i + b), \\ & x_i \\text{ inside margin}\\ \\end{cases} $$ ::: ::: Formulation \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} $$ \\min \\limits_{w, b} \\quad\\cfrac{||w||^2}{2} + C \\sum \\limits_{i = 1}^{n} \\xi_i $$ subject to $$ \\xi_i \\geq 0,\\quad 1 \\leq i \\leq n $$ $$ y_i(w^T x_i + b) \\geq 1 - \\xi_i, \\quad 1 \\leq i \\leq n $$ ::: ::: {.column width=\"50%\"} $$ \\xi_i = \\max \\bigg(0, 1 - y_i(w^Tx_i + b) \\bigg) $$ ::: ::: Formulation \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} $$ \\min \\limits_{w, b} \\quad\\cfrac{||w||^2}{2} + C \\sum \\limits_{i = 1}^{n} \\xi_i $$ subject to $$ \\xi_i \\geq 0,\\quad 1 \\leq i \\leq n $$ $$ y_i(w^T x_i + b) \\geq 1 - \\xi_i, \\quad 1 \\leq i \\leq n $$ ::: ::: {.column width=\"50%\"} $$ \\min \\limits_{w, b} \\quad\\cfrac{||w||^2}{2} + C \\sum \\limits_{i = 1}^{n} \\max \\bigg(0, 1 - y_i(w^Tx_i + b) \\bigg) $$ ::: ::: Formulation \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} $$ \\min \\limits_{w, b} \\quad\\cfrac{||w||^2}{2} + C \\sum \\limits_{i = 1}^{n} \\xi_i $$ subject to $$ \\xi_i \\geq 0,\\quad 1 \\leq i \\leq n $$ $$ y_i(w^T x_i + b) \\geq 1 - \\xi_i, \\quad 1 \\leq i \\leq n $$ ::: ::: {.column width=\"50%\"} $$ \\min \\limits_{w, b} \\quad\\cfrac{||w||^2}{2} + C \\sum \\limits_{i = 1}^{n} \\max \\bigg(0, 1 - y_i(w^Tx_i + b) \\bigg) $$ ::: incremental Unconstraind Convex objective Enter gradient descent ::: ::: ::: Loss \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} $$ \\small{\\min \\limits_{w, b} \\quad\\cfrac{||w||^2}{2} + C \\sum \\limits_{i = 1}^{n} \\max \\bigg(0, 1 - y_i(w^Tx_i + b) \\bigg)} $$ ::: ::: {.column width=\"50%\"} ::: incremental \\(\\small \\sum \\limits_{i = 1}^{n} \\max \\bigg(0, 1 - y_i(w^Tx_i + b) \\bigg)\\) \u2014 loss function \\(C > 0\\) is a regularization term \\(C\\) controls the amount of slackness (OR) margin violations we can tolerate Does this expression remind you of regularized linear regression? What happens when: \\(C \\ll 1\\) \\(C \\rightarrow \\infty\\) ::: ::: ::: Hinge Loss \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\max \\bigg(0, 1 - y_i(w^Tx_i + b) \\bigg) $$ ::: ::: {.column width=\"50%\"} ::: ::: Hinge Loss \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\max \\bigg(0, 1 - z \\bigg) $$ ::: ::: {.column width=\"50%\"} ::: ::: Hinge Loss \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} $$ \\max \\bigg(0, 1 - z \\bigg) $$ ::: ::: {.column width=\"50%\"} ::: ::: Gradients \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} $$ f(z) = \\max \\bigg(0, 1 - z \\bigg) $$ ::: ::: {.column width=\"50%\"} $$ \\nabla_z f = \\begin{cases} 0, & z \\geq 1\\ -1, & z < 1 \\end{cases} $$ ::: ::: Gradients \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} $$ f(w, b) = \\max \\bigg(0, 1 - y_i(w^T x_i + b) \\bigg) $$ ::: ::: {.column width=\"50%\"} $$ \\nabla_w f = \\begin{cases} 0, & y_i(w^T x_i + b) \\geq 1\\ -y_i x_i, & y_i(w^T x_i + b) < 1 \\end{cases} $$ ::: ::: Gradients \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} $$ f(w, b) = \\max \\bigg(0, 1 - y_i(w^T x_i + b) \\bigg) $$ ::: ::: {.column width=\"50%\"} $$ \\nabla_w f = \\begin{cases} 0, & y_i(w^T x_i + b) \\geq 1\\ -y_i x_i, & y_i(w^T x_i + b) < 1 \\end{cases} $$ $$ \\nabla_b f = \\begin{cases} 0, & y_i(w^T x_i + b) \\geq 1\\ -y_i, & y_i(w^T x_i + b) < 1 \\end{cases} $$ ::: :::","title":"Support Vector Machines"},{"location":"slides/week-8/lecture_7/#course-outline","text":"Linear regression Least square classification Perceptron Logistic regression Naive Bayes Softmax regression Support Vector Machines (SVM) Decision trees Ensemble techniques K-means clustering Artificial Neural Networks","title":"Course Outline"},{"location":"slides/week-8/lecture_7/#lecture-outline","text":"::: incremental Motivation Geometry Hard-margin SVM Formulation Optimization (recap) Optimization Soft-margin SVM Approximate solution :::","title":"Lecture Outline"},{"location":"slides/week-8/lecture_7/#formulation","text":"::: {.columns align=center} ::: {.column width=\"50%\"} $$ \\min \\limits_{w, b} \\quad\\cfrac{||w||^2}{2} + C \\sum \\limits_{i = 1}^{n} \\xi_i $$ subject to $$ \\xi_i \\geq 0,\\quad 1 \\leq i \\leq n $$ $$ y_i(w^T x_i + b) \\geq 1 - \\xi_i, \\quad 1 \\leq i \\leq n $$ ::: ::: {.column width=\"50%\"} $$ \\xi_i = \\begin{cases} 0,\\ & x_i \\text{ outside margin}\\ 1 - y_i(w^T x_i + b), \\ & x_i \\text{ inside margin}\\ \\end{cases} $$ ::: :::","title":"Formulation"},{"location":"slides/week-8/lecture_7/#formulation_1","text":"::: {.columns align=center} ::: {.column width=\"50%\"} $$ \\min \\limits_{w, b} \\quad\\cfrac{||w||^2}{2} + C \\sum \\limits_{i = 1}^{n} \\xi_i $$ subject to $$ \\xi_i \\geq 0,\\quad 1 \\leq i \\leq n $$ $$ y_i(w^T x_i + b) \\geq 1 - \\xi_i, \\quad 1 \\leq i \\leq n $$ ::: ::: {.column width=\"50%\"} $$ \\xi_i = \\max \\bigg(0, 1 - y_i(w^Tx_i + b) \\bigg) $$ ::: :::","title":"Formulation"},{"location":"slides/week-8/lecture_7/#formulation_2","text":"::: {.columns align=center} ::: {.column width=\"50%\"} $$ \\min \\limits_{w, b} \\quad\\cfrac{||w||^2}{2} + C \\sum \\limits_{i = 1}^{n} \\xi_i $$ subject to $$ \\xi_i \\geq 0,\\quad 1 \\leq i \\leq n $$ $$ y_i(w^T x_i + b) \\geq 1 - \\xi_i, \\quad 1 \\leq i \\leq n $$ ::: ::: {.column width=\"50%\"} $$ \\min \\limits_{w, b} \\quad\\cfrac{||w||^2}{2} + C \\sum \\limits_{i = 1}^{n} \\max \\bigg(0, 1 - y_i(w^Tx_i + b) \\bigg) $$ ::: :::","title":"Formulation"},{"location":"slides/week-8/lecture_7/#formulation_3","text":"::: {.columns align=left} ::: {.column width=\"50%\"} $$ \\min \\limits_{w, b} \\quad\\cfrac{||w||^2}{2} + C \\sum \\limits_{i = 1}^{n} \\xi_i $$ subject to $$ \\xi_i \\geq 0,\\quad 1 \\leq i \\leq n $$ $$ y_i(w^T x_i + b) \\geq 1 - \\xi_i, \\quad 1 \\leq i \\leq n $$ ::: ::: {.column width=\"50%\"} $$ \\min \\limits_{w, b} \\quad\\cfrac{||w||^2}{2} + C \\sum \\limits_{i = 1}^{n} \\max \\bigg(0, 1 - y_i(w^Tx_i + b) \\bigg) $$ ::: incremental Unconstraind Convex objective Enter gradient descent ::: ::: :::","title":"Formulation"},{"location":"slides/week-8/lecture_7/#loss","text":"::: {.columns align=left} ::: {.column width=\"50%\"} $$ \\small{\\min \\limits_{w, b} \\quad\\cfrac{||w||^2}{2} + C \\sum \\limits_{i = 1}^{n} \\max \\bigg(0, 1 - y_i(w^Tx_i + b) \\bigg)} $$ ::: ::: {.column width=\"50%\"} ::: incremental \\(\\small \\sum \\limits_{i = 1}^{n} \\max \\bigg(0, 1 - y_i(w^Tx_i + b) \\bigg)\\) \u2014 loss function \\(C > 0\\) is a regularization term \\(C\\) controls the amount of slackness (OR) margin violations we can tolerate Does this expression remind you of regularized linear regression? What happens when: \\(C \\ll 1\\) \\(C \\rightarrow \\infty\\) ::: ::: :::","title":"Loss"},{"location":"slides/week-8/lecture_7/#hinge-loss","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\max \\bigg(0, 1 - y_i(w^Tx_i + b) \\bigg) $$ ::: ::: {.column width=\"50%\"} ::: :::","title":"Hinge Loss"},{"location":"slides/week-8/lecture_7/#hinge-loss_1","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\max \\bigg(0, 1 - z \\bigg) $$ ::: ::: {.column width=\"50%\"} ::: :::","title":"Hinge Loss"},{"location":"slides/week-8/lecture_7/#hinge-loss_2","text":"::: {.columns align=center} ::: {.column width=\"50%\"} $$ \\max \\bigg(0, 1 - z \\bigg) $$ ::: ::: {.column width=\"50%\"} ::: :::","title":"Hinge Loss"},{"location":"slides/week-8/lecture_7/#gradients","text":"::: {.columns align=center} ::: {.column width=\"50%\"} $$ f(z) = \\max \\bigg(0, 1 - z \\bigg) $$ ::: ::: {.column width=\"50%\"} $$ \\nabla_z f = \\begin{cases} 0, & z \\geq 1\\ -1, & z < 1 \\end{cases} $$ ::: :::","title":"Gradients"},{"location":"slides/week-8/lecture_7/#gradients_1","text":"::: {.columns align=center} ::: {.column width=\"50%\"} $$ f(w, b) = \\max \\bigg(0, 1 - y_i(w^T x_i + b) \\bigg) $$ ::: ::: {.column width=\"50%\"} $$ \\nabla_w f = \\begin{cases} 0, & y_i(w^T x_i + b) \\geq 1\\ -y_i x_i, & y_i(w^T x_i + b) < 1 \\end{cases} $$ ::: :::","title":"Gradients"},{"location":"slides/week-8/lecture_7/#gradients_2","text":"::: {.columns align=center} ::: {.column width=\"50%\"} $$ f(w, b) = \\max \\bigg(0, 1 - y_i(w^T x_i + b) \\bigg) $$ ::: ::: {.column width=\"50%\"} $$ \\nabla_w f = \\begin{cases} 0, & y_i(w^T x_i + b) \\geq 1\\ -y_i x_i, & y_i(w^T x_i + b) < 1 \\end{cases} $$ $$ \\nabla_b f = \\begin{cases} 0, & y_i(w^T x_i + b) \\geq 1\\ -y_i, & y_i(w^T x_i + b) < 1 \\end{cases} $$ ::: :::","title":"Gradients"},{"location":"week-12/appendix/","text":"Appendix \u00b6 Categorical Cross Entropy \u00b6 Part-1 \u00b6 In the case of multi-class classification, the output layer after softmax can be viewed as a conditional probability distribution over the labels given the data-point. This is a categorical distribution over the \\(k\\) classes. Let us call it \\(q(y\\ |\\ x)\\) . This is the predicted distribution. There is a true distribution over the labels. Let us call it \\(p(y\\ |\\ x)\\) . Note that both are conditional distributions of the label given the data-point. The (conditional) cross-entropy of these two conditional distributions is given as follows: $$ H \\Big( p(y\\ |\\ x), q(y\\ |\\ x) \\Big) = -\\sum \\limits_{x} p(x) \\sum \\limits_{y} p(y\\ |\\ x) \\log q(y\\ |\\ x) $$ Note that this is really the expectation of some function \\(f(x)\\) over the marginal distribution \\(p(x)\\) : \\[ \\begin{aligned} E[f(x)] &= \\sum \\limits_{x} p(x) f(x)\\\\ &= E \\left[- \\sum \\limits_{y} p(y\\ |\\ x) \\log q(y\\ |\\ x) \\right] \\end{aligned} \\] To compute an expectation, we need to sum over all samples drawn from this distribution. In practice, this is not going to be possible. This expectation is typically approximated by sampling data-points from the marginal distribution. We can replace the exhaustive summation with a sum over the \\(n\\) data-points in our batch of examples: $$ H \\Big( p(y\\ |\\ x), q(y\\ |\\ x) \\Big) = -\\cfrac{1}{n} \\sum \\limits_{i = 1}^{n} \\sum \\limits_{y} p(y\\ |\\ x^{(i)}) \\log q(y\\ |\\ x^{(i)}) $$ A crude way of seeing this is as follows: if a particular data-point \\(x^{*}\\) is sampled \\(r\\) times, (appears \\(r\\) times in the batch) then: $$ p(x^{*}) \\approx \\cfrac{r}{n} $$ Let us get rid of the factor of \\(\\frac{1}{n}\\) as it doesn't matter from the point of view of optimization. Part-2 \u00b6 We are left with the following expression for the cross-entropy: $$ H \\Big( p(y\\ |\\ x), q(y\\ |\\ x) \\Big) = -\\sum \\limits_{i = 1}^{n} \\sum \\limits_{y} p(y\\ |\\ x^{(i)}) \\log q(y\\ |\\ x^{(i)}) $$ We now need to understand the inner sum over the labels. Some notation needs to be be setup to make things more tractable: \\[ \\begin{aligned} y^{(i)}&: \\text{true label for data-point i, scalar}\\\\ \\boldsymbol{y^{(i)}}&: \\text{true label for data-point i, one-hot vector}\\\\ \\boldsymbol{\\hat{y}^{(i)}}&: \\text{predicted probabilities for data-point i, vector} \\end{aligned} \\] The symbols in bold are are vectors of length \\(k\\) , where \\(k\\) is the number of classes. With this notation we can get a better idea about the true and predicted distributions, \\(p\\) and \\(q\\) , respectively: \\[ \\begin{aligned} p(y = c\\ |\\ x^{(i)}) &= \\boldsymbol{y^{(i)}_{c}}\\\\ q(y = c\\ |\\ x^{(i)}) &= \\boldsymbol{\\hat{y}^{(i)}_{c}} \\end{aligned} \\] Note that \\(\\boldsymbol{\\hat{y}^{(i)}_{c}}\\) is the \\(c^{th}\\) component of a vector and hence a scalar. Same is the case for \\(\\boldsymbol{y^{(i)}_{c}}\\) We are now ready to compute the categorical cross-entropy: \\[ \\begin{aligned} H \\Big( p(y\\ |\\ x), q(y\\ |\\ x) \\Big) &= -\\sum \\limits_{i = 1}^{n} \\sum \\limits_{y} p(y\\ |\\ x^{(i)}) \\log q(y\\ |\\ x^{(i)})\\\\ &= - \\sum \\limits_{i = 1}^{n} \\sum \\limits_{c = 1}^{k} p(y = c\\ |\\ x^{(i)}) \\log q(y = c\\ |\\ x^{(i)})\\\\ &= - \\sum \\limits_{i = 1}^{n} \\sum \\limits_{c = 1}^{k} \\boldsymbol{y^{(i)}_{c}}\\ \\cdot \\log(\\boldsymbol{\\hat{y}^{(i)}_{c}}) \\end{aligned} \\] This is the final expression for the categorical cross-entropy (CCE) loss. Even though the inner sum is over \\(k\\) classes, only one of the values for \\(\\boldsymbol{y^{(i)}_{c}}\\) is non-zero. In the end, the sum reduces to a very simple scalar expression in principle: \\[ \\text{CCE} = \\sum \\limits_{i = 1}^{n} -\\log \\boldsymbol{\\hat{y}^{(i)}_{y^{(i)}}} \\] I know that this looks hopelessly complicated, thanks to the subscripts and superscripts. But what it is saying is this: The categorical cross-entropy for a batch of data-points can be computed as follows: For each data-point, get the predicted probability corresponding to the true label. Take negative log of that probability. Sum this quantity across the entire batch. Part-3 \u00b6 Intuitively, this loss makes a lot of sense. In fact, we can see why this should qualify as a loss in the first place by asking these questions: When is this quantity minimized? When the predicted probability for the correct class (true label) is \\(1\\) for every data-point in the batch. What happens if the predicted probability for the correct class for some data-point is close to \\(0\\) ? In such a case, \\(-\\log \\boldsymbol{\\hat{y}^{(i)}}_{y^{(i)}}\\) will be a huge positive value, something that is undesirable. What does minimizing cross-entropy really mean? The attempt to minimize the categorical cross-entropy translates to pushing the probability of the correct-class closer and closer to \\(1\\) for each data-point. Our hope is that in this process, the model will learn something useful that generalizes to unseen data-points as well. Part-4 \u00b6 We can see how this reduces to the binary cross-entropy loss in the case of two classes: \\[ \\begin{aligned} BCE &= - \\sum \\limits_{i = 1}^{n} \\sum \\limits_{c = 1}^{2} \\boldsymbol{y^{(i)}_{c}}\\ \\cdot \\log(\\boldsymbol{\\hat{y}^{(i)}_{c}})\\\\ &= - \\sum \\limits_{i = 1}^{n} \\left( y^{(i)} \\log (\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log (1 - \\hat{y}^{(i)}) \\right) \\end{aligned} \\] Chain rule for matrix product \u00b6 Let \\(P = QR\\) , where \\(P, Q\\) and \\(R\\) are matrices of compatible dimensions. Let \\(\\phi\\) be some function of \\(P\\) . Let us assume that we have access to the gradient of \\(\\phi\\) with respect to \\(P\\) . Call this \\(P^{(g)}\\) . Then, we have the following equations: \\[ Q^{(g)} = P^{(g)} R^T\\\\ R^{(g)} = Q^T P^{(g)} \\] This is nothing but the chain rule of differentiation in the presence of a matrix product. Gradient for softmax layer \u00b6 In order to derive the gradient matrix at the final layer, it is easier to consider the simpler case of the gradient vector. Let \\(\\boldsymbol{z}\\) be the pre-activation vector at layer \\(L\\) of the network. After applying softmax function \\(g\\) on the pre-activation vector, we get \\(\\boldsymbol{a}\\) , the activation vector. The two vectors are related by the following equations: \\[ \\boldsymbol{a} = g(\\boldsymbol{z}) = \\begin{bmatrix} \\cdots & \\cfrac{e^{z_j}}{\\sum \\limits_{r = 1}^{k} e^{z_r}} & \\cdots \\end{bmatrix} \\] The gradient of the loss with respect to the activation vector is given by: \\[ \\boldsymbol{a^{(g)}} = \\nabla_{\\boldsymbol{a}} L \\] The tricky part is the gradient of the loss with respect to the pre-activations. Let us consider a single element in the pre-activation vector, \\(z_j\\) . We are interested in the partial derivative \\(\\cfrac{\\partial L}{\\partial z_j}\\) . What this means is this: how much does the loss change when a small change is made to \\(z_j\\) . It is not exactly this, but the ratio of these two quantities, but you get the idea. \\(z_j\\) isn't directly connected to the loss function. The elements of the activation vector \\(a_i\\) s intervene. In other words, the impact of \\(z_j\\) on the loss is mediated by the \\(a_i\\) s. Changing \\(z_j\\) has an impact on each of the \\(a_i\\) s because of the way softmax is defined. We already know how the loss is affected when the \\(a_j\\) s are disturbed. This is captured by \\(\\boldsymbol{a^{(g)}}\\) . So much for the chain rule. We are now ready to state it: \\[ \\cfrac{\\partial L}{\\partial z_j} = \\sum \\limits_{i = 1}^{k} \\cfrac{\\partial L}{\\partial a_i} \\cfrac{\\partial a_i}{\\partial z_j} \\] The second term in the product is the \\(ij^{th}\\) element of the Jacobian matrix \\(J_{\\boldsymbol{z}}(\\boldsymbol{a})\\) : \\[ J_{\\boldsymbol{z}}(\\boldsymbol{a}) \\Bigg|_{ij} = \\left [ \\cfrac{\\partial a_i}{\\partial z_j} \\right] \\] The Jacobian matrix is a matrix of partial derivatives of the components of the vector \\(\\boldsymbol{a}\\) with respect to \\(\\boldsymbol{z}\\) . With the introduction of the Jacobian matrix, we can now state the gradient of the loss with respect to the pre-activations in the following succinct manner: \\[ \\boldsymbol{z^{(g)}} = \\nabla_{\\boldsymbol{z}} L = J_{\\boldsymbol{z}}(\\boldsymbol{a})^T\\boldsymbol{a^{(g)}} \\] We can now turn our attention to computing the Jacobian: \\[ J_{\\boldsymbol{z}}(\\boldsymbol{a}) \\Bigg|_{ij} = \\begin{cases} -a_i a_j &\\quad i \\neq j\\\\ a_j - a_j^2 &\\quad i = j \\end{cases} \\] This form of expressing the Jacobian element-wise is unwieldy for computation. Fortunately, this has a simple matrix-representation: \\[ J_{\\boldsymbol{z}}(\\boldsymbol{a}) = \\text{diag}(\\boldsymbol{a}) - \\boldsymbol{a} \\boldsymbol{a}^T \\] Here, \\(\\text{diag}({\\boldsymbol{a}})\\) is a diagonal matrix with the elements of the activation vector making up the diagonal. The second term on the RHS is called the outer-product. We can now plug the Jacobian into the earlier equation: \\[ \\begin{aligned} \\boldsymbol{z^{(g)}} &= J_{\\boldsymbol{z}}(\\boldsymbol{a})^T\\boldsymbol{a^{(g)}}\\\\ &= \\Big[ \\text{diag}(\\boldsymbol{a}) - \\boldsymbol{a} \\boldsymbol{a}^T \\Big] \\boldsymbol{a^{(g)}}\\\\ &= \\left( \\boldsymbol{a} \\odot \\boldsymbol{a^{(g)}} \\right) - \\left( \\boldsymbol{a}^T \\boldsymbol{a^{(g)}}\\boldsymbol{a}\\right)\\\\ &= \\left(\\boldsymbol{a} \\odot \\boldsymbol{a^{(g)}} \\right) - \\left( \\boldsymbol{a} \\odot \\boldsymbol{a^{(g)}} \\boldsymbol{1}_k \\boldsymbol{a}\\right)\\\\ \\end{aligned} \\] That might seem terribly complicated. Step-2 of the equation is just substituting the Jacobian. To understand step-3, try to think about the product of a diagonal matrix and a vector, and see how that transforms to element-wise product between two vectors. The final-step is another trick where we convert the dot product between two vectors into an element-wise product followed by multiplication by a vector of ones. Why are all these transformations necessary? The moment we express them as element-wise products, we can almost effortlessly extend this formula to a matrix of activations: \\[ \\boldsymbol{Z_L^{(g)}} = \\left(\\boldsymbol{A_L} \\odot \\boldsymbol{A_L^{(g)}} \\right) - \\left( \\boldsymbol{A_L} \\odot \\boldsymbol{A_L^{(g)}} \\boldsymbol{1}_{k \\times k} \\right) \\odot \\boldsymbol{A_L}\\\\ \\] That again looks complicated. But if we stare at it for a while, the RHS is actually remarkably simple: The first term is is simply the element-wise multiplication of the activation matrix and its corresponding gradient. Call this some matrix \\(\\boldsymbol{B}\\) . The second term is just the row-wise sum of \\(\\boldsymbol{B}\\) followed by an element-wise multiplication with the activation-matrix. These expressions are powerful because we can directly translate them into NumPy . For example, the NumPy equivalent of this equation would be: B = A_L * A_L_g Z_l_g = B - np.sum(B, axis = 1) * A_l The good news is that the expression for \\(\\boldsymbol{Z_L^{(g)}}\\) In the case of softmax with categorical cross-entropy loss can be further simplified. Recall the following results: \\[ \\begin{aligned} \\boldsymbol{A_L} &= \\boldsymbol{\\hat{Y}}\\\\ \\boldsymbol{A_L^{(g)}} &= - \\boldsymbol{Y} \\odot \\boldsymbol{\\hat{Y}}^{\\odot -1} \\end{aligned} \\] The element-wise product of these two matrices is simply the matrix \\(-\\boldsymbol{Y}\\) ! With that, the expression for \\(\\boldsymbol{Z_L^{(g)}}\\) becomes: \\[ \\boldsymbol{Z_L^{(g)}} = \\boldsymbol{\\hat{Y}} - \\boldsymbol{Y} \\] All this computation seems justified given the elementary nature of the result.","title":"Appendix"},{"location":"week-12/appendix/#appendix","text":"","title":"Appendix"},{"location":"week-12/appendix/#categorical-cross-entropy","text":"","title":"Categorical Cross Entropy"},{"location":"week-12/appendix/#part-1","text":"In the case of multi-class classification, the output layer after softmax can be viewed as a conditional probability distribution over the labels given the data-point. This is a categorical distribution over the \\(k\\) classes. Let us call it \\(q(y\\ |\\ x)\\) . This is the predicted distribution. There is a true distribution over the labels. Let us call it \\(p(y\\ |\\ x)\\) . Note that both are conditional distributions of the label given the data-point. The (conditional) cross-entropy of these two conditional distributions is given as follows: $$ H \\Big( p(y\\ |\\ x), q(y\\ |\\ x) \\Big) = -\\sum \\limits_{x} p(x) \\sum \\limits_{y} p(y\\ |\\ x) \\log q(y\\ |\\ x) $$ Note that this is really the expectation of some function \\(f(x)\\) over the marginal distribution \\(p(x)\\) : \\[ \\begin{aligned} E[f(x)] &= \\sum \\limits_{x} p(x) f(x)\\\\ &= E \\left[- \\sum \\limits_{y} p(y\\ |\\ x) \\log q(y\\ |\\ x) \\right] \\end{aligned} \\] To compute an expectation, we need to sum over all samples drawn from this distribution. In practice, this is not going to be possible. This expectation is typically approximated by sampling data-points from the marginal distribution. We can replace the exhaustive summation with a sum over the \\(n\\) data-points in our batch of examples: $$ H \\Big( p(y\\ |\\ x), q(y\\ |\\ x) \\Big) = -\\cfrac{1}{n} \\sum \\limits_{i = 1}^{n} \\sum \\limits_{y} p(y\\ |\\ x^{(i)}) \\log q(y\\ |\\ x^{(i)}) $$ A crude way of seeing this is as follows: if a particular data-point \\(x^{*}\\) is sampled \\(r\\) times, (appears \\(r\\) times in the batch) then: $$ p(x^{*}) \\approx \\cfrac{r}{n} $$ Let us get rid of the factor of \\(\\frac{1}{n}\\) as it doesn't matter from the point of view of optimization.","title":"Part-1"},{"location":"week-12/appendix/#part-2","text":"We are left with the following expression for the cross-entropy: $$ H \\Big( p(y\\ |\\ x), q(y\\ |\\ x) \\Big) = -\\sum \\limits_{i = 1}^{n} \\sum \\limits_{y} p(y\\ |\\ x^{(i)}) \\log q(y\\ |\\ x^{(i)}) $$ We now need to understand the inner sum over the labels. Some notation needs to be be setup to make things more tractable: \\[ \\begin{aligned} y^{(i)}&: \\text{true label for data-point i, scalar}\\\\ \\boldsymbol{y^{(i)}}&: \\text{true label for data-point i, one-hot vector}\\\\ \\boldsymbol{\\hat{y}^{(i)}}&: \\text{predicted probabilities for data-point i, vector} \\end{aligned} \\] The symbols in bold are are vectors of length \\(k\\) , where \\(k\\) is the number of classes. With this notation we can get a better idea about the true and predicted distributions, \\(p\\) and \\(q\\) , respectively: \\[ \\begin{aligned} p(y = c\\ |\\ x^{(i)}) &= \\boldsymbol{y^{(i)}_{c}}\\\\ q(y = c\\ |\\ x^{(i)}) &= \\boldsymbol{\\hat{y}^{(i)}_{c}} \\end{aligned} \\] Note that \\(\\boldsymbol{\\hat{y}^{(i)}_{c}}\\) is the \\(c^{th}\\) component of a vector and hence a scalar. Same is the case for \\(\\boldsymbol{y^{(i)}_{c}}\\) We are now ready to compute the categorical cross-entropy: \\[ \\begin{aligned} H \\Big( p(y\\ |\\ x), q(y\\ |\\ x) \\Big) &= -\\sum \\limits_{i = 1}^{n} \\sum \\limits_{y} p(y\\ |\\ x^{(i)}) \\log q(y\\ |\\ x^{(i)})\\\\ &= - \\sum \\limits_{i = 1}^{n} \\sum \\limits_{c = 1}^{k} p(y = c\\ |\\ x^{(i)}) \\log q(y = c\\ |\\ x^{(i)})\\\\ &= - \\sum \\limits_{i = 1}^{n} \\sum \\limits_{c = 1}^{k} \\boldsymbol{y^{(i)}_{c}}\\ \\cdot \\log(\\boldsymbol{\\hat{y}^{(i)}_{c}}) \\end{aligned} \\] This is the final expression for the categorical cross-entropy (CCE) loss. Even though the inner sum is over \\(k\\) classes, only one of the values for \\(\\boldsymbol{y^{(i)}_{c}}\\) is non-zero. In the end, the sum reduces to a very simple scalar expression in principle: \\[ \\text{CCE} = \\sum \\limits_{i = 1}^{n} -\\log \\boldsymbol{\\hat{y}^{(i)}_{y^{(i)}}} \\] I know that this looks hopelessly complicated, thanks to the subscripts and superscripts. But what it is saying is this: The categorical cross-entropy for a batch of data-points can be computed as follows: For each data-point, get the predicted probability corresponding to the true label. Take negative log of that probability. Sum this quantity across the entire batch.","title":"Part-2"},{"location":"week-12/appendix/#part-3","text":"Intuitively, this loss makes a lot of sense. In fact, we can see why this should qualify as a loss in the first place by asking these questions: When is this quantity minimized? When the predicted probability for the correct class (true label) is \\(1\\) for every data-point in the batch. What happens if the predicted probability for the correct class for some data-point is close to \\(0\\) ? In such a case, \\(-\\log \\boldsymbol{\\hat{y}^{(i)}}_{y^{(i)}}\\) will be a huge positive value, something that is undesirable. What does minimizing cross-entropy really mean? The attempt to minimize the categorical cross-entropy translates to pushing the probability of the correct-class closer and closer to \\(1\\) for each data-point. Our hope is that in this process, the model will learn something useful that generalizes to unseen data-points as well.","title":"Part-3"},{"location":"week-12/appendix/#part-4","text":"We can see how this reduces to the binary cross-entropy loss in the case of two classes: \\[ \\begin{aligned} BCE &= - \\sum \\limits_{i = 1}^{n} \\sum \\limits_{c = 1}^{2} \\boldsymbol{y^{(i)}_{c}}\\ \\cdot \\log(\\boldsymbol{\\hat{y}^{(i)}_{c}})\\\\ &= - \\sum \\limits_{i = 1}^{n} \\left( y^{(i)} \\log (\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log (1 - \\hat{y}^{(i)}) \\right) \\end{aligned} \\]","title":"Part-4"},{"location":"week-12/appendix/#chain-rule-for-matrix-product","text":"Let \\(P = QR\\) , where \\(P, Q\\) and \\(R\\) are matrices of compatible dimensions. Let \\(\\phi\\) be some function of \\(P\\) . Let us assume that we have access to the gradient of \\(\\phi\\) with respect to \\(P\\) . Call this \\(P^{(g)}\\) . Then, we have the following equations: \\[ Q^{(g)} = P^{(g)} R^T\\\\ R^{(g)} = Q^T P^{(g)} \\] This is nothing but the chain rule of differentiation in the presence of a matrix product.","title":"Chain rule for matrix product"},{"location":"week-12/appendix/#gradient-for-softmax-layer","text":"In order to derive the gradient matrix at the final layer, it is easier to consider the simpler case of the gradient vector. Let \\(\\boldsymbol{z}\\) be the pre-activation vector at layer \\(L\\) of the network. After applying softmax function \\(g\\) on the pre-activation vector, we get \\(\\boldsymbol{a}\\) , the activation vector. The two vectors are related by the following equations: \\[ \\boldsymbol{a} = g(\\boldsymbol{z}) = \\begin{bmatrix} \\cdots & \\cfrac{e^{z_j}}{\\sum \\limits_{r = 1}^{k} e^{z_r}} & \\cdots \\end{bmatrix} \\] The gradient of the loss with respect to the activation vector is given by: \\[ \\boldsymbol{a^{(g)}} = \\nabla_{\\boldsymbol{a}} L \\] The tricky part is the gradient of the loss with respect to the pre-activations. Let us consider a single element in the pre-activation vector, \\(z_j\\) . We are interested in the partial derivative \\(\\cfrac{\\partial L}{\\partial z_j}\\) . What this means is this: how much does the loss change when a small change is made to \\(z_j\\) . It is not exactly this, but the ratio of these two quantities, but you get the idea. \\(z_j\\) isn't directly connected to the loss function. The elements of the activation vector \\(a_i\\) s intervene. In other words, the impact of \\(z_j\\) on the loss is mediated by the \\(a_i\\) s. Changing \\(z_j\\) has an impact on each of the \\(a_i\\) s because of the way softmax is defined. We already know how the loss is affected when the \\(a_j\\) s are disturbed. This is captured by \\(\\boldsymbol{a^{(g)}}\\) . So much for the chain rule. We are now ready to state it: \\[ \\cfrac{\\partial L}{\\partial z_j} = \\sum \\limits_{i = 1}^{k} \\cfrac{\\partial L}{\\partial a_i} \\cfrac{\\partial a_i}{\\partial z_j} \\] The second term in the product is the \\(ij^{th}\\) element of the Jacobian matrix \\(J_{\\boldsymbol{z}}(\\boldsymbol{a})\\) : \\[ J_{\\boldsymbol{z}}(\\boldsymbol{a}) \\Bigg|_{ij} = \\left [ \\cfrac{\\partial a_i}{\\partial z_j} \\right] \\] The Jacobian matrix is a matrix of partial derivatives of the components of the vector \\(\\boldsymbol{a}\\) with respect to \\(\\boldsymbol{z}\\) . With the introduction of the Jacobian matrix, we can now state the gradient of the loss with respect to the pre-activations in the following succinct manner: \\[ \\boldsymbol{z^{(g)}} = \\nabla_{\\boldsymbol{z}} L = J_{\\boldsymbol{z}}(\\boldsymbol{a})^T\\boldsymbol{a^{(g)}} \\] We can now turn our attention to computing the Jacobian: \\[ J_{\\boldsymbol{z}}(\\boldsymbol{a}) \\Bigg|_{ij} = \\begin{cases} -a_i a_j &\\quad i \\neq j\\\\ a_j - a_j^2 &\\quad i = j \\end{cases} \\] This form of expressing the Jacobian element-wise is unwieldy for computation. Fortunately, this has a simple matrix-representation: \\[ J_{\\boldsymbol{z}}(\\boldsymbol{a}) = \\text{diag}(\\boldsymbol{a}) - \\boldsymbol{a} \\boldsymbol{a}^T \\] Here, \\(\\text{diag}({\\boldsymbol{a}})\\) is a diagonal matrix with the elements of the activation vector making up the diagonal. The second term on the RHS is called the outer-product. We can now plug the Jacobian into the earlier equation: \\[ \\begin{aligned} \\boldsymbol{z^{(g)}} &= J_{\\boldsymbol{z}}(\\boldsymbol{a})^T\\boldsymbol{a^{(g)}}\\\\ &= \\Big[ \\text{diag}(\\boldsymbol{a}) - \\boldsymbol{a} \\boldsymbol{a}^T \\Big] \\boldsymbol{a^{(g)}}\\\\ &= \\left( \\boldsymbol{a} \\odot \\boldsymbol{a^{(g)}} \\right) - \\left( \\boldsymbol{a}^T \\boldsymbol{a^{(g)}}\\boldsymbol{a}\\right)\\\\ &= \\left(\\boldsymbol{a} \\odot \\boldsymbol{a^{(g)}} \\right) - \\left( \\boldsymbol{a} \\odot \\boldsymbol{a^{(g)}} \\boldsymbol{1}_k \\boldsymbol{a}\\right)\\\\ \\end{aligned} \\] That might seem terribly complicated. Step-2 of the equation is just substituting the Jacobian. To understand step-3, try to think about the product of a diagonal matrix and a vector, and see how that transforms to element-wise product between two vectors. The final-step is another trick where we convert the dot product between two vectors into an element-wise product followed by multiplication by a vector of ones. Why are all these transformations necessary? The moment we express them as element-wise products, we can almost effortlessly extend this formula to a matrix of activations: \\[ \\boldsymbol{Z_L^{(g)}} = \\left(\\boldsymbol{A_L} \\odot \\boldsymbol{A_L^{(g)}} \\right) - \\left( \\boldsymbol{A_L} \\odot \\boldsymbol{A_L^{(g)}} \\boldsymbol{1}_{k \\times k} \\right) \\odot \\boldsymbol{A_L}\\\\ \\] That again looks complicated. But if we stare at it for a while, the RHS is actually remarkably simple: The first term is is simply the element-wise multiplication of the activation matrix and its corresponding gradient. Call this some matrix \\(\\boldsymbol{B}\\) . The second term is just the row-wise sum of \\(\\boldsymbol{B}\\) followed by an element-wise multiplication with the activation-matrix. These expressions are powerful because we can directly translate them into NumPy . For example, the NumPy equivalent of this equation would be: B = A_L * A_L_g Z_l_g = B - np.sum(B, axis = 1) * A_l The good news is that the expression for \\(\\boldsymbol{Z_L^{(g)}}\\) In the case of softmax with categorical cross-entropy loss can be further simplified. Recall the following results: \\[ \\begin{aligned} \\boldsymbol{A_L} &= \\boldsymbol{\\hat{Y}}\\\\ \\boldsymbol{A_L^{(g)}} &= - \\boldsymbol{Y} \\odot \\boldsymbol{\\hat{Y}}^{\\odot -1} \\end{aligned} \\] The element-wise product of these two matrices is simply the matrix \\(-\\boldsymbol{Y}\\) ! With that, the expression for \\(\\boldsymbol{Z_L^{(g)}}\\) becomes: \\[ \\boldsymbol{Z_L^{(g)}} = \\boldsymbol{\\hat{Y}} - \\boldsymbol{Y} \\] All this computation seems justified given the elementary nature of the result.","title":"Gradient for softmax layer"},{"location":"week-12/backward/","text":"Backward pass \u00b6 As we have been doing all along, gradient descent will be our optimizer. We need to compute the gradients of the loss with respect to the weights and the biases. Imagine doing this for a network with thousands of parameters. We would have to find the derivative of the loss with respect to \\(1000\\) different variables. This seems like a computational nightmare. Thankfully, researchers have developed an efficient algorithm called backpropagation that does the job for us. At its heart, backpropagation uses the chain rule of differentiation to compute the gradients. The idea is to first begin with the gradients at the final layer and keep propagating them all the way back to the first layer. This sequence of operations is termed a backward pass, as we start from the final layer and let the gradients flow all the way back to the first layer. Just as we had a \"forward pass\" to compute the output given the input, we have a \"backward pass\" to compute the gradients of the loss with respect to the weights. The backward pass can be divided into two parts: Hidden Layers \u00b6 The weights at layer \\(l\\) , \\(\\boldsymbol{W_{l}}\\) , influence the loss via the matrix of activations, \\(\\boldsymbol{A_{l}}\\) , at layer \\(l\\) . The equation that connects these two quantities is given below and should be familiar to you by now: \\[ \\begin{aligned} \\boldsymbol{Z_{l}} &= \\boldsymbol{A_{l - 1} W_l} + \\boldsymbol{b_l},\\quad 1 \\leq l \\leq L\\\\ \\\\ \\boldsymbol{A_{l}} &= g(\\boldsymbol{Z_{l}}),\\quad 1 \\leq l \\leq L \\end{aligned} \\] Let us assume that we already have access to the gradient of the loss with respect to the activations at layer \\(l\\) . Let us call it \\(\\boldsymbol{A_{l}^{(g)}}\\) . This is a matrix of the same shape as \\(\\boldsymbol{A_{l}}\\) . If we need the gradient of the loss with respect to the weights at layer \\(l\\) , then by the chain rule of differentiation, we need to compute the gradients with respect to the pre-activations at layer \\(l\\) . We shall use the following notation for the gradients at layer \\(l\\) : \\(\\boldsymbol{W_{l}^{(g)}}\\) : gradient of the loss with respect to the weights \\(\\boldsymbol{Z_{l}^{(g)}}\\) : gradient of the loss with respect to the pre-activations \\(\\boldsymbol{A_{l}^{(g)}}\\) : gradient of the loss with respect to the activations The rule is quite simple to state. It is just a product, the first one is element-wise product of two matrices, the second one is our usual matrix multiplication: \\[ \\begin{aligned} \\boldsymbol{Z_{l}^{(g)}} &= \\boldsymbol{A_{l}^{(g)}} \\odot g^{\\prime}(\\boldsymbol{Z_{l}})\\\\ \\\\ \\boldsymbol{W_l^{(g)}} &= \\boldsymbol{A_{l - 1}}^T \\boldsymbol{Z_{l}^{(g)}} \\end{aligned} \\] To see why the first of these two equations makes sense, recall that the activation function is applied element-wise on the pre-activations in the forward pass. This process is reversed for the backward pass: the derivative of the activation function is multiplied element-wise with the gradients of the activations. The second equation might seem more foreboding. A reasonable intuition is to forget that these quantities are matrices and to instead think of them as scalars. Consider the following simplification: \\(z = aw + b\\) $$ w^{(g)} = \\cfrac{\\partial L}{\\partial w} = \\cfrac{\\partial L}{\\partial z} \\cfrac{\\partial z}{\\partial w} = z^{(g)} a $$ With matrices, this simple scalar product becomes a matrix product. One way to remember the exact form is to make sure that the dimensions of all the matrices are compatible for matrix multiplication. To propagate this process to earlier layers, we also need \\(\\boldsymbol{A_{l - 1}^{(g)}}\\) . The expression for that is quite similar: \\[ \\boldsymbol{A_{l - 1}^{(g)}} = \\boldsymbol{Z_{l}^{(g)}} \\boldsymbol{W_{l}}^T \\] Now, you can see why the algorithm is termed backpropagation. We start with the gradients at a layer and keep \"propagating\" them back until we hit the input layer. We have ignored the gradients of the loss with respect to the biases. That is left as an exercise to the reader. Output layer \u00b6 The only thing that remains is to compute the gradient of the loss with respect to the activations in the last layer. This is in fact the first step of the back-propagation algorithm. The gradients depend on the form of the loss, which in turn depends on the type of problem being solved: Regression \u00b6 Recall that \\(\\boldsymbol{A_L} = \\boldsymbol{\\hat{y}}\\) . The activations at the last layer are the predicted labels. \\[ \\boldsymbol{A_L^{(g)}} = \\boldsymbol{\\hat{y}} - \\boldsymbol{y} \\] As the output-activation function is linear, computing the gradient of the activations and weights for the penultimate layer in the network is straightforward. In fact, \\(\\boldsymbol{Z_{L}^{(g)}} = \\boldsymbol{A_L^{(g)}}\\) . Multiclass classification \u00b6 Recall that \\(\\boldsymbol{A_L} = \\boldsymbol{\\hat{Y}}\\) . The activations at the last layer are the predicted probabilities. \\[ \\boldsymbol{A_L^{(g)}} = - \\boldsymbol{Y} \\odot \\boldsymbol{\\hat{Y}}^{\\odot -1} \\] This notation might be new. \\(P^{\\odot -1}\\) is element-wise inverse. \\(P^{\\odot -1}_{ij} = \\frac{1}{P_{ij}}\\) . This inverse arises from differentiating the the \\(\\log\\) term in the loss function. As the output-activation function is softmax, computing the gradients of the pre-activations is less straightforward. Refer to appendix for a detailed derivation of the same. The expression for the gradients turns out to be very simple in the end: $$ \\boldsymbol{Z_L^{(g)}} = \\boldsymbol{\\hat{Y}} - \\boldsymbol{Y} $$ Note how similar the expressions are for regression and classification. We will take advantage of this fact during our implementation of neural networks. Algorithm \u00b6 We can now put together all these equations together and specify the algorithm for backward pass:","title":"Backward pass"},{"location":"week-12/backward/#backward-pass","text":"As we have been doing all along, gradient descent will be our optimizer. We need to compute the gradients of the loss with respect to the weights and the biases. Imagine doing this for a network with thousands of parameters. We would have to find the derivative of the loss with respect to \\(1000\\) different variables. This seems like a computational nightmare. Thankfully, researchers have developed an efficient algorithm called backpropagation that does the job for us. At its heart, backpropagation uses the chain rule of differentiation to compute the gradients. The idea is to first begin with the gradients at the final layer and keep propagating them all the way back to the first layer. This sequence of operations is termed a backward pass, as we start from the final layer and let the gradients flow all the way back to the first layer. Just as we had a \"forward pass\" to compute the output given the input, we have a \"backward pass\" to compute the gradients of the loss with respect to the weights. The backward pass can be divided into two parts:","title":"Backward pass"},{"location":"week-12/backward/#hidden-layers","text":"The weights at layer \\(l\\) , \\(\\boldsymbol{W_{l}}\\) , influence the loss via the matrix of activations, \\(\\boldsymbol{A_{l}}\\) , at layer \\(l\\) . The equation that connects these two quantities is given below and should be familiar to you by now: \\[ \\begin{aligned} \\boldsymbol{Z_{l}} &= \\boldsymbol{A_{l - 1} W_l} + \\boldsymbol{b_l},\\quad 1 \\leq l \\leq L\\\\ \\\\ \\boldsymbol{A_{l}} &= g(\\boldsymbol{Z_{l}}),\\quad 1 \\leq l \\leq L \\end{aligned} \\] Let us assume that we already have access to the gradient of the loss with respect to the activations at layer \\(l\\) . Let us call it \\(\\boldsymbol{A_{l}^{(g)}}\\) . This is a matrix of the same shape as \\(\\boldsymbol{A_{l}}\\) . If we need the gradient of the loss with respect to the weights at layer \\(l\\) , then by the chain rule of differentiation, we need to compute the gradients with respect to the pre-activations at layer \\(l\\) . We shall use the following notation for the gradients at layer \\(l\\) : \\(\\boldsymbol{W_{l}^{(g)}}\\) : gradient of the loss with respect to the weights \\(\\boldsymbol{Z_{l}^{(g)}}\\) : gradient of the loss with respect to the pre-activations \\(\\boldsymbol{A_{l}^{(g)}}\\) : gradient of the loss with respect to the activations The rule is quite simple to state. It is just a product, the first one is element-wise product of two matrices, the second one is our usual matrix multiplication: \\[ \\begin{aligned} \\boldsymbol{Z_{l}^{(g)}} &= \\boldsymbol{A_{l}^{(g)}} \\odot g^{\\prime}(\\boldsymbol{Z_{l}})\\\\ \\\\ \\boldsymbol{W_l^{(g)}} &= \\boldsymbol{A_{l - 1}}^T \\boldsymbol{Z_{l}^{(g)}} \\end{aligned} \\] To see why the first of these two equations makes sense, recall that the activation function is applied element-wise on the pre-activations in the forward pass. This process is reversed for the backward pass: the derivative of the activation function is multiplied element-wise with the gradients of the activations. The second equation might seem more foreboding. A reasonable intuition is to forget that these quantities are matrices and to instead think of them as scalars. Consider the following simplification: \\(z = aw + b\\) $$ w^{(g)} = \\cfrac{\\partial L}{\\partial w} = \\cfrac{\\partial L}{\\partial z} \\cfrac{\\partial z}{\\partial w} = z^{(g)} a $$ With matrices, this simple scalar product becomes a matrix product. One way to remember the exact form is to make sure that the dimensions of all the matrices are compatible for matrix multiplication. To propagate this process to earlier layers, we also need \\(\\boldsymbol{A_{l - 1}^{(g)}}\\) . The expression for that is quite similar: \\[ \\boldsymbol{A_{l - 1}^{(g)}} = \\boldsymbol{Z_{l}^{(g)}} \\boldsymbol{W_{l}}^T \\] Now, you can see why the algorithm is termed backpropagation. We start with the gradients at a layer and keep \"propagating\" them back until we hit the input layer. We have ignored the gradients of the loss with respect to the biases. That is left as an exercise to the reader.","title":"Hidden Layers"},{"location":"week-12/backward/#output-layer","text":"The only thing that remains is to compute the gradient of the loss with respect to the activations in the last layer. This is in fact the first step of the back-propagation algorithm. The gradients depend on the form of the loss, which in turn depends on the type of problem being solved:","title":"Output layer"},{"location":"week-12/backward/#regression","text":"Recall that \\(\\boldsymbol{A_L} = \\boldsymbol{\\hat{y}}\\) . The activations at the last layer are the predicted labels. \\[ \\boldsymbol{A_L^{(g)}} = \\boldsymbol{\\hat{y}} - \\boldsymbol{y} \\] As the output-activation function is linear, computing the gradient of the activations and weights for the penultimate layer in the network is straightforward. In fact, \\(\\boldsymbol{Z_{L}^{(g)}} = \\boldsymbol{A_L^{(g)}}\\) .","title":"Regression"},{"location":"week-12/backward/#multiclass-classification","text":"Recall that \\(\\boldsymbol{A_L} = \\boldsymbol{\\hat{Y}}\\) . The activations at the last layer are the predicted probabilities. \\[ \\boldsymbol{A_L^{(g)}} = - \\boldsymbol{Y} \\odot \\boldsymbol{\\hat{Y}}^{\\odot -1} \\] This notation might be new. \\(P^{\\odot -1}\\) is element-wise inverse. \\(P^{\\odot -1}_{ij} = \\frac{1}{P_{ij}}\\) . This inverse arises from differentiating the the \\(\\log\\) term in the loss function. As the output-activation function is softmax, computing the gradients of the pre-activations is less straightforward. Refer to appendix for a detailed derivation of the same. The expression for the gradients turns out to be very simple in the end: $$ \\boldsymbol{Z_L^{(g)}} = \\boldsymbol{\\hat{Y}} - \\boldsymbol{Y} $$ Note how similar the expressions are for regression and classification. We will take advantage of this fact during our implementation of neural networks.","title":"Multiclass classification"},{"location":"week-12/backward/#algorithm","text":"We can now put together all these equations together and specify the algorithm for backward pass:","title":"Algorithm"},{"location":"week-12/forward/","text":"Forward pass \u00b6 As stated earlier, the network can be seen as a function \\(h: \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{k}\\) . In the case of a multi-class classification problem, given a data-point \\(\\boldsymbol{x} \\in \\mathbb{R}^m\\) , the network outputs a vector of probabilities. $$ h(\\boldsymbol{x}) = \\boldsymbol{\\hat{y}} $$ If the feature matrix \\(\\boldsymbol{X}\\) of size \\(n \\times m\\) is passed as input, we get a matrix of probabilities, \\(\\boldsymbol{\\hat{Y}}\\) of size \\(n \\times k\\) . For a regression problem, \\(k = 1\\) and the output is interpreted as some real number. This input-output relationship can be computed in an iterative manner. This is termed as a forward pass. Activations \u00b6 First we look at what happens at one layer of the network. At any layer, there are three steps that have to be performed in this sequence: Accept input Linearly combine the inputs Apply the activation function What comes out of a layer are called the activations at that layer. The linear combination of the inputs to a layer are called the pre-activations. \\(\\boldsymbol{Z_l}\\) and \\(\\boldsymbol{A_l}\\) represent the matrices of pre-activations and activations respectively at layer \\(l\\) for \\(0 \\leq l \\leq L\\) . \\(\\boldsymbol{A_0} = \\boldsymbol{X}\\) , the feature matrix. The activation matrix \\(\\boldsymbol{A_l}\\) is of size \\(n \\times S_l\\) . Each row of the activation matrix corresponds to the activation vector for one of the \\(n\\) data-points. Algorithm \u00b6 If \\(\\boldsymbol{A_{l - 1}}\\) represents the activation matrix at layer \\(l - 1\\) , then the activations at layer \\(l\\) can be computed iteratively using the following pair of equations: \\[ \\begin{aligned} \\boldsymbol{Z_{l}} &= \\boldsymbol{A_{l - 1} W_l} + \\boldsymbol{b_l},\\quad 1 \\leq l \\leq L\\\\ \\\\ \\boldsymbol{A_{l}} &= g(\\boldsymbol{Z_{l}}),\\quad 1 \\leq l \\leq L \\end{aligned} \\] Here, \\(\\boldsymbol{A_0} = \\boldsymbol{X}\\) . The shapes of these matrices/vectors are as follows: \\(\\boldsymbol{A_{l - 1}}\\) : \\(n \\times S_{l - 1}\\) \\(\\boldsymbol{W_l}\\) : \\(S_{l - 1} \\times S_{l}\\) \\(\\boldsymbol{b_l}\\) : \\(S_l\\) \\(\\boldsymbol{Z_l}\\) : \\(n \\times S_l\\) \\(\\boldsymbol{A_{l}}\\) : \\(n \\times S_{l}\\) Note that \\(\\boldsymbol{b_{l}}\\) gets added to each row of the product \\(\\boldsymbol{A_{l - 1} W_{l}}\\) according to NumPy broadcasting rules. \\(g\\) is the hidden-layer activation function for \\(1 \\leq l \\leq L - 1\\) and the output-layer activation function for \\(l = L\\) . The final shape of the output activations at layer \\(L\\) is: \\(n\\) for regression and binary classification problems \\(n \\times k\\) for a multi-class classification problem According to our notation, \\(\\boldsymbol{A_L} = \\boldsymbol{\\hat{Y}}\\) . The algorithm can now be specfieid as given below:","title":"Forward pass"},{"location":"week-12/forward/#forward-pass","text":"As stated earlier, the network can be seen as a function \\(h: \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{k}\\) . In the case of a multi-class classification problem, given a data-point \\(\\boldsymbol{x} \\in \\mathbb{R}^m\\) , the network outputs a vector of probabilities. $$ h(\\boldsymbol{x}) = \\boldsymbol{\\hat{y}} $$ If the feature matrix \\(\\boldsymbol{X}\\) of size \\(n \\times m\\) is passed as input, we get a matrix of probabilities, \\(\\boldsymbol{\\hat{Y}}\\) of size \\(n \\times k\\) . For a regression problem, \\(k = 1\\) and the output is interpreted as some real number. This input-output relationship can be computed in an iterative manner. This is termed as a forward pass.","title":"Forward pass"},{"location":"week-12/forward/#activations","text":"First we look at what happens at one layer of the network. At any layer, there are three steps that have to be performed in this sequence: Accept input Linearly combine the inputs Apply the activation function What comes out of a layer are called the activations at that layer. The linear combination of the inputs to a layer are called the pre-activations. \\(\\boldsymbol{Z_l}\\) and \\(\\boldsymbol{A_l}\\) represent the matrices of pre-activations and activations respectively at layer \\(l\\) for \\(0 \\leq l \\leq L\\) . \\(\\boldsymbol{A_0} = \\boldsymbol{X}\\) , the feature matrix. The activation matrix \\(\\boldsymbol{A_l}\\) is of size \\(n \\times S_l\\) . Each row of the activation matrix corresponds to the activation vector for one of the \\(n\\) data-points.","title":"Activations"},{"location":"week-12/forward/#algorithm","text":"If \\(\\boldsymbol{A_{l - 1}}\\) represents the activation matrix at layer \\(l - 1\\) , then the activations at layer \\(l\\) can be computed iteratively using the following pair of equations: \\[ \\begin{aligned} \\boldsymbol{Z_{l}} &= \\boldsymbol{A_{l - 1} W_l} + \\boldsymbol{b_l},\\quad 1 \\leq l \\leq L\\\\ \\\\ \\boldsymbol{A_{l}} &= g(\\boldsymbol{Z_{l}}),\\quad 1 \\leq l \\leq L \\end{aligned} \\] Here, \\(\\boldsymbol{A_0} = \\boldsymbol{X}\\) . The shapes of these matrices/vectors are as follows: \\(\\boldsymbol{A_{l - 1}}\\) : \\(n \\times S_{l - 1}\\) \\(\\boldsymbol{W_l}\\) : \\(S_{l - 1} \\times S_{l}\\) \\(\\boldsymbol{b_l}\\) : \\(S_l\\) \\(\\boldsymbol{Z_l}\\) : \\(n \\times S_l\\) \\(\\boldsymbol{A_{l}}\\) : \\(n \\times S_{l}\\) Note that \\(\\boldsymbol{b_{l}}\\) gets added to each row of the product \\(\\boldsymbol{A_{l - 1} W_{l}}\\) according to NumPy broadcasting rules. \\(g\\) is the hidden-layer activation function for \\(1 \\leq l \\leq L - 1\\) and the output-layer activation function for \\(l = L\\) . The final shape of the output activations at layer \\(L\\) is: \\(n\\) for regression and binary classification problems \\(n \\times k\\) for a multi-class classification problem According to our notation, \\(\\boldsymbol{A_L} = \\boldsymbol{\\hat{Y}}\\) . The algorithm can now be specfieid as given below:","title":"Algorithm"},{"location":"week-12/introduction/","text":"Introduction \u00b6 Until the early 2000s, neural networks were seen as one among several species in the zoo of ML algorithms. Due to a combination of several factors, the last decade witnessed intense research activity in this area. This resulted in neural networks becoming the model of choice in many domains of data science and AI. The impact was so huge that an entire field (deep learning) has blossomed out of it. One of the most important problems that contributed to the success of this model is image classification on large datasets, specifically the ImageNet dataset . In this course, we will study the most basic type of neural network called \"feedforward networks\". In a feedforward network, an input vector undergoes a sequence of transformations before it turns into the output. Each transformation is a linear combination of inputs followed by a non-linear activation function. Recall that this is similar to what happened in logistic regression. Instead of having just one unit, we have a network of units arranged in the form of layers: The term feedforward is used as the input is always fed forward. So, a feedforward network can also be viewed as a directed acyclic graph (DAG), with the direction of the edges always leading from the input to the output. Black box \u00b6 There are two ways to look at any ML model. The easiest and somewhat superficial way is to see it as a black-box that accepts an input and gives an output. This is true of any algorithm, not just ML models. Taking this route, a neural network is a model that learns some function: $$ h: \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{k} $$ For a regression problem, \\(k = 1\\) , and the output is interpreted as some real number. Given a data-point \\(\\boldsymbol{x} \\in \\mathbb{R}^m\\) , we have: \\[ h(\\boldsymbol{x}) = y \\] In the case of a multi-class classification problem, the network outputs a vector of probabilities. $$ h(\\boldsymbol{x}) = \\boldsymbol{\\hat{y}} $$ If the feature matrix \\(\\boldsymbol{X}\\) of size \\(n \\times m\\) is passed as input, we get a matrix of probabilities, \\(\\boldsymbol{\\hat{Y}}\\) of size \\(n \\times k\\) . Since each row of the matrix corresponds to a probability distribution over the \\(k\\) classes, the following relation holds: \\[ \\sum \\limits_{j = 1}^{k} \\hat{Y}_{ij} = 1 \\quad 1 \\leq i \\leq n \\] This equation can also be vectorized as follows: $$ \\boldsymbol{\\hat{Y}} \\boldsymbol{1}_{k} = \\boldsymbol{1}_n $$ Here, \\(\\boldsymbol{1}_k\\) and \\(\\boldsymbol{1}_n\\) are vectors of ones of sizes \\(k\\) and \\(n\\) respectively. For example: \\[ \\boldsymbol{1}_k = \\begin{bmatrix} 1\\\\ \\vdots\\\\ 1 \\end{bmatrix} \\] In terms of NumPy , this is: one_k = np.ones(k) one_n = np.ones(n) This notation helps us express the row-wise sum of a matrix. This may seem like an arbitrary point now, but this notation will keep making an appearance at various places. The NumPy equivalent of the expression \\(\\boldsymbol{\\hat{Y}1}_k\\) is this: Y_hat.sum(axis = 1)","title":"Introduction"},{"location":"week-12/introduction/#introduction","text":"Until the early 2000s, neural networks were seen as one among several species in the zoo of ML algorithms. Due to a combination of several factors, the last decade witnessed intense research activity in this area. This resulted in neural networks becoming the model of choice in many domains of data science and AI. The impact was so huge that an entire field (deep learning) has blossomed out of it. One of the most important problems that contributed to the success of this model is image classification on large datasets, specifically the ImageNet dataset . In this course, we will study the most basic type of neural network called \"feedforward networks\". In a feedforward network, an input vector undergoes a sequence of transformations before it turns into the output. Each transformation is a linear combination of inputs followed by a non-linear activation function. Recall that this is similar to what happened in logistic regression. Instead of having just one unit, we have a network of units arranged in the form of layers: The term feedforward is used as the input is always fed forward. So, a feedforward network can also be viewed as a directed acyclic graph (DAG), with the direction of the edges always leading from the input to the output.","title":"Introduction"},{"location":"week-12/introduction/#black-box","text":"There are two ways to look at any ML model. The easiest and somewhat superficial way is to see it as a black-box that accepts an input and gives an output. This is true of any algorithm, not just ML models. Taking this route, a neural network is a model that learns some function: $$ h: \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{k} $$ For a regression problem, \\(k = 1\\) , and the output is interpreted as some real number. Given a data-point \\(\\boldsymbol{x} \\in \\mathbb{R}^m\\) , we have: \\[ h(\\boldsymbol{x}) = y \\] In the case of a multi-class classification problem, the network outputs a vector of probabilities. $$ h(\\boldsymbol{x}) = \\boldsymbol{\\hat{y}} $$ If the feature matrix \\(\\boldsymbol{X}\\) of size \\(n \\times m\\) is passed as input, we get a matrix of probabilities, \\(\\boldsymbol{\\hat{Y}}\\) of size \\(n \\times k\\) . Since each row of the matrix corresponds to a probability distribution over the \\(k\\) classes, the following relation holds: \\[ \\sum \\limits_{j = 1}^{k} \\hat{Y}_{ij} = 1 \\quad 1 \\leq i \\leq n \\] This equation can also be vectorized as follows: $$ \\boldsymbol{\\hat{Y}} \\boldsymbol{1}_{k} = \\boldsymbol{1}_n $$ Here, \\(\\boldsymbol{1}_k\\) and \\(\\boldsymbol{1}_n\\) are vectors of ones of sizes \\(k\\) and \\(n\\) respectively. For example: \\[ \\boldsymbol{1}_k = \\begin{bmatrix} 1\\\\ \\vdots\\\\ 1 \\end{bmatrix} \\] In terms of NumPy , this is: one_k = np.ones(k) one_n = np.ones(n) This notation helps us express the row-wise sum of a matrix. This may seem like an arbitrary point now, but this notation will keep making an appearance at various places. The NumPy equivalent of the expression \\(\\boldsymbol{\\hat{Y}1}_k\\) is this: Y_hat.sum(axis = 1)","title":"Black box"},{"location":"week-12/learning-algo/","text":"Learning Algorithm \u00b6 We now have all the ingredients to specify the learning algorithm. Let \\(\\boldsymbol{\\theta}\\) refer to all the parameters in the model. We can now define the following functions: \\(\\boldsymbol{\\hat{Y}} = \\text{forward-pass}(\\boldsymbol{X})\\) \\(L = \\text{loss}(\\boldsymbol{Y}, \\boldsymbol{\\hat{Y}})\\) \\(\\boldsymbol{\\theta^{(g)}} = \\text{backward-pass}(\\boldsymbol{Y}, \\boldsymbol{\\hat{Y}})\\) Only the most important arguments are displayed here. With this, we can define the learning algorithm for neural networks:","title":"Learning Algorithm"},{"location":"week-12/learning-algo/#learning-algorithm","text":"We now have all the ingredients to specify the learning algorithm. Let \\(\\boldsymbol{\\theta}\\) refer to all the parameters in the model. We can now define the following functions: \\(\\boldsymbol{\\hat{Y}} = \\text{forward-pass}(\\boldsymbol{X})\\) \\(L = \\text{loss}(\\boldsymbol{Y}, \\boldsymbol{\\hat{Y}})\\) \\(\\boldsymbol{\\theta^{(g)}} = \\text{backward-pass}(\\boldsymbol{Y}, \\boldsymbol{\\hat{Y}})\\) Only the most important arguments are displayed here. With this, we can define the learning algorithm for neural networks:","title":"Learning Algorithm"},{"location":"week-12/loss/","text":"Loss \u00b6 The loss depends on the nature of the problem being solved. Regression \u00b6 \\(\\boldsymbol{y}\\) is a vector of target label for \\(n\\) examples. \\(\\boldsymbol{\\hat{y}}\\) is the output of the network and corresponds to the predicted labels. Both vectors are of size \\(n\\) . The loss is our usual squared error: $$ L(\\boldsymbol{y}, \\boldsymbol{\\hat{y}}) = \\cfrac{1}{2} \\cdot (\\boldsymbol{\\hat{y}} - \\boldsymbol{y})^T (\\boldsymbol{\\hat{y}} - \\boldsymbol{y}) $$ Multi-class classification \u00b6 \\(\\boldsymbol{Y}\\) is a matrix of target labels for \\(n\\) examples. Each row of this matrix is a one-hot vector. \\(\\boldsymbol{\\hat{Y}}\\) is a matrix of predicted probabilities. Both matrices are of size \\(n \\times k\\) . The categorical cross-entropy loss is given as follows: $$ L(\\boldsymbol{Y}, \\boldsymbol{\\hat{Y}})=-\\boldsymbol{1}^T_{n} \\left( \\boldsymbol{Y} \\odot \\log \\boldsymbol{\\hat{Y}} \\right) \\boldsymbol{1}_{k} $$ This equation can be understood as follows: \\(\\boldsymbol{1}_n\\) and \\(\\boldsymbol{1}_k\\) are vectors of ones of sizes \\(n\\) and \\(k\\) respectively. If \\(\\boldsymbol{M}\\) is a matrix of size \\(n \\times k\\) , then \\(\\boldsymbol{1}^T_{n} \\boldsymbol{M} \\boldsymbol{1}_k\\) is the sum of all elements in the matrix. \\(\\odot\\) is the element-wise product. The NumPy equivalent of the loss is: L = -np.sum(Y * np.log(Y_hat)) Note that the loss function is always a scalar. To understand why the cross-entropy takes this form, refer to the appendix.","title":"Loss"},{"location":"week-12/loss/#loss","text":"The loss depends on the nature of the problem being solved.","title":"Loss"},{"location":"week-12/loss/#regression","text":"\\(\\boldsymbol{y}\\) is a vector of target label for \\(n\\) examples. \\(\\boldsymbol{\\hat{y}}\\) is the output of the network and corresponds to the predicted labels. Both vectors are of size \\(n\\) . The loss is our usual squared error: $$ L(\\boldsymbol{y}, \\boldsymbol{\\hat{y}}) = \\cfrac{1}{2} \\cdot (\\boldsymbol{\\hat{y}} - \\boldsymbol{y})^T (\\boldsymbol{\\hat{y}} - \\boldsymbol{y}) $$","title":"Regression"},{"location":"week-12/loss/#multi-class-classification","text":"\\(\\boldsymbol{Y}\\) is a matrix of target labels for \\(n\\) examples. Each row of this matrix is a one-hot vector. \\(\\boldsymbol{\\hat{Y}}\\) is a matrix of predicted probabilities. Both matrices are of size \\(n \\times k\\) . The categorical cross-entropy loss is given as follows: $$ L(\\boldsymbol{Y}, \\boldsymbol{\\hat{Y}})=-\\boldsymbol{1}^T_{n} \\left( \\boldsymbol{Y} \\odot \\log \\boldsymbol{\\hat{Y}} \\right) \\boldsymbol{1}_{k} $$ This equation can be understood as follows: \\(\\boldsymbol{1}_n\\) and \\(\\boldsymbol{1}_k\\) are vectors of ones of sizes \\(n\\) and \\(k\\) respectively. If \\(\\boldsymbol{M}\\) is a matrix of size \\(n \\times k\\) , then \\(\\boldsymbol{1}^T_{n} \\boldsymbol{M} \\boldsymbol{1}_k\\) is the sum of all elements in the matrix. \\(\\odot\\) is the element-wise product. The NumPy equivalent of the loss is: L = -np.sum(Y * np.log(Y_hat)) Note that the loss function is always a scalar. To understand why the cross-entropy takes this form, refer to the appendix.","title":"Multi-class classification"},{"location":"week-12/model/","text":"Model \u00b6 A neural network has three components: layers weights activation functions Each layer is a collection of nodes, also called neurons. Layers are stacked one after the other. Neurons in successive layers are connected to each other by means of an edge. Edges have weights to denote the importance of the connections. The number of layers and their sizes determine the shape of the weights. Activation functions are independent of the layers and weights. With these three components, we can define the model equations that compute the output given the input. In the case of neural networks, this is called a forward pass and will be discussed in the next section. Layers \u00b6 All networks have at least three layers: input hidden output The number of hidden layers is a hyperparameter. For the rest of the document, we will follow a convention to denote the layers. The model has \\(L + 1\\) layers. Layers are indexed using \\(l\\) . \\(l = 0\\) : input layer \\(1 \\leq l \\leq L - 1\\) : hidden layers \\(l = L\\) : output layer \\(S_l\\) : number of neurons in layer \\(l\\) \\(S_0 = m\\) , the number of features for the problem. \\(S_L\\) can take the following values: \\(1\\) , for a regression problem \\(k\\) , for a multi-class classification problem Weights and biases \u00b6 The weight matrix connecting layer \\(l - 1\\) and \\(l\\) is as follows: \\(\\boldsymbol{W_l}\\) : weight matrix size: \\(S_{l - 1} \\times S_{l}\\) \\(W_{lij}\\) : weight connecting \\(i^{th}\\) neuron in layer \\((l - 1)\\) to \\(j^{th}\\) neuron in layer \\(l\\) \\(\\boldsymbol{b_l}\\) : bias vector size: \\(S_l\\) \\(b_{lj}\\) : bias of the \\(j^{th}\\) neuron in layer \\(l\\) There are no weights before the input layer. Weights start from the first hidden layer, \\(l = 1\\) . The last weight matrix is the one connecting the final hidden layer and the output layer, \\(l = L\\) . Therefore, there are \\(L\\) learnable weights and biases, namely for \\(1 \\leq l \\leq L\\) . Activation functions \u00b6 The type of the activation function depends on the layer in which it is used: hidden layers output layer Hidden layers \u00b6 \\(g(z)\\) will be used to refer to activation function. \\(z\\) is some real number. Sigmoid \u00b6 \\[ g(z) = \\cfrac{1}{1 + e^{-z}} \\] Tanh \u00b6 \\[ g(z) = \\tanh(z) \\] ReLU \u00b6 The full form of ReLU is Rectified Linear Unit: \\[ g(z) = \\begin{cases}z,\\quad z \\geq 0\\\\0,\\quad \\text{otherwise}\\end{cases} \\] When the argument to \\(g\\) is a vector or a matrix, the activation function will be applied element-wise. Output layer \u00b6 The activation function at the output layer depends on the type of problem being solved: regression or classification, and could be one of the following: Linear: \\(g(z) = z\\) Here, \\(z\\) is a scalar activation at the output, that is, there is exactly one neuron in the output layer. This is used for regression problems. Softmax: \\(g(\\boldsymbol{z}) = \\begin{bmatrix} \\cdots & \\cfrac{e^{z_i}}{\\sum \\limits_{r = 1}^{k} e^{z_r}} & \\cdots\\end{bmatrix}^T\\) Here, \\(\\boldsymbol{z}\\) is a vector of activations at the output layer, that is, there are \\(k\\) neurons in the output layer. This is used in multiclass classification problems. If the argument is a matrix, then the softmax will be applied row-wise. Network Architecture \u00b6 A network architecture is a specification of the number of layers, the number of neurons in each layer, and the activation functions used at these layers. A sample is shown below: Layer index ( \\(l\\) ) Type Neurons Activation function \\(0\\) Input \\(20\\) NA \\(1\\) Hidden \\(30\\) ReLU \\(2\\) Output \\(10\\) Softmax This is an architecture for a multiclass classification network with \\(10\\) classes. There is exactly one hidden layer. The number of parameters of this network can be computed as follows: Layer index ( \\(l\\) ) Number of weights Number of biases Number of parameters \\(0\\) \\(0\\) \\(0\\) \\(0\\) \\(1\\) \\(20 \\times 30 = 600\\) \\(30\\) \\(630\\) \\(2\\) \\(30 \\times 10 = 300\\) \\(10\\) \\(310\\) Total \\(940\\) Notice how a simple neural network with just one hidden layer has nearly \\(1000\\) parameters! Neural networks with a large number of hidden layers are thus prone to overfitting.","title":"Model"},{"location":"week-12/model/#model","text":"A neural network has three components: layers weights activation functions Each layer is a collection of nodes, also called neurons. Layers are stacked one after the other. Neurons in successive layers are connected to each other by means of an edge. Edges have weights to denote the importance of the connections. The number of layers and their sizes determine the shape of the weights. Activation functions are independent of the layers and weights. With these three components, we can define the model equations that compute the output given the input. In the case of neural networks, this is called a forward pass and will be discussed in the next section.","title":"Model"},{"location":"week-12/model/#layers","text":"All networks have at least three layers: input hidden output The number of hidden layers is a hyperparameter. For the rest of the document, we will follow a convention to denote the layers. The model has \\(L + 1\\) layers. Layers are indexed using \\(l\\) . \\(l = 0\\) : input layer \\(1 \\leq l \\leq L - 1\\) : hidden layers \\(l = L\\) : output layer \\(S_l\\) : number of neurons in layer \\(l\\) \\(S_0 = m\\) , the number of features for the problem. \\(S_L\\) can take the following values: \\(1\\) , for a regression problem \\(k\\) , for a multi-class classification problem","title":"Layers"},{"location":"week-12/model/#weights-and-biases","text":"The weight matrix connecting layer \\(l - 1\\) and \\(l\\) is as follows: \\(\\boldsymbol{W_l}\\) : weight matrix size: \\(S_{l - 1} \\times S_{l}\\) \\(W_{lij}\\) : weight connecting \\(i^{th}\\) neuron in layer \\((l - 1)\\) to \\(j^{th}\\) neuron in layer \\(l\\) \\(\\boldsymbol{b_l}\\) : bias vector size: \\(S_l\\) \\(b_{lj}\\) : bias of the \\(j^{th}\\) neuron in layer \\(l\\) There are no weights before the input layer. Weights start from the first hidden layer, \\(l = 1\\) . The last weight matrix is the one connecting the final hidden layer and the output layer, \\(l = L\\) . Therefore, there are \\(L\\) learnable weights and biases, namely for \\(1 \\leq l \\leq L\\) .","title":"Weights and biases"},{"location":"week-12/model/#activation-functions","text":"The type of the activation function depends on the layer in which it is used: hidden layers output layer","title":"Activation functions"},{"location":"week-12/model/#hidden-layers","text":"\\(g(z)\\) will be used to refer to activation function. \\(z\\) is some real number.","title":"Hidden layers"},{"location":"week-12/model/#sigmoid","text":"\\[ g(z) = \\cfrac{1}{1 + e^{-z}} \\]","title":"Sigmoid"},{"location":"week-12/model/#tanh","text":"\\[ g(z) = \\tanh(z) \\]","title":"Tanh"},{"location":"week-12/model/#relu","text":"The full form of ReLU is Rectified Linear Unit: \\[ g(z) = \\begin{cases}z,\\quad z \\geq 0\\\\0,\\quad \\text{otherwise}\\end{cases} \\] When the argument to \\(g\\) is a vector or a matrix, the activation function will be applied element-wise.","title":"ReLU"},{"location":"week-12/model/#output-layer","text":"The activation function at the output layer depends on the type of problem being solved: regression or classification, and could be one of the following: Linear: \\(g(z) = z\\) Here, \\(z\\) is a scalar activation at the output, that is, there is exactly one neuron in the output layer. This is used for regression problems. Softmax: \\(g(\\boldsymbol{z}) = \\begin{bmatrix} \\cdots & \\cfrac{e^{z_i}}{\\sum \\limits_{r = 1}^{k} e^{z_r}} & \\cdots\\end{bmatrix}^T\\) Here, \\(\\boldsymbol{z}\\) is a vector of activations at the output layer, that is, there are \\(k\\) neurons in the output layer. This is used in multiclass classification problems. If the argument is a matrix, then the softmax will be applied row-wise.","title":"Output layer"},{"location":"week-12/model/#network-architecture","text":"A network architecture is a specification of the number of layers, the number of neurons in each layer, and the activation functions used at these layers. A sample is shown below: Layer index ( \\(l\\) ) Type Neurons Activation function \\(0\\) Input \\(20\\) NA \\(1\\) Hidden \\(30\\) ReLU \\(2\\) Output \\(10\\) Softmax This is an architecture for a multiclass classification network with \\(10\\) classes. There is exactly one hidden layer. The number of parameters of this network can be computed as follows: Layer index ( \\(l\\) ) Number of weights Number of biases Number of parameters \\(0\\) \\(0\\) \\(0\\) \\(0\\) \\(1\\) \\(20 \\times 30 = 600\\) \\(30\\) \\(630\\) \\(2\\) \\(30 \\times 10 = 300\\) \\(10\\) \\(310\\) Total \\(940\\) Notice how a simple neural network with just one hidden layer has nearly \\(1000\\) parameters! Neural networks with a large number of hidden layers are thus prone to overfitting.","title":"Network Architecture"},{"location":"week-12/preliminaries/","text":"Preliminaries \u00b6 Problems \u00b6 We will discuss two types of problems: Regression Multiclass classification We won't be discussing binary classification explicitly. Most of the discussion about neural networks apply to both classes of problems. There are a handful of places where the network has to be specified differently for regression and classification. Watch out for these instances. Notation \u00b6 Scalars will be represented using normal font, lower-case letters. Vectors will be represented using bold font, lower-case letters. Matrices will be represented using bold font, upper-case letters. When indexing elements of a vector or a matrix, normal font will be used, but the case will be inherited from the object that is being indexed. \\(a\\) : scalar \\(\\boldsymbol{a}\\) : vector \\(\\boldsymbol{A}\\) : matrix \\(a_i\\) : \\(i^{\\text{th}}\\) component of the vector and \\(A_{ij}\\) : \\(j^{th}\\) element in the \\(i^{\\text{th}}\\) row of the matrix. Indices are used minimally as most of the equations are vectorized. This is a convention that we will largely stick to. But we might have to override them in a few occasions. In such situations, the nature of the object should be inferred from the context. Data \u00b6 The data-matrix is \\(\\boldsymbol{X}\\) : size: \\(n \\times m\\) \\(n\\) data-points \\(m\\) features The data-matrix is common to both problems. Labels are represented differently in the case of regression and classification: Regression \u00b6 The predicted labels for regression is \\(\\boldsymbol{y}\\) , a vector of real numbers: size: \\(n\\) \\(n\\) data-points single target corresponding to each point Multiclass Classification \u00b6 The one-hot matrix of labels for a multiclass classification problem is \\(\\boldsymbol{Y}\\) : size: \\(n \\times k\\) \\(n\\) data-points \\(k\\) classes Each row of the matrix is a one-hot vector.","title":"Preliminaries"},{"location":"week-12/preliminaries/#preliminaries","text":"","title":"Preliminaries"},{"location":"week-12/preliminaries/#problems","text":"We will discuss two types of problems: Regression Multiclass classification We won't be discussing binary classification explicitly. Most of the discussion about neural networks apply to both classes of problems. There are a handful of places where the network has to be specified differently for regression and classification. Watch out for these instances.","title":"Problems"},{"location":"week-12/preliminaries/#notation","text":"Scalars will be represented using normal font, lower-case letters. Vectors will be represented using bold font, lower-case letters. Matrices will be represented using bold font, upper-case letters. When indexing elements of a vector or a matrix, normal font will be used, but the case will be inherited from the object that is being indexed. \\(a\\) : scalar \\(\\boldsymbol{a}\\) : vector \\(\\boldsymbol{A}\\) : matrix \\(a_i\\) : \\(i^{\\text{th}}\\) component of the vector and \\(A_{ij}\\) : \\(j^{th}\\) element in the \\(i^{\\text{th}}\\) row of the matrix. Indices are used minimally as most of the equations are vectorized. This is a convention that we will largely stick to. But we might have to override them in a few occasions. In such situations, the nature of the object should be inferred from the context.","title":"Notation"},{"location":"week-12/preliminaries/#data","text":"The data-matrix is \\(\\boldsymbol{X}\\) : size: \\(n \\times m\\) \\(n\\) data-points \\(m\\) features The data-matrix is common to both problems. Labels are represented differently in the case of regression and classification:","title":"Data"},{"location":"week-12/preliminaries/#regression","text":"The predicted labels for regression is \\(\\boldsymbol{y}\\) , a vector of real numbers: size: \\(n\\) \\(n\\) data-points single target corresponding to each point","title":"Regression"},{"location":"week-12/preliminaries/#multiclass-classification","text":"The one-hot matrix of labels for a multiclass classification problem is \\(\\boldsymbol{Y}\\) : size: \\(n \\times k\\) \\(n\\) data-points \\(k\\) classes Each row of the matrix is a one-hot vector.","title":"Multiclass Classification"},{"location":"week-3/week-3_1/","text":"Polynomial regression \u00b6 Question What if there is no linear relationship between input features and the output label? If your data is more complex than a simple straight line, how will you fit a linear regression model to it? Take a look at the data shown below. Let us fit a linear regression model \\(h_\\mathbf{w}(x)=\\color{red}w_0\\color{black}+\\color{red}w_1\\color{black}x\\) for this data. Clearly, this is not a great fit and we need to search for a better model. Polynomial regression \u00b6 We can create polynomial features by combining the existing input features as new features, then train a linear model on this extended set of features. Let \\(\\phi_k\\) (features) be called polynomial transformation of \\(k\\) -th order. Comparison between linear regression and polynomial regression model. Let's represent polynomial transformation in form of a vector \\(\\phi\\) with \\(k\\) components. \\[ \\mathbf{\\phi} = \\begin{bmatrix} \\color{purple}{\\phi_0} \\\\ \\color{purple}{\\phi_1} \\\\ \\color{purple}{\\vdots} \\\\ \\color{purple}{\\phi_k} \\end{bmatrix} \\] Each component denotes a specific transform to be applied to the input feature. The polynomial regression model becomes: \\[ \\begin{aligned} y &= \\sum_{i=0}^{k} w_i ({\\phi}(\\mathbf{x}))_i \\\\ \\\\ y &= \\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x}) \\\\ \\end{aligned} \\] Polynomial Regression is capable of finding relationships between features (which is something a plain Linear Regression model cannot do). Note : The model is a non-linear function of \\((x)\\) , but is a linear function of weight vector ( \\(\\textbf w=[w_0, w_1, \\ldots, w_k]\\) ) Polynomial Features (degree = \\(d\\) ) transforms an array containing \\(n\\) features into an array containing \\(\\cfrac{(n+d)!}{d!n!}\\) features, where \\(n!\\) is the factorial of \\(n\\) , equal to \\(1 \\times 2 \\times 3 \\times \\cdots \\times n\\) .","title":"Polynomial regression"},{"location":"week-3/week-3_1/#polynomial-regression","text":"Question What if there is no linear relationship between input features and the output label? If your data is more complex than a simple straight line, how will you fit a linear regression model to it? Take a look at the data shown below. Let us fit a linear regression model \\(h_\\mathbf{w}(x)=\\color{red}w_0\\color{black}+\\color{red}w_1\\color{black}x\\) for this data. Clearly, this is not a great fit and we need to search for a better model.","title":"Polynomial regression"},{"location":"week-3/week-3_1/#polynomial-regression_1","text":"We can create polynomial features by combining the existing input features as new features, then train a linear model on this extended set of features. Let \\(\\phi_k\\) (features) be called polynomial transformation of \\(k\\) -th order. Comparison between linear regression and polynomial regression model. Let's represent polynomial transformation in form of a vector \\(\\phi\\) with \\(k\\) components. \\[ \\mathbf{\\phi} = \\begin{bmatrix} \\color{purple}{\\phi_0} \\\\ \\color{purple}{\\phi_1} \\\\ \\color{purple}{\\vdots} \\\\ \\color{purple}{\\phi_k} \\end{bmatrix} \\] Each component denotes a specific transform to be applied to the input feature. The polynomial regression model becomes: \\[ \\begin{aligned} y &= \\sum_{i=0}^{k} w_i ({\\phi}(\\mathbf{x}))_i \\\\ \\\\ y &= \\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x}) \\\\ \\end{aligned} \\] Polynomial Regression is capable of finding relationships between features (which is something a plain Linear Regression model cannot do). Note : The model is a non-linear function of \\((x)\\) , but is a linear function of weight vector ( \\(\\textbf w=[w_0, w_1, \\ldots, w_k]\\) ) Polynomial Features (degree = \\(d\\) ) transforms an array containing \\(n\\) features into an array containing \\(\\cfrac{(n+d)!}{d!n!}\\) features, where \\(n!\\) is the factorial of \\(n\\) , equal to \\(1 \\times 2 \\times 3 \\times \\cdots \\times n\\) .","title":"Polynomial regression"},{"location":"week-3/week-3_2/","text":"Generating Polynomial Transformations \u00b6 Question What are the steps for generating a polynomial transformation of degree \\(M\\) ? 1. Generate combinations of input features of lengths \\(= 0, 1, \\ldots, M\\) . 2. Perform multiplication operations on existing features to obtain the new features. For a single feature \\(x_1, \\phi_m = \\left[1, x_1^1, x_1^2, \\ldots, x_1^m \\right]\\) Generate combinations of \\(\\\\{1, x_1, (x_1,x_1) (x_1, x_1, x_1), \\ldots, (x_1, x_1, \\ldots,( m \\space \\text{times})\\\\}\\) 0-th degree: 1 (bias) 1-st degree: \\(x_1\\) 2-nd degree: \\((x_1, x_1)\\) 3-rd degree: \\((x_1, x_1, x_1)\\) m-th degree: \\((x_1, x_1, x_1, \\ldots m \\text{times})\\) Taking the product of elements in combination: \\(\\phi_m(x_1) = {1, x_1, (x_1*x_1), (x_1* x_1*x_1), \\ldots, \\pi_{i=1}^{m} x_1}\\) \\(\\phi_m(x_1) = \\{1, x_1, x_1^2, x_1^3, \\ldots, x_1^m\\}\\) For two features say \\((x_1, x_2)\\) , obtain \\(\\phi_2(x_1, x_2)\\) : Generate combinations of \\(\\{1, x_1, x_2, (x_1, x_1), (x_2, x_2), (x_1, x_2)\\}\\) 0-th degree: 1 (bias) 1-st degree: \\(x_1, x_2\\) 2-nd degree: \\((x_1, x_1), (x_1, x_2), (x_2, x_2)\\) Taking the product of elements in combination: \\(\\phi_2(x_1, x_2) = \\{1, x_1, x_2, (x_1*x_1), (x_2*x_2), (x_1*x_2)\\}\\) \\(\\phi_2(x_1, x_2) = \\{1, x_1, x_2, x_1^2, x_2^2, x_1 x_2 \\}\\) PolynomialFeatures with degree=3 would not only add the features \\(x_1^2\\) , \\(x_2^3\\) , \\(x_2^2\\) and \\(x_3^3\\) , but also the combinations \\(x_1x_2\\) , \\(x_1^2x_2\\) , and \\(x_1x_2^2\\) . Examples For input feature vector \\(x\\) , let us compute polynomial features \\(\\phi_2\\) with degree = 2. \\(x = \\begin{bmatrix} 2 \\end{bmatrix}\\) \\(\\rightarrow \\phi_2= \\begin{bmatrix} 1 & 2 & 4 \\end{bmatrix}\\) \\(x = \\begin{bmatrix} 2 & 3 \\\\ 5 & 6 \\end{bmatrix}\\) \\(\\rightarrow \\phi_2= \\begin{bmatrix} 1 & 2 & 3 & 4 & 6 & 9 \\\\ 1 & 5 & 6 & 25 & 30 & 36 \\end{bmatrix}\\) Now, let us fit polynomial regression models of degrees 2 to 9 to the data that we had initially. What did you infer from the above plots?","title":"Generating Polynomial Transformations"},{"location":"week-3/week-3_2/#generating-polynomial-transformations","text":"Question What are the steps for generating a polynomial transformation of degree \\(M\\) ? 1. Generate combinations of input features of lengths \\(= 0, 1, \\ldots, M\\) . 2. Perform multiplication operations on existing features to obtain the new features. For a single feature \\(x_1, \\phi_m = \\left[1, x_1^1, x_1^2, \\ldots, x_1^m \\right]\\) Generate combinations of \\(\\\\{1, x_1, (x_1,x_1) (x_1, x_1, x_1), \\ldots, (x_1, x_1, \\ldots,( m \\space \\text{times})\\\\}\\) 0-th degree: 1 (bias) 1-st degree: \\(x_1\\) 2-nd degree: \\((x_1, x_1)\\) 3-rd degree: \\((x_1, x_1, x_1)\\) m-th degree: \\((x_1, x_1, x_1, \\ldots m \\text{times})\\) Taking the product of elements in combination: \\(\\phi_m(x_1) = {1, x_1, (x_1*x_1), (x_1* x_1*x_1), \\ldots, \\pi_{i=1}^{m} x_1}\\) \\(\\phi_m(x_1) = \\{1, x_1, x_1^2, x_1^3, \\ldots, x_1^m\\}\\) For two features say \\((x_1, x_2)\\) , obtain \\(\\phi_2(x_1, x_2)\\) : Generate combinations of \\(\\{1, x_1, x_2, (x_1, x_1), (x_2, x_2), (x_1, x_2)\\}\\) 0-th degree: 1 (bias) 1-st degree: \\(x_1, x_2\\) 2-nd degree: \\((x_1, x_1), (x_1, x_2), (x_2, x_2)\\) Taking the product of elements in combination: \\(\\phi_2(x_1, x_2) = \\{1, x_1, x_2, (x_1*x_1), (x_2*x_2), (x_1*x_2)\\}\\) \\(\\phi_2(x_1, x_2) = \\{1, x_1, x_2, x_1^2, x_2^2, x_1 x_2 \\}\\) PolynomialFeatures with degree=3 would not only add the features \\(x_1^2\\) , \\(x_2^3\\) , \\(x_2^2\\) and \\(x_3^3\\) , but also the combinations \\(x_1x_2\\) , \\(x_1^2x_2\\) , and \\(x_1x_2^2\\) . Examples For input feature vector \\(x\\) , let us compute polynomial features \\(\\phi_2\\) with degree = 2. \\(x = \\begin{bmatrix} 2 \\end{bmatrix}\\) \\(\\rightarrow \\phi_2= \\begin{bmatrix} 1 & 2 & 4 \\end{bmatrix}\\) \\(x = \\begin{bmatrix} 2 & 3 \\\\ 5 & 6 \\end{bmatrix}\\) \\(\\rightarrow \\phi_2= \\begin{bmatrix} 1 & 2 & 3 & 4 & 6 & 9 \\\\ 1 & 5 & 6 & 25 & 30 & 36 \\end{bmatrix}\\) Now, let us fit polynomial regression models of degrees 2 to 9 to the data that we had initially. What did you infer from the above plots?","title":"Generating Polynomial Transformations"},{"location":"week-3/week-3_3/","text":"Finding degree \u00b6 Question How to find the right degree? If you perform high-degree Polynomial Regression, you will likely fit the training data much better than with plain Linear Regression. Let us apply a degree- \\(200\\) polynomial model to the preceding training data, and compare the result with a linear model and cubic model ( \\(3^{rd}\\) -degree polynomial). Notice how the 200-degree polynomial model wiggles around to get as close as possible to the training instances. Of course, this high-degree Polynomial Regression model is overfitting the training data, while the linear model is underfitting it. The model that will generalize best, in this case, is the cubic model. It makes sense, right.!! In general, you won\u2019t know what function generated the data, so: How can you decide how complex your model should be? How can you tell that your model is overfitting or underfitting the data? Solution: Learning curves","title":"Finding degree"},{"location":"week-3/week-3_3/#finding-degree","text":"Question How to find the right degree? If you perform high-degree Polynomial Regression, you will likely fit the training data much better than with plain Linear Regression. Let us apply a degree- \\(200\\) polynomial model to the preceding training data, and compare the result with a linear model and cubic model ( \\(3^{rd}\\) -degree polynomial). Notice how the 200-degree polynomial model wiggles around to get as close as possible to the training instances. Of course, this high-degree Polynomial Regression model is overfitting the training data, while the linear model is underfitting it. The model that will generalize best, in this case, is the cubic model. It makes sense, right.!! In general, you won\u2019t know what function generated the data, so: How can you decide how complex your model should be? How can you tell that your model is overfitting or underfitting the data? Solution: Learning curves","title":"Finding degree"},{"location":"week-3/week-3_4/","text":"Learning curves \u00b6 These are plots of the model\u2019s performance on the training set and the validation set as a function of the degree. To generate the plots, follow these steps. Step 1: Train different polynomial model of degrees: \\(({0, 1, \\ldots, k})\\) Step 2: Calculate training and validation root mean square error (RMSE). Step 3: Plot degree vs. RMSE plot. Inference: The training and validation errors are close until certain degree. After a point, the training error continues to reduce, while the validation error keeps increasing. RMSE increases sharply for degrees \\(\\ge 7\\) . This is a signature of overfitting. Issues with polynomial regression Higher order polynomial models are very flexible, or in other words, they have higher capacity compared to lower order models. Hence they are prone to overfitting compared to the lower degree polynomials. Perfect fit to training data, but poor prediction accuracy on validation data.","title":"Learning curves"},{"location":"week-3/week-3_4/#learning-curves","text":"These are plots of the model\u2019s performance on the training set and the validation set as a function of the degree. To generate the plots, follow these steps. Step 1: Train different polynomial model of degrees: \\(({0, 1, \\ldots, k})\\) Step 2: Calculate training and validation root mean square error (RMSE). Step 3: Plot degree vs. RMSE plot. Inference: The training and validation errors are close until certain degree. After a point, the training error continues to reduce, while the validation error keeps increasing. RMSE increases sharply for degrees \\(\\ge 7\\) . This is a signature of overfitting. Issues with polynomial regression Higher order polynomial models are very flexible, or in other words, they have higher capacity compared to lower order models. Hence they are prone to overfitting compared to the lower degree polynomials. Perfect fit to training data, but poor prediction accuracy on validation data.","title":"Learning curves"}]}