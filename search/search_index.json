{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Machine Learning Techniques \u00b6 This is a set of notes for the MLT course.","title":"Machine Learning Techniques"},{"location":"#machine-learning-techniques","text":"This is a set of notes for the MLT course.","title":"Machine Learning Techniques"},{"location":"neural-networks/00-preliminaries/","text":"Preliminaries \u00b6 Problems \u00b6 We will discuss two types of problems: Regression Multiclass classification We won't be discussing binary classification explicitly. Most of the discussion about neural networks apply to both classes of problems. There are a handful of places where the network has to be specified differently for regression and classification. Watch out for these instances. Notation \u00b6 Scalars will be represented using normal font, lower-case letters. Vectors will be represented using bold font, lower-case letters. Matrices will be represented using bold font, upper-case letters. When indexing elements of a vector or a matrix, normal font will be used, but the case will be inherited from the object that is being indexed. \\(a\\) : scalar \\(\\boldsymbol{a}\\) : vector \\(\\boldsymbol{A}\\) : matrix \\(a_i\\) : \\(i^{\\text{th}}\\) component of the vector and \\(A_{ij}\\) : \\(j^{th}\\) element in the \\(i^{\\text{th}}\\) row of the matrix. Indices are used minimally as most of the equations are vectorized. This is a convention that we will largely stick to. But we might have to override them in a few occasions. In such situations, the nature of the object should be inferred from the context. Data \u00b6 The data-matrix is \\(\\boldsymbol{X}\\) : size: \\(n \\times m\\) \\(n\\) data-points \\(m\\) features The data-matrix is common to both problems. Labels are represented differently in the case of regression and classification: Regression \u00b6 The predicted labels for regression is \\(\\boldsymbol{y}\\) , a vector of real numbers: size: \\(n\\) \\(n\\) data-points single target corresponding to each point Multiclass Classification \u00b6 The one-hot matrix of labels for a multiclass classification problem is \\(\\boldsymbol{Y}\\) : size: \\(n \\times k\\) \\(n\\) data-points \\(k\\) classes Each row of the matrix is a one-hot vector.","title":"Preliminaries"},{"location":"neural-networks/00-preliminaries/#preliminaries","text":"","title":"Preliminaries"},{"location":"neural-networks/00-preliminaries/#problems","text":"We will discuss two types of problems: Regression Multiclass classification We won't be discussing binary classification explicitly. Most of the discussion about neural networks apply to both classes of problems. There are a handful of places where the network has to be specified differently for regression and classification. Watch out for these instances.","title":"Problems"},{"location":"neural-networks/00-preliminaries/#notation","text":"Scalars will be represented using normal font, lower-case letters. Vectors will be represented using bold font, lower-case letters. Matrices will be represented using bold font, upper-case letters. When indexing elements of a vector or a matrix, normal font will be used, but the case will be inherited from the object that is being indexed. \\(a\\) : scalar \\(\\boldsymbol{a}\\) : vector \\(\\boldsymbol{A}\\) : matrix \\(a_i\\) : \\(i^{\\text{th}}\\) component of the vector and \\(A_{ij}\\) : \\(j^{th}\\) element in the \\(i^{\\text{th}}\\) row of the matrix. Indices are used minimally as most of the equations are vectorized. This is a convention that we will largely stick to. But we might have to override them in a few occasions. In such situations, the nature of the object should be inferred from the context.","title":"Notation"},{"location":"neural-networks/00-preliminaries/#data","text":"The data-matrix is \\(\\boldsymbol{X}\\) : size: \\(n \\times m\\) \\(n\\) data-points \\(m\\) features The data-matrix is common to both problems. Labels are represented differently in the case of regression and classification:","title":"Data"},{"location":"neural-networks/00-preliminaries/#regression","text":"The predicted labels for regression is \\(\\boldsymbol{y}\\) , a vector of real numbers: size: \\(n\\) \\(n\\) data-points single target corresponding to each point","title":"Regression"},{"location":"neural-networks/00-preliminaries/#multiclass-classification","text":"The one-hot matrix of labels for a multiclass classification problem is \\(\\boldsymbol{Y}\\) : size: \\(n \\times k\\) \\(n\\) data-points \\(k\\) classes Each row of the matrix is a one-hot vector.","title":"Multiclass Classification"},{"location":"neural-networks/01-introduction/","text":"Introduction \u00b6 Until the early 2000s, neural networks were seen as one among several species in the zoo of ML algorithms. Due to a combination of several factors, the last decade witnessed intense research activity in this area. This resulted in neural networks becoming the model of choice in many domains of data science and AI. The impact was so huge that an entire field (deep learning) has blossomed out of it. One of the most important problems that contributed to the success of this model is image classification on large datasets, specifically the ImageNet dataset . In this course, we will study the most basic type of neural network called \"feedforward networks\". In a feedforward network, an input vector undergoes a sequence of transformations before it turns into the output. Each transformation is a linear combination of inputs followed by a non-linear activation function. Recall that this is similar to what happened in logistic regression. Instead of having just one unit, we have a network of units arranged in the form of layers: The term feedforward is used as the input is always fed forward. So, a feedforward network can also be viewed as a directed acyclic graph (DAG), with the direction of the edges always leading from the input to the output. Black box \u00b6 There are two ways to look at any ML model. The easiest and somewhat superficial way is to see it as a black-box that accepts an input and gives an output. This is true of any algorithm, not just ML models. Taking this route, a neural network is a model that learns some function: $$ h: \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{k} $$ For a regression problem, \\(k = 1\\) , and the output is interpreted as some real number. Given a data-point \\(\\boldsymbol{x} \\in \\mathbb{R}^m\\) , we have: \\[ h(\\boldsymbol{x}) = y \\] In the case of a multi-class classification problem, the network outputs a vector of probabilities. $$ h(\\boldsymbol{x}) = \\boldsymbol{\\hat{y}} $$ If the feature matrix \\(\\boldsymbol{X}\\) of size \\(n \\times m\\) is passed as input, we get a matrix of probabilities, \\(\\boldsymbol{\\hat{Y}}\\) of size \\(n \\times k\\) . Since each row of the matrix corresponds to a probability distribution over the \\(k\\) classes, the following relation holds: \\[ \\sum \\limits_{j = 1}^{k} \\hat{Y}_{ij} = 1 \\quad 1 \\leq i \\leq n \\] This equation can also be vectorized as follows: $$ \\boldsymbol{\\hat{Y}} \\boldsymbol{1}_{k} = \\boldsymbol{1}_n $$ Here, \\(\\boldsymbol{1}_k\\) and \\(\\boldsymbol{1}_n\\) are vectors of ones of sizes \\(k\\) and \\(n\\) respectively. For example: \\[ \\boldsymbol{1}_k = \\begin{bmatrix} 1\\\\ \\vdots\\\\ 1 \\end{bmatrix} \\] In terms of NumPy , this is: one_k = np.ones(k) one_n = np.ones(n) This notation helps us express the row-wise sum of a matrix. This may seem like an arbitrary point now, but this notation will keep making an appearance at various places. The NumPy equivalent of the expression \\(\\boldsymbol{\\hat{Y}1}_k\\) is this: Y_hat.sum(axis = 1)","title":"Introduction"},{"location":"neural-networks/01-introduction/#introduction","text":"Until the early 2000s, neural networks were seen as one among several species in the zoo of ML algorithms. Due to a combination of several factors, the last decade witnessed intense research activity in this area. This resulted in neural networks becoming the model of choice in many domains of data science and AI. The impact was so huge that an entire field (deep learning) has blossomed out of it. One of the most important problems that contributed to the success of this model is image classification on large datasets, specifically the ImageNet dataset . In this course, we will study the most basic type of neural network called \"feedforward networks\". In a feedforward network, an input vector undergoes a sequence of transformations before it turns into the output. Each transformation is a linear combination of inputs followed by a non-linear activation function. Recall that this is similar to what happened in logistic regression. Instead of having just one unit, we have a network of units arranged in the form of layers: The term feedforward is used as the input is always fed forward. So, a feedforward network can also be viewed as a directed acyclic graph (DAG), with the direction of the edges always leading from the input to the output.","title":"Introduction"},{"location":"neural-networks/01-introduction/#black-box","text":"There are two ways to look at any ML model. The easiest and somewhat superficial way is to see it as a black-box that accepts an input and gives an output. This is true of any algorithm, not just ML models. Taking this route, a neural network is a model that learns some function: $$ h: \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{k} $$ For a regression problem, \\(k = 1\\) , and the output is interpreted as some real number. Given a data-point \\(\\boldsymbol{x} \\in \\mathbb{R}^m\\) , we have: \\[ h(\\boldsymbol{x}) = y \\] In the case of a multi-class classification problem, the network outputs a vector of probabilities. $$ h(\\boldsymbol{x}) = \\boldsymbol{\\hat{y}} $$ If the feature matrix \\(\\boldsymbol{X}\\) of size \\(n \\times m\\) is passed as input, we get a matrix of probabilities, \\(\\boldsymbol{\\hat{Y}}\\) of size \\(n \\times k\\) . Since each row of the matrix corresponds to a probability distribution over the \\(k\\) classes, the following relation holds: \\[ \\sum \\limits_{j = 1}^{k} \\hat{Y}_{ij} = 1 \\quad 1 \\leq i \\leq n \\] This equation can also be vectorized as follows: $$ \\boldsymbol{\\hat{Y}} \\boldsymbol{1}_{k} = \\boldsymbol{1}_n $$ Here, \\(\\boldsymbol{1}_k\\) and \\(\\boldsymbol{1}_n\\) are vectors of ones of sizes \\(k\\) and \\(n\\) respectively. For example: \\[ \\boldsymbol{1}_k = \\begin{bmatrix} 1\\\\ \\vdots\\\\ 1 \\end{bmatrix} \\] In terms of NumPy , this is: one_k = np.ones(k) one_n = np.ones(n) This notation helps us express the row-wise sum of a matrix. This may seem like an arbitrary point now, but this notation will keep making an appearance at various places. The NumPy equivalent of the expression \\(\\boldsymbol{\\hat{Y}1}_k\\) is this: Y_hat.sum(axis = 1)","title":"Black box"},{"location":"neural-networks/02-model/","text":"Model \u00b6 A neural network has three components: layers weights activation functions Each layer is a collection of nodes, also called neurons. Layers are stacked one after the other. Neurons in successive layers are connected to each other by means of an edge. Edges have weights to denote the importance of the connections. The number of layers and their sizes determine the shape of the weights. Activation functions are independent of the layers and weights. With these three components, we can define the model equations that compute the output given the input. In the case of neural networks, this is called a forward pass and will be discussed in the next section. Layers \u00b6 All networks have at least three layers: input hidden output The number of hidden layers is a hyperparameter. For the rest of the document, we will follow a convention to denote the layers. The model has \\(L + 1\\) layers. Layers are indexed using \\(l\\) . \\(l = 0\\) : input layer \\(1 \\leq l \\leq L - 1\\) : hidden layers \\(l = L\\) : output layer \\(S_l\\) : number of neurons in layer \\(l\\) \\(S_0 = m\\) , the number of features for the problem. \\(S_L\\) can take the following values: \\(1\\) , for a regression problem \\(k\\) , for a multi-class classification problem Weights and biases \u00b6 The weight matrix connecting layer \\(l - 1\\) and \\(l\\) is as follows: \\(\\boldsymbol{W_l}\\) : weight matrix size: \\(S_{l - 1} \\times S_{l}\\) \\(W_{lij}\\) : weight connecting \\(i^{th}\\) neuron in layer \\((l - 1)\\) to \\(j^{th}\\) neuron in layer \\(l\\) \\(\\boldsymbol{b_l}\\) : bias vector size: \\(S_l\\) \\(b_{lj}\\) : bias of the \\(j^{th}\\) neuron in layer \\(l\\) There are no weights before the input layer. Weights start from the first hidden layer, \\(l = 1\\) . The last weight matrix is the one connecting the final hidden layer and the output layer, \\(l = L\\) . Therefore, there are \\(L\\) learnable weights and biases, namely for \\(1 \\leq l \\leq L\\) . Activation functions \u00b6 The type of the activation function depends on the layer in which it is used: hidden layers output layer Hidden layers \u00b6 \\(g(z)\\) will be used to refer to activation function. \\(z\\) is some real number. Sigmoid \u00b6 \\[ g(z) = \\cfrac{1}{1 + e^{-z}} \\] Tanh \u00b6 \\[ g(z) = \\tanh(z) \\] ReLU \u00b6 The full form of ReLU is Rectified Linear Unit: \\[ g(z) = \\begin{cases}z,\\quad z \\geq 0\\\\0,\\quad \\text{otherwise}\\end{cases} \\] When the argument to \\(g\\) is a vector or a matrix, the activation function will be applied element-wise. Output layer \u00b6 The activation function at the output layer depends on the type of problem being solved: regression or classification, and could be one of the following: Linear: \\(g(z) = z\\) Here, \\(z\\) is a scalar activation at the output, that is, there is exactly one neuron in the output layer. This is used for regression problems. Softmax: \\(g(\\boldsymbol{z}) = \\begin{bmatrix} \\cdots & \\cfrac{e^{z_i}}{\\sum \\limits_{r = 1}^{k} e^{z_r}} & \\cdots\\end{bmatrix}^T\\) Here, \\(\\boldsymbol{z}\\) is a vector of activations at the output layer, that is, there are \\(k\\) neurons in the output layer. This is used in multiclass classification problems. If the argument is a matrix, then the softmax will be applied row-wise. Network Architecture \u00b6 A network architecture is a specification of the number of layers, the number of neurons in each layer, and the activation functions used at these layers. A sample is shown below: Layer index ( \\(l\\) ) Type Neurons Activation function \\(0\\) Input \\(20\\) NA \\(1\\) Hidden \\(30\\) ReLU \\(2\\) Output \\(10\\) Softmax This is an architecture for a multiclass classification network with \\(10\\) classes. There is exactly one hidden layer. The number of parameters of this network can be computed as follows: Layer index ( \\(l\\) ) Number of weights Number of biases Number of parameters \\(0\\) \\(0\\) \\(0\\) \\(0\\) \\(1\\) \\(20 \\times 30 = 600\\) \\(30\\) \\(630\\) \\(2\\) \\(30 \\times 10 = 300\\) \\(10\\) \\(310\\) Total \\(940\\) Notice how a simple neural network with just one hidden layer has nearly \\(1000\\) parameters! Neural networks with a large number of hidden layers are thus prone to overfitting.","title":"Model"},{"location":"neural-networks/02-model/#model","text":"A neural network has three components: layers weights activation functions Each layer is a collection of nodes, also called neurons. Layers are stacked one after the other. Neurons in successive layers are connected to each other by means of an edge. Edges have weights to denote the importance of the connections. The number of layers and their sizes determine the shape of the weights. Activation functions are independent of the layers and weights. With these three components, we can define the model equations that compute the output given the input. In the case of neural networks, this is called a forward pass and will be discussed in the next section.","title":"Model"},{"location":"neural-networks/02-model/#layers","text":"All networks have at least three layers: input hidden output The number of hidden layers is a hyperparameter. For the rest of the document, we will follow a convention to denote the layers. The model has \\(L + 1\\) layers. Layers are indexed using \\(l\\) . \\(l = 0\\) : input layer \\(1 \\leq l \\leq L - 1\\) : hidden layers \\(l = L\\) : output layer \\(S_l\\) : number of neurons in layer \\(l\\) \\(S_0 = m\\) , the number of features for the problem. \\(S_L\\) can take the following values: \\(1\\) , for a regression problem \\(k\\) , for a multi-class classification problem","title":"Layers"},{"location":"neural-networks/02-model/#weights-and-biases","text":"The weight matrix connecting layer \\(l - 1\\) and \\(l\\) is as follows: \\(\\boldsymbol{W_l}\\) : weight matrix size: \\(S_{l - 1} \\times S_{l}\\) \\(W_{lij}\\) : weight connecting \\(i^{th}\\) neuron in layer \\((l - 1)\\) to \\(j^{th}\\) neuron in layer \\(l\\) \\(\\boldsymbol{b_l}\\) : bias vector size: \\(S_l\\) \\(b_{lj}\\) : bias of the \\(j^{th}\\) neuron in layer \\(l\\) There are no weights before the input layer. Weights start from the first hidden layer, \\(l = 1\\) . The last weight matrix is the one connecting the final hidden layer and the output layer, \\(l = L\\) . Therefore, there are \\(L\\) learnable weights and biases, namely for \\(1 \\leq l \\leq L\\) .","title":"Weights and biases"},{"location":"neural-networks/02-model/#activation-functions","text":"The type of the activation function depends on the layer in which it is used: hidden layers output layer","title":"Activation functions"},{"location":"neural-networks/02-model/#hidden-layers","text":"\\(g(z)\\) will be used to refer to activation function. \\(z\\) is some real number.","title":"Hidden layers"},{"location":"neural-networks/02-model/#sigmoid","text":"\\[ g(z) = \\cfrac{1}{1 + e^{-z}} \\]","title":"Sigmoid"},{"location":"neural-networks/02-model/#tanh","text":"\\[ g(z) = \\tanh(z) \\]","title":"Tanh"},{"location":"neural-networks/02-model/#relu","text":"The full form of ReLU is Rectified Linear Unit: \\[ g(z) = \\begin{cases}z,\\quad z \\geq 0\\\\0,\\quad \\text{otherwise}\\end{cases} \\] When the argument to \\(g\\) is a vector or a matrix, the activation function will be applied element-wise.","title":"ReLU"},{"location":"neural-networks/02-model/#output-layer","text":"The activation function at the output layer depends on the type of problem being solved: regression or classification, and could be one of the following: Linear: \\(g(z) = z\\) Here, \\(z\\) is a scalar activation at the output, that is, there is exactly one neuron in the output layer. This is used for regression problems. Softmax: \\(g(\\boldsymbol{z}) = \\begin{bmatrix} \\cdots & \\cfrac{e^{z_i}}{\\sum \\limits_{r = 1}^{k} e^{z_r}} & \\cdots\\end{bmatrix}^T\\) Here, \\(\\boldsymbol{z}\\) is a vector of activations at the output layer, that is, there are \\(k\\) neurons in the output layer. This is used in multiclass classification problems. If the argument is a matrix, then the softmax will be applied row-wise.","title":"Output layer"},{"location":"neural-networks/02-model/#network-architecture","text":"A network architecture is a specification of the number of layers, the number of neurons in each layer, and the activation functions used at these layers. A sample is shown below: Layer index ( \\(l\\) ) Type Neurons Activation function \\(0\\) Input \\(20\\) NA \\(1\\) Hidden \\(30\\) ReLU \\(2\\) Output \\(10\\) Softmax This is an architecture for a multiclass classification network with \\(10\\) classes. There is exactly one hidden layer. The number of parameters of this network can be computed as follows: Layer index ( \\(l\\) ) Number of weights Number of biases Number of parameters \\(0\\) \\(0\\) \\(0\\) \\(0\\) \\(1\\) \\(20 \\times 30 = 600\\) \\(30\\) \\(630\\) \\(2\\) \\(30 \\times 10 = 300\\) \\(10\\) \\(310\\) Total \\(940\\) Notice how a simple neural network with just one hidden layer has nearly \\(1000\\) parameters! Neural networks with a large number of hidden layers are thus prone to overfitting.","title":"Network Architecture"},{"location":"neural-networks/03-forward/","text":"Forward pass \u00b6 As stated earlier, the network can be seen as a function \\(h: \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{k}\\) . In the case of a multi-class classification problem, given a data-point \\(\\boldsymbol{x} \\in \\mathbb{R}^m\\) , the network outputs a vector of probabilities. $$ h(\\boldsymbol{x}) = \\boldsymbol{\\hat{y}} $$ If the feature matrix \\(\\boldsymbol{X}\\) of size \\(n \\times m\\) is passed as input, we get a matrix of probabilities, \\(\\boldsymbol{\\hat{Y}}\\) of size \\(n \\times k\\) . For a regression problem, \\(k = 1\\) and the output is interpreted as some real number. This input-output relationship can be computed in an iterative manner. This is termed as a forward pass. Activations \u00b6 First we look at what happens at one layer of the network. At any layer, there are three steps that have to be performed in this sequence: Accept input Linearly combine the inputs Apply the activation function What comes out of a layer are called the activations at that layer. The linear combination of the inputs to a layer are called the pre-activations. \\(\\boldsymbol{Z_l}\\) and \\(\\boldsymbol{A_l}\\) represent the matrices of pre-activations and activations respectively at layer \\(l\\) for \\(0 \\leq l \\leq L\\) . \\(\\boldsymbol{A_0} = \\boldsymbol{X}\\) , the feature matrix. The activation matrix \\(\\boldsymbol{A_l}\\) is of size \\(n \\times S_l\\) . Each row of the activation matrix corresponds to the activation vector for one of the \\(n\\) data-points. Algorithm \u00b6 If \\(\\boldsymbol{A_{l - 1}}\\) represents the activation matrix at layer \\(l - 1\\) , then the activations at layer \\(l\\) can be computed iteratively using the following pair of equations: \\[ \\begin{aligned} \\boldsymbol{Z_{l}} &= \\boldsymbol{A_{l - 1} W_l} + \\boldsymbol{b_l},\\quad 1 \\leq l \\leq L\\\\ \\\\ \\boldsymbol{A_{l}} &= g(\\boldsymbol{Z_{l}}),\\quad 1 \\leq l \\leq L \\end{aligned} \\] Here, \\(\\boldsymbol{A_0} = \\boldsymbol{X}\\) . The shapes of these matrices/vectors are as follows: \\(\\boldsymbol{A_{l - 1}}\\) : \\(n \\times S_{l - 1}\\) \\(\\boldsymbol{W_l}\\) : \\(S_{l - 1} \\times S_{l}\\) \\(\\boldsymbol{b_l}\\) : \\(S_l\\) \\(\\boldsymbol{Z_l}\\) : \\(n \\times S_l\\) \\(\\boldsymbol{A_{l}}\\) : \\(n \\times S_{l}\\) Note that \\(\\boldsymbol{b_{l}}\\) gets added to each row of the product \\(\\boldsymbol{A_{l - 1} W_{l}}\\) according to NumPy broadcasting rules. \\(g\\) is the hidden-layer activation function for \\(1 \\leq l \\leq L - 1\\) and the output-layer activation function for \\(l = L\\) . The final shape of the output activations at layer \\(L\\) is: \\(n\\) for regression and binary classification problems \\(n \\times k\\) for a multi-class classification problem According to our notation, \\(\\boldsymbol{A_L} = \\boldsymbol{\\hat{Y}}\\) . The algorithm can now be specfieid as given below:","title":"Forward pass"},{"location":"neural-networks/03-forward/#forward-pass","text":"As stated earlier, the network can be seen as a function \\(h: \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{k}\\) . In the case of a multi-class classification problem, given a data-point \\(\\boldsymbol{x} \\in \\mathbb{R}^m\\) , the network outputs a vector of probabilities. $$ h(\\boldsymbol{x}) = \\boldsymbol{\\hat{y}} $$ If the feature matrix \\(\\boldsymbol{X}\\) of size \\(n \\times m\\) is passed as input, we get a matrix of probabilities, \\(\\boldsymbol{\\hat{Y}}\\) of size \\(n \\times k\\) . For a regression problem, \\(k = 1\\) and the output is interpreted as some real number. This input-output relationship can be computed in an iterative manner. This is termed as a forward pass.","title":"Forward pass"},{"location":"neural-networks/03-forward/#activations","text":"First we look at what happens at one layer of the network. At any layer, there are three steps that have to be performed in this sequence: Accept input Linearly combine the inputs Apply the activation function What comes out of a layer are called the activations at that layer. The linear combination of the inputs to a layer are called the pre-activations. \\(\\boldsymbol{Z_l}\\) and \\(\\boldsymbol{A_l}\\) represent the matrices of pre-activations and activations respectively at layer \\(l\\) for \\(0 \\leq l \\leq L\\) . \\(\\boldsymbol{A_0} = \\boldsymbol{X}\\) , the feature matrix. The activation matrix \\(\\boldsymbol{A_l}\\) is of size \\(n \\times S_l\\) . Each row of the activation matrix corresponds to the activation vector for one of the \\(n\\) data-points.","title":"Activations"},{"location":"neural-networks/03-forward/#algorithm","text":"If \\(\\boldsymbol{A_{l - 1}}\\) represents the activation matrix at layer \\(l - 1\\) , then the activations at layer \\(l\\) can be computed iteratively using the following pair of equations: \\[ \\begin{aligned} \\boldsymbol{Z_{l}} &= \\boldsymbol{A_{l - 1} W_l} + \\boldsymbol{b_l},\\quad 1 \\leq l \\leq L\\\\ \\\\ \\boldsymbol{A_{l}} &= g(\\boldsymbol{Z_{l}}),\\quad 1 \\leq l \\leq L \\end{aligned} \\] Here, \\(\\boldsymbol{A_0} = \\boldsymbol{X}\\) . The shapes of these matrices/vectors are as follows: \\(\\boldsymbol{A_{l - 1}}\\) : \\(n \\times S_{l - 1}\\) \\(\\boldsymbol{W_l}\\) : \\(S_{l - 1} \\times S_{l}\\) \\(\\boldsymbol{b_l}\\) : \\(S_l\\) \\(\\boldsymbol{Z_l}\\) : \\(n \\times S_l\\) \\(\\boldsymbol{A_{l}}\\) : \\(n \\times S_{l}\\) Note that \\(\\boldsymbol{b_{l}}\\) gets added to each row of the product \\(\\boldsymbol{A_{l - 1} W_{l}}\\) according to NumPy broadcasting rules. \\(g\\) is the hidden-layer activation function for \\(1 \\leq l \\leq L - 1\\) and the output-layer activation function for \\(l = L\\) . The final shape of the output activations at layer \\(L\\) is: \\(n\\) for regression and binary classification problems \\(n \\times k\\) for a multi-class classification problem According to our notation, \\(\\boldsymbol{A_L} = \\boldsymbol{\\hat{Y}}\\) . The algorithm can now be specfieid as given below:","title":"Algorithm"},{"location":"neural-networks/04-loss/","text":"Loss \u00b6 The loss depends on the nature of the problem being solved. Regression \u00b6 \\(\\boldsymbol{y}\\) is a vector of target label for \\(n\\) examples. \\(\\boldsymbol{\\hat{y}}\\) is the output of the network and corresponds to the predicted labels. Both vectors are of size \\(n\\) . The loss is our usual squared error: $$ L(\\boldsymbol{y}, \\boldsymbol{\\hat{y}}) = \\cfrac{1}{2} \\cdot (\\boldsymbol{\\hat{y}} - \\boldsymbol{y})^T (\\boldsymbol{\\hat{y}} - \\boldsymbol{y}) $$ Multi-class classification \u00b6 \\(\\boldsymbol{Y}\\) is a matrix of target labels for \\(n\\) examples. Each row of this matrix is a one-hot vector. \\(\\boldsymbol{\\hat{Y}}\\) is a matrix of predicted probabilities. Both matrices are of size \\(n \\times k\\) . The categorical cross-entropy loss is given as follows: $$ L(\\boldsymbol{Y}, \\boldsymbol{\\hat{Y}})=-\\boldsymbol{1}^T_{n} \\left( \\boldsymbol{Y} \\odot \\log \\boldsymbol{\\hat{Y}} \\right) \\boldsymbol{1}_{k} $$ This equation can be understood as follows: \\(\\boldsymbol{1}_n\\) and \\(\\boldsymbol{1}_k\\) are vectors of ones of sizes \\(n\\) and \\(k\\) respectively. If \\(\\boldsymbol{M}\\) is a matrix of size \\(n \\times k\\) , then \\(\\boldsymbol{1}^T_{n} \\boldsymbol{M} \\boldsymbol{1}_k\\) is the sum of all elements in the matrix. \\(\\odot\\) is the element-wise product. The NumPy equivalent of the loss is: L = -np.sum(Y * np.log(Y_hat)) Note that the loss function is always a scalar. To understand why the cross-entropy takes this form, refer to the appendix.","title":"Loss"},{"location":"neural-networks/04-loss/#loss","text":"The loss depends on the nature of the problem being solved.","title":"Loss"},{"location":"neural-networks/04-loss/#regression","text":"\\(\\boldsymbol{y}\\) is a vector of target label for \\(n\\) examples. \\(\\boldsymbol{\\hat{y}}\\) is the output of the network and corresponds to the predicted labels. Both vectors are of size \\(n\\) . The loss is our usual squared error: $$ L(\\boldsymbol{y}, \\boldsymbol{\\hat{y}}) = \\cfrac{1}{2} \\cdot (\\boldsymbol{\\hat{y}} - \\boldsymbol{y})^T (\\boldsymbol{\\hat{y}} - \\boldsymbol{y}) $$","title":"Regression"},{"location":"neural-networks/04-loss/#multi-class-classification","text":"\\(\\boldsymbol{Y}\\) is a matrix of target labels for \\(n\\) examples. Each row of this matrix is a one-hot vector. \\(\\boldsymbol{\\hat{Y}}\\) is a matrix of predicted probabilities. Both matrices are of size \\(n \\times k\\) . The categorical cross-entropy loss is given as follows: $$ L(\\boldsymbol{Y}, \\boldsymbol{\\hat{Y}})=-\\boldsymbol{1}^T_{n} \\left( \\boldsymbol{Y} \\odot \\log \\boldsymbol{\\hat{Y}} \\right) \\boldsymbol{1}_{k} $$ This equation can be understood as follows: \\(\\boldsymbol{1}_n\\) and \\(\\boldsymbol{1}_k\\) are vectors of ones of sizes \\(n\\) and \\(k\\) respectively. If \\(\\boldsymbol{M}\\) is a matrix of size \\(n \\times k\\) , then \\(\\boldsymbol{1}^T_{n} \\boldsymbol{M} \\boldsymbol{1}_k\\) is the sum of all elements in the matrix. \\(\\odot\\) is the element-wise product. The NumPy equivalent of the loss is: L = -np.sum(Y * np.log(Y_hat)) Note that the loss function is always a scalar. To understand why the cross-entropy takes this form, refer to the appendix.","title":"Multi-class classification"},{"location":"neural-networks/05-backward/","text":"Backward pass \u00b6 As we have been doing all along, gradient descent will be our optimizer. We need to compute the gradients of the loss with respect to the weights and the biases. Imagine doing this for a network with thousands of parameters. We would have to find the derivative of the loss with respect to \\(1000\\) different variables. This seems like a computational nightmare. Thankfully, researchers have developed an efficient algorithm called backpropagation that does the job for us. At its heart, backpropagation uses the chain rule of differentiation to compute the gradients. The idea is to first begin with the gradients at the final layer and keep propagating them all the way back to the first layer. This sequence of operations is termed a backward pass, as we start from the final layer and let the gradients flow all the way back to the first layer. Just as we had a \"forward pass\" to compute the output given the input, we have a \"backward pass\" to compute the gradients of the loss with respect to the weights. The backward pass can be divided into two parts: Hidden Layers \u00b6 The weights at layer \\(l\\) , \\(\\boldsymbol{W_{l}}\\) , influence the loss via the matrix of activations, \\(\\boldsymbol{A_{l}}\\) , at layer \\(l\\) . The equation that connects these two quantities is given below and should be familiar to you by now: \\[ \\begin{aligned} \\boldsymbol{Z_{l}} &= \\boldsymbol{A_{l - 1} W_l} + \\boldsymbol{b_l},\\quad 1 \\leq l \\leq L\\\\ \\\\ \\boldsymbol{A_{l}} &= g(\\boldsymbol{Z_{l}}),\\quad 1 \\leq l \\leq L \\end{aligned} \\] Let us assume that we already have access to the gradient of the loss with respect to the activations at layer \\(l\\) . Let us call it \\(\\boldsymbol{A_{l}^{(g)}}\\) . This is a matrix of the same shape as \\(\\boldsymbol{A_{l}}\\) . If we need the gradient of the loss with respect to the weights at layer \\(l\\) , then by the chain rule of differentiation, we need to compute the gradients with respect to the pre-activations at layer \\(l\\) . We shall use the following notation for the gradients at layer \\(l\\) : \\(\\boldsymbol{W_{l}^{(g)}}\\) : gradient of the loss with respect to the weights \\(\\boldsymbol{Z_{l}^{(g)}}\\) : gradient of the loss with respect to the pre-activations \\(\\boldsymbol{A_{l}^{(g)}}\\) : gradient of the loss with respect to the activations The rule is quite simple to state. It is just a product, the first one is element-wise product of two matrices, the second one is our usual matrix multiplication: \\[ \\begin{aligned} \\boldsymbol{Z_{l}^{(g)}} &= \\boldsymbol{A_{l}^{(g)}} \\odot g^{\\prime}(\\boldsymbol{Z_{l}})\\\\ \\\\ \\boldsymbol{W_l^{(g)}} &= \\boldsymbol{A_{l - 1}}^T \\boldsymbol{Z_{l}^{(g)}} \\end{aligned} \\] To see why the first of these two equations makes sense, recall that the activation function is applied element-wise on the pre-activations in the forward pass. This process is reversed for the backward pass: the derivative of the activation function is multiplied element-wise with the gradients of the activations. The second equation might seem more foreboding. A reasonable intuition is to forget that these quantities are matrices and to instead think of them as scalars. Consider the following simplification: \\(z = aw + b\\) $$ w^{(g)} = \\cfrac{\\partial L}{\\partial w} = \\cfrac{\\partial L}{\\partial z} \\cfrac{\\partial z}{\\partial w} = z^{(g)} a $$ With matrices, this simple scalar product becomes a matrix product. One way to remember the exact form is to make sure that the dimensions of all the matrices are compatible for matrix multiplication. To propagate this process to earlier layers, we also need \\(\\boldsymbol{A_{l - 1}^{(g)}}\\) . The expression for that is quite similar: \\[ \\boldsymbol{A_{l - 1}^{(g)}} = \\boldsymbol{Z_{l}^{(g)}} \\boldsymbol{W_{l}}^T \\] Now, you can see why the algorithm is termed backpropagation. We start with the gradients at a layer and keep \"propagating\" them back until we hit the input layer. We have ignored the gradients of the loss with respect to the biases. That is left as an exercise to the reader. Output layer \u00b6 The only thing that remains is to compute the gradient of the loss with respect to the activations in the last layer. This is in fact the first step of the back-propagation algorithm. The gradients depend on the form of the loss, which in turn depends on the type of problem being solved: Regression \u00b6 Recall that \\(\\boldsymbol{A_L} = \\boldsymbol{\\hat{y}}\\) . The activations at the last layer are the predicted labels. \\[ \\boldsymbol{A_L^{(g)}} = \\boldsymbol{\\hat{y}} - \\boldsymbol{y} \\] As the output-activation function is linear, computing the gradient of the activations and weights for the penultimate layer in the network is straightforward. In fact, \\(\\boldsymbol{Z_{L}^{(g)}} = \\boldsymbol{A_L^{(g)}}\\) . Multiclass classification \u00b6 Recall that \\(\\boldsymbol{A_L} = \\boldsymbol{\\hat{Y}}\\) . The activations at the last layer are the predicted probabilities. \\[ \\boldsymbol{A_L^{(g)}} = - \\boldsymbol{Y} \\odot \\boldsymbol{\\hat{Y}}^{\\odot -1} \\] This notation might be new. \\(P^{\\odot -1}\\) is element-wise inverse. \\(P^{\\odot -1}_{ij} = \\frac{1}{P_{ij}}\\) . This inverse arises from differentiating the the \\(\\log\\) term in the loss function. As the output-activation function is softmax, computing the gradients of the pre-activations is less straightforward. Refer to appendix for a detailed derivation of the same. The expression for the gradients turns out to be very simple in the end: $$ \\boldsymbol{Z_L^{(g)}} = \\boldsymbol{\\hat{Y}} - \\boldsymbol{Y} $$ Note how similar the expressions are for regression and classification. We will take advantage of this fact during our implementation of neural networks. Algorithm \u00b6 We can now put together all these equations together and specify the algorithm for backward pass:","title":"Backward pass"},{"location":"neural-networks/05-backward/#backward-pass","text":"As we have been doing all along, gradient descent will be our optimizer. We need to compute the gradients of the loss with respect to the weights and the biases. Imagine doing this for a network with thousands of parameters. We would have to find the derivative of the loss with respect to \\(1000\\) different variables. This seems like a computational nightmare. Thankfully, researchers have developed an efficient algorithm called backpropagation that does the job for us. At its heart, backpropagation uses the chain rule of differentiation to compute the gradients. The idea is to first begin with the gradients at the final layer and keep propagating them all the way back to the first layer. This sequence of operations is termed a backward pass, as we start from the final layer and let the gradients flow all the way back to the first layer. Just as we had a \"forward pass\" to compute the output given the input, we have a \"backward pass\" to compute the gradients of the loss with respect to the weights. The backward pass can be divided into two parts:","title":"Backward pass"},{"location":"neural-networks/05-backward/#hidden-layers","text":"The weights at layer \\(l\\) , \\(\\boldsymbol{W_{l}}\\) , influence the loss via the matrix of activations, \\(\\boldsymbol{A_{l}}\\) , at layer \\(l\\) . The equation that connects these two quantities is given below and should be familiar to you by now: \\[ \\begin{aligned} \\boldsymbol{Z_{l}} &= \\boldsymbol{A_{l - 1} W_l} + \\boldsymbol{b_l},\\quad 1 \\leq l \\leq L\\\\ \\\\ \\boldsymbol{A_{l}} &= g(\\boldsymbol{Z_{l}}),\\quad 1 \\leq l \\leq L \\end{aligned} \\] Let us assume that we already have access to the gradient of the loss with respect to the activations at layer \\(l\\) . Let us call it \\(\\boldsymbol{A_{l}^{(g)}}\\) . This is a matrix of the same shape as \\(\\boldsymbol{A_{l}}\\) . If we need the gradient of the loss with respect to the weights at layer \\(l\\) , then by the chain rule of differentiation, we need to compute the gradients with respect to the pre-activations at layer \\(l\\) . We shall use the following notation for the gradients at layer \\(l\\) : \\(\\boldsymbol{W_{l}^{(g)}}\\) : gradient of the loss with respect to the weights \\(\\boldsymbol{Z_{l}^{(g)}}\\) : gradient of the loss with respect to the pre-activations \\(\\boldsymbol{A_{l}^{(g)}}\\) : gradient of the loss with respect to the activations The rule is quite simple to state. It is just a product, the first one is element-wise product of two matrices, the second one is our usual matrix multiplication: \\[ \\begin{aligned} \\boldsymbol{Z_{l}^{(g)}} &= \\boldsymbol{A_{l}^{(g)}} \\odot g^{\\prime}(\\boldsymbol{Z_{l}})\\\\ \\\\ \\boldsymbol{W_l^{(g)}} &= \\boldsymbol{A_{l - 1}}^T \\boldsymbol{Z_{l}^{(g)}} \\end{aligned} \\] To see why the first of these two equations makes sense, recall that the activation function is applied element-wise on the pre-activations in the forward pass. This process is reversed for the backward pass: the derivative of the activation function is multiplied element-wise with the gradients of the activations. The second equation might seem more foreboding. A reasonable intuition is to forget that these quantities are matrices and to instead think of them as scalars. Consider the following simplification: \\(z = aw + b\\) $$ w^{(g)} = \\cfrac{\\partial L}{\\partial w} = \\cfrac{\\partial L}{\\partial z} \\cfrac{\\partial z}{\\partial w} = z^{(g)} a $$ With matrices, this simple scalar product becomes a matrix product. One way to remember the exact form is to make sure that the dimensions of all the matrices are compatible for matrix multiplication. To propagate this process to earlier layers, we also need \\(\\boldsymbol{A_{l - 1}^{(g)}}\\) . The expression for that is quite similar: \\[ \\boldsymbol{A_{l - 1}^{(g)}} = \\boldsymbol{Z_{l}^{(g)}} \\boldsymbol{W_{l}}^T \\] Now, you can see why the algorithm is termed backpropagation. We start with the gradients at a layer and keep \"propagating\" them back until we hit the input layer. We have ignored the gradients of the loss with respect to the biases. That is left as an exercise to the reader.","title":"Hidden Layers"},{"location":"neural-networks/05-backward/#output-layer","text":"The only thing that remains is to compute the gradient of the loss with respect to the activations in the last layer. This is in fact the first step of the back-propagation algorithm. The gradients depend on the form of the loss, which in turn depends on the type of problem being solved:","title":"Output layer"},{"location":"neural-networks/05-backward/#regression","text":"Recall that \\(\\boldsymbol{A_L} = \\boldsymbol{\\hat{y}}\\) . The activations at the last layer are the predicted labels. \\[ \\boldsymbol{A_L^{(g)}} = \\boldsymbol{\\hat{y}} - \\boldsymbol{y} \\] As the output-activation function is linear, computing the gradient of the activations and weights for the penultimate layer in the network is straightforward. In fact, \\(\\boldsymbol{Z_{L}^{(g)}} = \\boldsymbol{A_L^{(g)}}\\) .","title":"Regression"},{"location":"neural-networks/05-backward/#multiclass-classification","text":"Recall that \\(\\boldsymbol{A_L} = \\boldsymbol{\\hat{Y}}\\) . The activations at the last layer are the predicted probabilities. \\[ \\boldsymbol{A_L^{(g)}} = - \\boldsymbol{Y} \\odot \\boldsymbol{\\hat{Y}}^{\\odot -1} \\] This notation might be new. \\(P^{\\odot -1}\\) is element-wise inverse. \\(P^{\\odot -1}_{ij} = \\frac{1}{P_{ij}}\\) . This inverse arises from differentiating the the \\(\\log\\) term in the loss function. As the output-activation function is softmax, computing the gradients of the pre-activations is less straightforward. Refer to appendix for a detailed derivation of the same. The expression for the gradients turns out to be very simple in the end: $$ \\boldsymbol{Z_L^{(g)}} = \\boldsymbol{\\hat{Y}} - \\boldsymbol{Y} $$ Note how similar the expressions are for regression and classification. We will take advantage of this fact during our implementation of neural networks.","title":"Multiclass classification"},{"location":"neural-networks/05-backward/#algorithm","text":"We can now put together all these equations together and specify the algorithm for backward pass:","title":"Algorithm"},{"location":"neural-networks/06-learning-algo/","text":"Learning Algorithm \u00b6 We now have all the ingredients to specify the learning algorithm. Let \\(\\boldsymbol{\\theta}\\) refer to all the parameters in the model. We can now define the following functions: \\(\\boldsymbol{\\hat{Y}} = \\text{forward-pass}(\\boldsymbol{X})\\) \\(L = \\text{loss}(\\boldsymbol{Y}, \\boldsymbol{\\hat{Y}})\\) \\(\\boldsymbol{\\theta^{(g)}} = \\text{backward-pass}(\\boldsymbol{Y}, \\boldsymbol{\\hat{Y}})\\) Only the most important arguments are displayed here. With this, we can define the learning algorithm for neural networks:","title":"Learning Algorithm"},{"location":"neural-networks/06-learning-algo/#learning-algorithm","text":"We now have all the ingredients to specify the learning algorithm. Let \\(\\boldsymbol{\\theta}\\) refer to all the parameters in the model. We can now define the following functions: \\(\\boldsymbol{\\hat{Y}} = \\text{forward-pass}(\\boldsymbol{X})\\) \\(L = \\text{loss}(\\boldsymbol{Y}, \\boldsymbol{\\hat{Y}})\\) \\(\\boldsymbol{\\theta^{(g)}} = \\text{backward-pass}(\\boldsymbol{Y}, \\boldsymbol{\\hat{Y}})\\) Only the most important arguments are displayed here. With this, we can define the learning algorithm for neural networks:","title":"Learning Algorithm"},{"location":"neural-networks/07-appendix/","text":"Appendix \u00b6 Categorical Cross Entropy \u00b6 Part-1 \u00b6 In the case of multi-class classification, the output layer after softmax can be viewed as a conditional probability distribution over the labels given the data-point. This is a categorical distribution over the \\(k\\) classes. Let us call it \\(q(y\\ |\\ x)\\) . This is the predicted distribution. There is a true distribution over the labels. Let us call it \\(p(y\\ |\\ x)\\) . Note that both are conditional distributions of the label given the data-point. The (conditional) cross-entropy of these two conditional distributions is given as follows: $$ H \\Big( p(y\\ |\\ x), q(y\\ |\\ x) \\Big) = -\\sum \\limits_{x} p(x) \\sum \\limits_{y} p(y\\ |\\ x) \\log q(y\\ |\\ x) $$ Note that this is really the expectation of some function \\(f(x)\\) over the marginal distribution \\(p(x)\\) : \\[ \\begin{aligned} E[f(x)] &= \\sum \\limits_{x} p(x) f(x)\\\\ &= E \\left[- \\sum \\limits_{y} p(y\\ |\\ x) \\log q(y\\ |\\ x) \\right] \\end{aligned} \\] To compute an expectation, we need to sum over all samples drawn from this distribution. In practice, this is not going to be possible. This expectation is typically approximated by sampling data-points from the marginal distribution. We can replace the exhaustive summation with a sum over the \\(n\\) data-points in our batch of examples: $$ H \\Big( p(y\\ |\\ x), q(y\\ |\\ x) \\Big) = -\\cfrac{1}{n} \\sum \\limits_{i = 1}^{n} \\sum \\limits_{y} p(y\\ |\\ x^{(i)}) \\log q(y\\ |\\ x^{(i)}) $$ A crude way of seeing this is as follows: if a particular data-point \\(x^{*}\\) is sampled \\(r\\) times, (appears \\(r\\) times in the batch) then: $$ p(x^{*}) \\approx \\cfrac{r}{n} $$ Let us get rid of the factor of \\(\\frac{1}{n}\\) as it doesn't matter from the point of view of optimization. Part-2 \u00b6 We are left with the following expression for the cross-entropy: $$ H \\Big( p(y\\ |\\ x), q(y\\ |\\ x) \\Big) = -\\sum \\limits_{i = 1}^{n} \\sum \\limits_{y} p(y\\ |\\ x^{(i)}) \\log q(y\\ |\\ x^{(i)}) $$ We now need to understand the inner sum over the labels. Some notation needs to be be setup to make things more tractable: \\[ \\begin{aligned} y^{(i)}&: \\text{true label for data-point i, scalar}\\\\ \\boldsymbol{y^{(i)}}&: \\text{true label for data-point i, one-hot vector}\\\\ \\boldsymbol{\\hat{y}^{(i)}}&: \\text{predicted probabilities for data-point i, vector} \\end{aligned} \\] The symbols in bold are are vectors of length \\(k\\) , where \\(k\\) is the number of classes. With this notation we can get a better idea about the true and predicted distributions, \\(p\\) and \\(q\\) , respectively: \\[ \\begin{aligned} p(y = c\\ |\\ x^{(i)}) &= \\boldsymbol{y^{(i)}_{c}}\\\\ q(y = c\\ |\\ x^{(i)}) &= \\boldsymbol{\\hat{y}^{(i)}_{c}} \\end{aligned} \\] Note that \\(\\boldsymbol{\\hat{y}^{(i)}_{c}}\\) is the \\(c^{th}\\) component of a vector and hence a scalar. Same is the case for \\(\\boldsymbol{y^{(i)}_{c}}\\) We are now ready to compute the categorical cross-entropy: \\[ \\begin{aligned} H \\Big( p(y\\ |\\ x), q(y\\ |\\ x) \\Big) &= -\\sum \\limits_{i = 1}^{n} \\sum \\limits_{y} p(y\\ |\\ x^{(i)}) \\log q(y\\ |\\ x^{(i)})\\\\ &= - \\sum \\limits_{i = 1}^{n} \\sum \\limits_{c = 1}^{k} p(y = c\\ |\\ x^{(i)}) \\log q(y = c\\ |\\ x^{(i)})\\\\ &= - \\sum \\limits_{i = 1}^{n} \\sum \\limits_{c = 1}^{k} \\boldsymbol{y^{(i)}_{c}}\\ \\cdot \\log(\\boldsymbol{\\hat{y}^{(i)}_{c}}) \\end{aligned} \\] This is the final expression for the categorical cross-entropy (CCE) loss. Even though the inner sum is over \\(k\\) classes, only one of the values for \\(\\boldsymbol{y^{(i)}_{c}}\\) is non-zero. In the end, the sum reduces to a very simple scalar expression in principle: \\[ \\text{CCE} = \\sum \\limits_{i = 1}^{n} -\\log \\boldsymbol{\\hat{y}^{(i)}_{y^{(i)}}} \\] I know that this looks hopelessly complicated, thanks to the subscripts and superscripts. But what it is saying is this: The categorical cross-entropy for a batch of data-points can be computed as follows: For each data-point, get the predicted probability corresponding to the true label. Take negative log of that probability. Sum this quantity across the entire batch. Part-3 \u00b6 Intuitively, this loss makes a lot of sense. In fact, we can see why this should qualify as a loss in the first place by asking these questions: When is this quantity minimized? When the predicted probability for the correct class (true label) is \\(1\\) for every data-point in the batch. What happens if the predicted probability for the correct class for some data-point is close to \\(0\\) ? In such a case, \\(-\\log \\boldsymbol{\\hat{y}^{(i)}}_{y^{(i)}}\\) will be a huge positive value, something that is undesirable. What does minimizing cross-entropy really mean? The attempt to minimize the categorical cross-entropy translates to pushing the probability of the correct-class closer and closer to \\(1\\) for each data-point. Our hope is that in this process, the model will learn something useful that generalizes to unseen data-points as well. Part-4 \u00b6 We can see how this reduces to the binary cross-entropy loss in the case of two classes: \\[ \\begin{aligned} BCE &= - \\sum \\limits_{i = 1}^{n} \\sum \\limits_{c = 1}^{2} \\boldsymbol{y^{(i)}_{c}}\\ \\cdot \\log(\\boldsymbol{\\hat{y}^{(i)}_{c}})\\\\ &= - \\sum \\limits_{i = 1}^{n} \\left( y^{(i)} \\log (\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log (1 - \\hat{y}^{(i)}) \\right) \\end{aligned} \\] Chain rule for matrix product \u00b6 Let \\(P = QR\\) , where \\(P, Q\\) and \\(R\\) are matrices of compatible dimensions. Let \\(\\phi\\) be some function of \\(P\\) . Let us assume that we have access to the gradient of \\(\\phi\\) with respect to \\(P\\) . Call this \\(P^{(g)}\\) . Then, we have the following equations: \\[ Q^{(g)} = P^{(g)} R^T\\\\ R^{(g)} = Q^T P^{(g)} \\] This is nothing but the chain rule of differentiation in the presence of a matrix product. Gradient for softmax layer \u00b6 In order to derive the gradient matrix at the final layer, it is easier to consider the simpler case of the gradient vector. Let \\(\\boldsymbol{z}\\) be the pre-activation vector at layer \\(L\\) of the network. After applying softmax function \\(g\\) on the pre-activation vector, we get \\(\\boldsymbol{a}\\) , the activation vector. The two vectors are related by the following equations: \\[ \\boldsymbol{a} = g(\\boldsymbol{z}) = \\begin{bmatrix} \\cdots & \\cfrac{e^{z_j}}{\\sum \\limits_{r = 1}^{k} e^{z_r}} & \\cdots \\end{bmatrix} \\] The gradient of the loss with respect to the activation vector is given by: \\[ \\boldsymbol{a^{(g)}} = \\nabla_{\\boldsymbol{a}} L \\] The tricky part is the gradient of the loss with respect to the pre-activations. Let us consider a single element in the pre-activation vector, \\(z_j\\) . We are interested in the partial derivative \\(\\cfrac{\\partial L}{\\partial z_j}\\) . What this means is this: how much does the loss change when a small change is made to \\(z_j\\) . It is not exactly this, but the ratio of these two quantities, but you get the idea. \\(z_j\\) isn't directly connected to the loss function. The elements of the activation vector \\(a_i\\) s intervene. In other words, the impact of \\(z_j\\) on the loss is mediated by the \\(a_i\\) s. Changing \\(z_j\\) has an impact on each of the \\(a_i\\) s because of the way softmax is defined. We already know how the loss is affected when the \\(a_j\\) s are disturbed. This is captured by \\(\\boldsymbol{a^{(g)}}\\) . So much for the chain rule. We are now ready to state it: \\[ \\cfrac{\\partial L}{\\partial z_j} = \\sum \\limits_{i = 1}^{k} \\cfrac{\\partial L}{\\partial a_i} \\cfrac{\\partial a_i}{\\partial z_j} \\] The second term in the product is the \\(ij^{th}\\) element of the Jacobian matrix \\(J_{\\boldsymbol{z}}(\\boldsymbol{a})\\) : \\[ J_{\\boldsymbol{z}}(\\boldsymbol{a}) \\Bigg|_{ij} = \\left [ \\cfrac{\\partial a_i}{\\partial z_j} \\right] \\] The Jacobian matrix is a matrix of partial derivatives of the components of the vector \\(\\boldsymbol{a}\\) with respect to \\(\\boldsymbol{z}\\) . With the introduction of the Jacobian matrix, we can now state the gradient of the loss with respect to the pre-activations in the following succinct manner: \\[ \\boldsymbol{z^{(g)}} = \\nabla_{\\boldsymbol{z}} L = J_{\\boldsymbol{z}}(\\boldsymbol{a})^T\\boldsymbol{a^{(g)}} \\] We can now turn our attention to computing the Jacobian: \\[ J_{\\boldsymbol{z}}(\\boldsymbol{a}) \\Bigg|_{ij} = \\begin{cases} -a_i a_j &\\quad i \\neq j\\\\ a_j - a_j^2 &\\quad i = j \\end{cases} \\] This form of expressing the Jacobian element-wise is unwieldy for computation. Fortunately, this has a simple matrix-representation: \\[ J_{\\boldsymbol{z}}(\\boldsymbol{a}) = \\text{diag}(\\boldsymbol{a}) - \\boldsymbol{a} \\boldsymbol{a}^T \\] Here, \\(\\text{diag}({\\boldsymbol{a}})\\) is a diagonal matrix with the elements of the activation vector making up the diagonal. The second term on the RHS is called the outer-product. We can now plug the Jacobian into the earlier equation: \\[ \\begin{aligned} \\boldsymbol{z^{(g)}} &= J_{\\boldsymbol{z}}(\\boldsymbol{a})^T\\boldsymbol{a^{(g)}}\\\\ &= \\Big[ \\text{diag}(\\boldsymbol{a}) - \\boldsymbol{a} \\boldsymbol{a}^T \\Big] \\boldsymbol{a^{(g)}}\\\\ &= \\left( \\boldsymbol{a} \\odot \\boldsymbol{a^{(g)}} \\right) - \\left( \\boldsymbol{a}^T \\boldsymbol{a^{(g)}}\\boldsymbol{a}\\right)\\\\ &= \\left(\\boldsymbol{a} \\odot \\boldsymbol{a^{(g)}} \\right) - \\left( \\boldsymbol{a} \\odot \\boldsymbol{a^{(g)}} \\boldsymbol{1}_k \\boldsymbol{a}\\right)\\\\ \\end{aligned} \\] That might seem terribly complicated. Step-2 of the equation is just substituting the Jacobian. To understand step-3, try to think about the product of a diagonal matrix and a vector, and see how that transforms to element-wise product between two vectors. The final-step is another trick where we convert the dot product between two vectors into an element-wise product followed by multiplication by a vector of ones. Why are all these transformations necessary? The moment we express them as element-wise products, we can almost effortlessly extend this formula to a matrix of activations: \\[ \\boldsymbol{Z_L^{(g)}} = \\left(\\boldsymbol{A_L} \\odot \\boldsymbol{A_L^{(g)}} \\right) - \\left( \\boldsymbol{A_L} \\odot \\boldsymbol{A_L^{(g)}} \\boldsymbol{1}_{k \\times k} \\right) \\odot \\boldsymbol{A_L}\\\\ \\] That again looks complicated. But if we stare at it for a while, the RHS is actually remarkably simple: The first term is is simply the element-wise multiplication of the activation matrix and its corresponding gradient. Call this some matrix \\(\\boldsymbol{B}\\) . The second term is just the row-wise sum of \\(\\boldsymbol{B}\\) followed by an element-wise multiplication with the activation-matrix. These expressions are powerful because we can directly translate them into NumPy . For example, the NumPy equivalent of this equation would be: B = A_L * A_L_g Z_l_g = B - np.sum(B, axis = 1) * A_l The good news is that the expression for \\(\\boldsymbol{Z_L^{(g)}}\\) In the case of softmax with categorical cross-entropy loss can be further simplified. Recall the following results: \\[ \\begin{aligned} \\boldsymbol{A_L} &= \\boldsymbol{\\hat{Y}}\\\\ \\boldsymbol{A_L^{(g)}} &= - \\boldsymbol{Y} \\odot \\boldsymbol{\\hat{Y}}^{\\odot -1} \\end{aligned} \\] The element-wise product of these two matrices is simply the matrix \\(-\\boldsymbol{Y}\\) ! With that, the expression for \\(\\boldsymbol{Z_L^{(g)}}\\) becomes: \\[ \\boldsymbol{Z_L^{(g)}} = \\boldsymbol{\\hat{Y}} - \\boldsymbol{Y} \\] All this computation seems justified given the elementary nature of the result.","title":"Appendix"},{"location":"neural-networks/07-appendix/#appendix","text":"","title":"Appendix"},{"location":"neural-networks/07-appendix/#categorical-cross-entropy","text":"","title":"Categorical Cross Entropy"},{"location":"neural-networks/07-appendix/#part-1","text":"In the case of multi-class classification, the output layer after softmax can be viewed as a conditional probability distribution over the labels given the data-point. This is a categorical distribution over the \\(k\\) classes. Let us call it \\(q(y\\ |\\ x)\\) . This is the predicted distribution. There is a true distribution over the labels. Let us call it \\(p(y\\ |\\ x)\\) . Note that both are conditional distributions of the label given the data-point. The (conditional) cross-entropy of these two conditional distributions is given as follows: $$ H \\Big( p(y\\ |\\ x), q(y\\ |\\ x) \\Big) = -\\sum \\limits_{x} p(x) \\sum \\limits_{y} p(y\\ |\\ x) \\log q(y\\ |\\ x) $$ Note that this is really the expectation of some function \\(f(x)\\) over the marginal distribution \\(p(x)\\) : \\[ \\begin{aligned} E[f(x)] &= \\sum \\limits_{x} p(x) f(x)\\\\ &= E \\left[- \\sum \\limits_{y} p(y\\ |\\ x) \\log q(y\\ |\\ x) \\right] \\end{aligned} \\] To compute an expectation, we need to sum over all samples drawn from this distribution. In practice, this is not going to be possible. This expectation is typically approximated by sampling data-points from the marginal distribution. We can replace the exhaustive summation with a sum over the \\(n\\) data-points in our batch of examples: $$ H \\Big( p(y\\ |\\ x), q(y\\ |\\ x) \\Big) = -\\cfrac{1}{n} \\sum \\limits_{i = 1}^{n} \\sum \\limits_{y} p(y\\ |\\ x^{(i)}) \\log q(y\\ |\\ x^{(i)}) $$ A crude way of seeing this is as follows: if a particular data-point \\(x^{*}\\) is sampled \\(r\\) times, (appears \\(r\\) times in the batch) then: $$ p(x^{*}) \\approx \\cfrac{r}{n} $$ Let us get rid of the factor of \\(\\frac{1}{n}\\) as it doesn't matter from the point of view of optimization.","title":"Part-1"},{"location":"neural-networks/07-appendix/#part-2","text":"We are left with the following expression for the cross-entropy: $$ H \\Big( p(y\\ |\\ x), q(y\\ |\\ x) \\Big) = -\\sum \\limits_{i = 1}^{n} \\sum \\limits_{y} p(y\\ |\\ x^{(i)}) \\log q(y\\ |\\ x^{(i)}) $$ We now need to understand the inner sum over the labels. Some notation needs to be be setup to make things more tractable: \\[ \\begin{aligned} y^{(i)}&: \\text{true label for data-point i, scalar}\\\\ \\boldsymbol{y^{(i)}}&: \\text{true label for data-point i, one-hot vector}\\\\ \\boldsymbol{\\hat{y}^{(i)}}&: \\text{predicted probabilities for data-point i, vector} \\end{aligned} \\] The symbols in bold are are vectors of length \\(k\\) , where \\(k\\) is the number of classes. With this notation we can get a better idea about the true and predicted distributions, \\(p\\) and \\(q\\) , respectively: \\[ \\begin{aligned} p(y = c\\ |\\ x^{(i)}) &= \\boldsymbol{y^{(i)}_{c}}\\\\ q(y = c\\ |\\ x^{(i)}) &= \\boldsymbol{\\hat{y}^{(i)}_{c}} \\end{aligned} \\] Note that \\(\\boldsymbol{\\hat{y}^{(i)}_{c}}\\) is the \\(c^{th}\\) component of a vector and hence a scalar. Same is the case for \\(\\boldsymbol{y^{(i)}_{c}}\\) We are now ready to compute the categorical cross-entropy: \\[ \\begin{aligned} H \\Big( p(y\\ |\\ x), q(y\\ |\\ x) \\Big) &= -\\sum \\limits_{i = 1}^{n} \\sum \\limits_{y} p(y\\ |\\ x^{(i)}) \\log q(y\\ |\\ x^{(i)})\\\\ &= - \\sum \\limits_{i = 1}^{n} \\sum \\limits_{c = 1}^{k} p(y = c\\ |\\ x^{(i)}) \\log q(y = c\\ |\\ x^{(i)})\\\\ &= - \\sum \\limits_{i = 1}^{n} \\sum \\limits_{c = 1}^{k} \\boldsymbol{y^{(i)}_{c}}\\ \\cdot \\log(\\boldsymbol{\\hat{y}^{(i)}_{c}}) \\end{aligned} \\] This is the final expression for the categorical cross-entropy (CCE) loss. Even though the inner sum is over \\(k\\) classes, only one of the values for \\(\\boldsymbol{y^{(i)}_{c}}\\) is non-zero. In the end, the sum reduces to a very simple scalar expression in principle: \\[ \\text{CCE} = \\sum \\limits_{i = 1}^{n} -\\log \\boldsymbol{\\hat{y}^{(i)}_{y^{(i)}}} \\] I know that this looks hopelessly complicated, thanks to the subscripts and superscripts. But what it is saying is this: The categorical cross-entropy for a batch of data-points can be computed as follows: For each data-point, get the predicted probability corresponding to the true label. Take negative log of that probability. Sum this quantity across the entire batch.","title":"Part-2"},{"location":"neural-networks/07-appendix/#part-3","text":"Intuitively, this loss makes a lot of sense. In fact, we can see why this should qualify as a loss in the first place by asking these questions: When is this quantity minimized? When the predicted probability for the correct class (true label) is \\(1\\) for every data-point in the batch. What happens if the predicted probability for the correct class for some data-point is close to \\(0\\) ? In such a case, \\(-\\log \\boldsymbol{\\hat{y}^{(i)}}_{y^{(i)}}\\) will be a huge positive value, something that is undesirable. What does minimizing cross-entropy really mean? The attempt to minimize the categorical cross-entropy translates to pushing the probability of the correct-class closer and closer to \\(1\\) for each data-point. Our hope is that in this process, the model will learn something useful that generalizes to unseen data-points as well.","title":"Part-3"},{"location":"neural-networks/07-appendix/#part-4","text":"We can see how this reduces to the binary cross-entropy loss in the case of two classes: \\[ \\begin{aligned} BCE &= - \\sum \\limits_{i = 1}^{n} \\sum \\limits_{c = 1}^{2} \\boldsymbol{y^{(i)}_{c}}\\ \\cdot \\log(\\boldsymbol{\\hat{y}^{(i)}_{c}})\\\\ &= - \\sum \\limits_{i = 1}^{n} \\left( y^{(i)} \\log (\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log (1 - \\hat{y}^{(i)}) \\right) \\end{aligned} \\]","title":"Part-4"},{"location":"neural-networks/07-appendix/#chain-rule-for-matrix-product","text":"Let \\(P = QR\\) , where \\(P, Q\\) and \\(R\\) are matrices of compatible dimensions. Let \\(\\phi\\) be some function of \\(P\\) . Let us assume that we have access to the gradient of \\(\\phi\\) with respect to \\(P\\) . Call this \\(P^{(g)}\\) . Then, we have the following equations: \\[ Q^{(g)} = P^{(g)} R^T\\\\ R^{(g)} = Q^T P^{(g)} \\] This is nothing but the chain rule of differentiation in the presence of a matrix product.","title":"Chain rule for matrix product"},{"location":"neural-networks/07-appendix/#gradient-for-softmax-layer","text":"In order to derive the gradient matrix at the final layer, it is easier to consider the simpler case of the gradient vector. Let \\(\\boldsymbol{z}\\) be the pre-activation vector at layer \\(L\\) of the network. After applying softmax function \\(g\\) on the pre-activation vector, we get \\(\\boldsymbol{a}\\) , the activation vector. The two vectors are related by the following equations: \\[ \\boldsymbol{a} = g(\\boldsymbol{z}) = \\begin{bmatrix} \\cdots & \\cfrac{e^{z_j}}{\\sum \\limits_{r = 1}^{k} e^{z_r}} & \\cdots \\end{bmatrix} \\] The gradient of the loss with respect to the activation vector is given by: \\[ \\boldsymbol{a^{(g)}} = \\nabla_{\\boldsymbol{a}} L \\] The tricky part is the gradient of the loss with respect to the pre-activations. Let us consider a single element in the pre-activation vector, \\(z_j\\) . We are interested in the partial derivative \\(\\cfrac{\\partial L}{\\partial z_j}\\) . What this means is this: how much does the loss change when a small change is made to \\(z_j\\) . It is not exactly this, but the ratio of these two quantities, but you get the idea. \\(z_j\\) isn't directly connected to the loss function. The elements of the activation vector \\(a_i\\) s intervene. In other words, the impact of \\(z_j\\) on the loss is mediated by the \\(a_i\\) s. Changing \\(z_j\\) has an impact on each of the \\(a_i\\) s because of the way softmax is defined. We already know how the loss is affected when the \\(a_j\\) s are disturbed. This is captured by \\(\\boldsymbol{a^{(g)}}\\) . So much for the chain rule. We are now ready to state it: \\[ \\cfrac{\\partial L}{\\partial z_j} = \\sum \\limits_{i = 1}^{k} \\cfrac{\\partial L}{\\partial a_i} \\cfrac{\\partial a_i}{\\partial z_j} \\] The second term in the product is the \\(ij^{th}\\) element of the Jacobian matrix \\(J_{\\boldsymbol{z}}(\\boldsymbol{a})\\) : \\[ J_{\\boldsymbol{z}}(\\boldsymbol{a}) \\Bigg|_{ij} = \\left [ \\cfrac{\\partial a_i}{\\partial z_j} \\right] \\] The Jacobian matrix is a matrix of partial derivatives of the components of the vector \\(\\boldsymbol{a}\\) with respect to \\(\\boldsymbol{z}\\) . With the introduction of the Jacobian matrix, we can now state the gradient of the loss with respect to the pre-activations in the following succinct manner: \\[ \\boldsymbol{z^{(g)}} = \\nabla_{\\boldsymbol{z}} L = J_{\\boldsymbol{z}}(\\boldsymbol{a})^T\\boldsymbol{a^{(g)}} \\] We can now turn our attention to computing the Jacobian: \\[ J_{\\boldsymbol{z}}(\\boldsymbol{a}) \\Bigg|_{ij} = \\begin{cases} -a_i a_j &\\quad i \\neq j\\\\ a_j - a_j^2 &\\quad i = j \\end{cases} \\] This form of expressing the Jacobian element-wise is unwieldy for computation. Fortunately, this has a simple matrix-representation: \\[ J_{\\boldsymbol{z}}(\\boldsymbol{a}) = \\text{diag}(\\boldsymbol{a}) - \\boldsymbol{a} \\boldsymbol{a}^T \\] Here, \\(\\text{diag}({\\boldsymbol{a}})\\) is a diagonal matrix with the elements of the activation vector making up the diagonal. The second term on the RHS is called the outer-product. We can now plug the Jacobian into the earlier equation: \\[ \\begin{aligned} \\boldsymbol{z^{(g)}} &= J_{\\boldsymbol{z}}(\\boldsymbol{a})^T\\boldsymbol{a^{(g)}}\\\\ &= \\Big[ \\text{diag}(\\boldsymbol{a}) - \\boldsymbol{a} \\boldsymbol{a}^T \\Big] \\boldsymbol{a^{(g)}}\\\\ &= \\left( \\boldsymbol{a} \\odot \\boldsymbol{a^{(g)}} \\right) - \\left( \\boldsymbol{a}^T \\boldsymbol{a^{(g)}}\\boldsymbol{a}\\right)\\\\ &= \\left(\\boldsymbol{a} \\odot \\boldsymbol{a^{(g)}} \\right) - \\left( \\boldsymbol{a} \\odot \\boldsymbol{a^{(g)}} \\boldsymbol{1}_k \\boldsymbol{a}\\right)\\\\ \\end{aligned} \\] That might seem terribly complicated. Step-2 of the equation is just substituting the Jacobian. To understand step-3, try to think about the product of a diagonal matrix and a vector, and see how that transforms to element-wise product between two vectors. The final-step is another trick where we convert the dot product between two vectors into an element-wise product followed by multiplication by a vector of ones. Why are all these transformations necessary? The moment we express them as element-wise products, we can almost effortlessly extend this formula to a matrix of activations: \\[ \\boldsymbol{Z_L^{(g)}} = \\left(\\boldsymbol{A_L} \\odot \\boldsymbol{A_L^{(g)}} \\right) - \\left( \\boldsymbol{A_L} \\odot \\boldsymbol{A_L^{(g)}} \\boldsymbol{1}_{k \\times k} \\right) \\odot \\boldsymbol{A_L}\\\\ \\] That again looks complicated. But if we stare at it for a while, the RHS is actually remarkably simple: The first term is is simply the element-wise multiplication of the activation matrix and its corresponding gradient. Call this some matrix \\(\\boldsymbol{B}\\) . The second term is just the row-wise sum of \\(\\boldsymbol{B}\\) followed by an element-wise multiplication with the activation-matrix. These expressions are powerful because we can directly translate them into NumPy . For example, the NumPy equivalent of this equation would be: B = A_L * A_L_g Z_l_g = B - np.sum(B, axis = 1) * A_l The good news is that the expression for \\(\\boldsymbol{Z_L^{(g)}}\\) In the case of softmax with categorical cross-entropy loss can be further simplified. Recall the following results: \\[ \\begin{aligned} \\boldsymbol{A_L} &= \\boldsymbol{\\hat{Y}}\\\\ \\boldsymbol{A_L^{(g)}} &= - \\boldsymbol{Y} \\odot \\boldsymbol{\\hat{Y}}^{\\odot -1} \\end{aligned} \\] The element-wise product of these two matrices is simply the matrix \\(-\\boldsymbol{Y}\\) ! With that, the expression for \\(\\boldsymbol{Z_L^{(g)}}\\) becomes: \\[ \\boldsymbol{Z_L^{(g)}} = \\boldsymbol{\\hat{Y}} - \\boldsymbol{Y} \\] All this computation seems justified given the elementary nature of the result.","title":"Gradient for softmax layer"}]}